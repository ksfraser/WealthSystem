%
% -----------------------------------------------------------------------------
%
% A license is hereby granted to reproduce this software source code and
% to create executable versions from this source code for personal,
% non-commercial use.  The copyright notice included with the software
% must be maintained in all copies produced.
%
% THIS PROGRAM IS PROVIDED "AS IS". THE AUTHOR PROVIDES NO WARRANTIES
% WHATSOEVER, EXPRESSED OR IMPLIED, INCLUDING WARRANTIES OF
% MERCHANTABILITY, TITLE, OR FITNESS FOR ANY PARTICULAR PURPOSE.  THE
% AUTHOR DOES NOT WARRANT THAT USE OF THIS PROGRAM DOES NOT INFRINGE THE
% INTELLECTUAL PROPERTY RIGHTS OF ANY THIRD PARTY IN ANY COUNTRY.
%
% Copyright (c) 1994-2006, John Conover, All Rights Reserved.
%
% Comments and/or bug reports should be addressed to:
%
%     john@email.johncon.com (John Conover)
%
% -----------------------------------------------------------------------------
%
% Revision: \RCSRevision \\
% Revision Time: \RCSTime UMT \\
% Revision Date: \RCSDate \\
% Revision Id: \RCSId \\
% Revision File: \RCSLog \\
\RCS $Revision: 0.0 $
\RCS $Date: 2006/01/25 04:38:13 $
\RCS $Id: appb.tex,v 0.0 2006/01/25 04:38:13 john Exp $
% $Log: appb.tex,v $
% Revision 0.0  2006/01/25 04:38:13  john
% Initial version
%
%
\chapter{Computer Programs Used in the Analysis of Fractal Time Series}
    \label{programs}
    \idx{programs}
    \subidx{programs}{C language}
    \subidx{programs}{availability}
    \subidx{programs}{Licensing}
    \subidx{programs}{sources}

    The ``C'' language sources to the following programs are available
    by sending an electronic mail to
    john-archive-request@johncon.johncon.com with a subject of
    ``archive get fractal''. The source distribution also contains the
    {\LaTeX} sources to this document. The figures in this appendix
    were made from the regression tests for the programs, and can be
    reconstructed with the Unix make(1) utility in
    the~../simulation/test, and~../utilities/test directories.

    \section{Legal Restrictions}

        A license is hereby granted to reproduce this software source
        code and to create executable versions from this source code
        for personal, non---commercial use.  The copyright notice
        included with the software must be maintained in all copies
        produced.

        THESE PROGRAMS ARE PROVIDED ``AS IS''. THE AUTHOR PROVIDES NO
        WARRANTIES WHATSOEVER, EXPRESSED OR IMPLIED, INCLUDING
        WARRANTIES OF MERCHANTABILITY, TITLE, OR FITNESS FOR ANY
        PARTICULAR PURPOSE\@.  THE AUTHOR DOES NOT WARRANT THAT USE OF
        THIS PROGRAM DOES NOT INFRINGE THE INTELLECTUAL PROPERTY
        RIGHTS OF ANY THIRD PARTY IN ANY COUNTRY.

    \section{Fractal Time Series Analytical Utilities}

        \subsection{tsderivative}
            \subidx{programs}{tsderivative}
            \subidx{tsderivative}{program}
            \subidx{time series}{derivative}
            \subidx{derivative}{derivative}

            Source tsderivative.c, for taking the derivative of a time
            series.  The value of a sample in the time series is
            subtracted from the previous sample in the time
            series. The derivative time series is printed to stdout.

            The input file structure is a text file consisting of
            records, in temporal order, one record per time series
            sample.  Blank records are ignored, and comment records
            are signified by a `\#' character as the first non white
            space character in the record. Data records must contain
            at least one field, which is the data value of the sample,
            but may contain many fields---if the record contains many
            fields, then the first field is regarded as the sample's
            time, and the last field as the sample's value at that
            time.

            An example output from the {\it tsderivative}\/ program
            appears in Figure~\ref{derivativeexample}.

        \subsection{tsintegrate}
            \subidx{programs}{tsintegrate}
            \subidx{tsintegrate}{program}
            \subidx{time series}{integration}
            \subidx{integration}{time series}

            Source tsintegrate.c, for taking the integral of a time
            series.  The value of a sample in the time series is added
            to the previous samples in the time series. The integral
            time series is printed to stdout.

            The input file structure is a text file consisting of
            records, in temporal order, one record per time series
            sample.  Blank records are ignored, and comment records
            are signified by a `\#' character as the first non white
            space character in the record. Data records must contain
            at least one field, which is the data value of the sample,
            but may contain many fields---if the record contains many
            fields, then the first field is regarded as the sample's
            time, and the last field as the sample's value at that
            time.

            An example output from the {\it tsintegrate}\/ program
            appears in Figure~\ref{integrateexample}.

            \begin{figure}[ht]
                \begin{center}
                    \begin{minipage}[t]{0.45\textwidth}
                        \epsfxsize=1.0\linewidth
                        \epsffile{../utilities/test/tsbrownian.tsderivative.tsnormal.eps}
                        \caption[Example output of the {\it
                            tsderivative}\/ program]{Example output of
                            the {\it tsderivative}\/ program, using
                            the output of the {\it tsbrownian}\/
                            program with 1500 records as input. The
                            frequency histogram should have the same
                            distribution as that produced by the {\it
                            tswhite}\/ program in
                            Figure~\ref{whiteexample}.}
                        \label{derivativeexample}
                    \end{minipage}
                    \hfill
                    \begin{minipage}[t]{0.45\textwidth}
                        \epsfxsize=1.0\linewidth
                        \epsffile{../utilities/test/tswhite.tsintegrate.tsnormal.eps}
                        \caption[Example output of the {\it
                            tsintegrate}\/ program]{Example output of
                            the {\it tsintegrate}\/ program, using the
                            output of the {\it tswhite}\/ program with
                            1500 records as input. The frequency
                            histogram should have the same
                            distribution as that produced by the {\it
                            tsbrownian}\/ program in
                            Figure~\ref{brownianexample}.}
                        \label{integrateexample}
                    \end{minipage}
                \end{center}
            \end{figure}

        \subsection{tshcalc}
            \subidx{programs}{tshcalc}
            \subidx{tshcalc}{program}
            \subidx{H parameter}{program}
            \subidx{program}{H parameter}

            Source tshcalc.c, for calculating the H parameter for a
            one variable fractional Brownian motion time series. The
            algorithm is from~\cite[pp. 249]{Crownover}.

            The input file structure is a text file consisting of
            records, in temporal order, one record per time series
            sample.  Blank records are ignored, and comment records
            are signified by a `\#' character as the first non white
            space character in the record. Data records must contain
            at least one field, which is the data value of the sample,
            but may contain many fields---if the record contains many
            fields, then the first field is regarded as the sample's
            time, and the last field as the sample's value at that
            time.

            An example output from the {\it tshcalc}\/ program appears
            in Figure~\ref{hcalcexample}.

        \subsection{tshurst}
            \subidx{programs}{tshurst}
            \subidx{tshurst}{program}
            \subidx{Hurst coefficient}{program}
            \subidx{program}{Hurst coefficient}
            \subidx{fractal}{range}
            \subidx{fractal}{standard deviation}
            \subidx{fractal}{R/S analysis}

            Source tshurst.c, for calculating the Hurst coefficient
            for a time series.  The method used is
            from~\cite[pp. 253]{Casti:C},~\cite[pp. 63]{Peters:CAOITCM},~\cite[pp. 129]{Schroeder},
            or~\cite[pp. 172]{Cambel}.  The time series is broken into
            variable length intervals, which are assumed to be
            independent of each other, and the R/S value is computed
            for each interval based on the deviation from the average
            over the interval. These R/S values are then averaged for
            all of the intervals, then printed to stdout. The -r flag
            sets operation as described in ``Chaos and Order in the
            Capital Markets,'' by Edgar E. Peters, pp 81, and should
            only be used for time series from market data since
            logarithmic returns sum to cumulative return---negative
            numbers in the time series file are not permitted with
            this option. The $\ln (\frac{R}{S})$ vs $\ln (time)$ plot
            is printed to stdout.

            The input file structure is a text file consisting of
            records, in temporal order, one record per time series
            sample.  Blank records are ignored, and comment records
            are signified by a `\#' character as the first non white
            space character in the record. Data records must contain
            at least one field, which is the data value of the sample,
            but may contain many fields---if the record contains many
            fields, then the first field is regarded as the sample's
            time, and the last field as the sample's value at that
            time.

            An example output from the {\it tshurst}\/ program appears
            in Figure~\ref{hurstexample}.

            \begin{figure}[ht]
                \begin{center}
                    \begin{minipage}[t]{0.45\textwidth}
                        \epsfxsize=1.0\linewidth
                        \epsffile{../utilities/test/tsfBm.tshcalc.eps}
                        \caption[Example output of the {\it tshcalc}\/
                            program] {Example output of the {\it
                            tshcalc}\/ program, using simulated Hurst
                            coefficients of 0.0, 0.1, 0.3, 0.5, 0.7,
                            0.9, and 1.0, as simulated by the {\it
                            tsfBm}\/ program.}
                        \label{hcalcexample}
                    \end{minipage}
                    \hfill
                    \begin{minipage}[t]{0.45\textwidth}
                        \epsfxsize=1.0\linewidth
                        \epsffile{../utilities/test/tsfBm.tshurst.eps}
                        \caption[Example output of the {\it tshurst}\/
                            program] {Example output of the {\it
                            tshurst}\/ program, using simulated Hurst
                            coefficients of 0.0, 0.1, 0.3, 0.5, 0.7,
                            0.9, and 1.0, as simulated by the {\it
                            tsfBm}\/ program.}
                        \label{hurstexample}
                    \end{minipage}
                \end{center}
            \end{figure}

        \subsection{tslogreturns}
            \subidx{programs}{tslogreturns}
            \subidx{tslogreturns}{program}
            \subidx{time series}{logarithmic returns}
            \subidx{logarithmic returns}{time series}

            Source tslogreturns.c, is for taking the logarithmic
            returns of of a time series.  The value of a sample in the
            time series is divided by the value of the previous sample
            in the time series, and the logarithm of the quotient is
            printed to stdout.

            The form of the best fit is $e^{at}$ for exponential least
            squares fit, or $x^{t}$ for power least squares fit, or
            $2^{bt}$ for binary fit.

            The input file structure is a text file consisting of
            records, in temporal order, one record per time series
            sample.  Blank records are ignored, and comment records
            are signified by a `\#' character as the first non white
            space character in the record. Data records must contain
            at least one field, which is the data value of the sample,
            but may contain many fields---if the record contains many
            fields, then the first field is regarded as the sample's
            time, and the last field as the sample's value at that
            time.

            \noindent Note: The derivation for exponential least
            squares fit is:

            \begin{eqnarray}
                y\left(t\right) & = & e^{k1 + k2t}\\
                s\left(t\right) & = & \ln \left(\frac{e^{k1 + k2t}}{e^{k1 + k2 \left(t - 1\right)}}\right)\\
                                & = & \ln \left(e^{k1 + k2t - k1 - k2t + k2}\right)\\
                                & = & \ln \left(e^{k2}\right)\\
                                & = & k2
            \end{eqnarray}

            \noindent Note: The derivation for power least squares fit
            is:

            \begin{eqnarray}
                  y\left(t\right) & = & e^{k1 + k2t}\\
                  s\left(t\right) & = & \ln \left(\frac{e^{k1 + k2t}}{e^{k1 + k2 \left(t - 1\right)}}\right)\\
                                  & = & \ln \left(e^{k1 + k2t - k1 - k2t + k2}\right)\\
                                  & = & \ln \left(e^{k2}\right)\\
                                  & = & k2\\
                           e^{xt} & = & a^{t}\\
                               xt & = & \ln \left(a^{t}\right)\\
                                  & = & t \ln \left(a\right)\\
                                x & = & \ln \left(a\right)\\
                                a & = & e^{x}
            \end{eqnarray}

            \noindent Note: The derivation for the binary least
            squares fit is:

            \begin{eqnarray}
                     y\left(t\right) & = & e^{k1 + k2t}\\
                     s\left(t\right) & = & \ln \left(\frac{e^{k1 + k2t}}{e^{k1 + k2 \left(t - 1\right)}}\right)\\
                                     & = & \ln \left(e^{k1 + k2t - k1 - k2t + k2}\right)\\
                                     & = & \ln \left(e^{k2}\right)\\
                                     & = & k2\\
                              e^{xt} & = & 2^{kt}\\
                               a^{t} & = & 2^{kt}\\
                                   a & = & 2^{k}\\
                k \ln \left(2\right) & = & \ln \left(a\right)\\
                                   k & = & \frac{\ln \left(a\right)}{\ln \left(2\right)}\\
            \end{eqnarray}

            An example output from the {\it tslogreturns}\/ program
            appears in Figure~\ref{logreturnsexample}.

        \subsection{tsshannon}
            \subidx{programs}{tsshannon}
            \subidx{tsshannon}{program}
            \subidx{time series}{Shannon probability}
            \subidx{Shannon probability}{time series}

            \subidx{Newton---Raphson}{iterated solution}
            \subidx{iterated solution}{Newton---Raphson}
            \subidx{searching for solutions}{methodology}
            Source tsshannon.c, for calculating the probability, given
            the Shannon information capacity. See~\cite[pp. 128,
            151]{Schroeder}.  Uses Newton---Raphson method for an
            iterative solution for the probability, $p$.

            As a reference on Newton---Raphson Method of root finding,
            see~\cite[pp. 270]{Press}.

            \noindent From~\cite[pp.  151]{Schroeder}:

            \noindent $p = 0.55$, $2^{C(0.55)} = 0.005$, (probably a
            typo, meaning $1.005$,) which by calculator, $C(0.55) =
            0.0072$, (this program gives $C(0.549912) = 0.0072$).

            \noindent Derivation, starting with~\cite[pp.
            151]{Schroeder}:

            \begin{eqnarray}
                            C\left(p\right) & = & 1 + p ln_2 \left(p\right) + \left(1 - p\right) ln_2 \left(1 - p\right)\\
                            C\left(p\right) & = & 1 + p \left(\frac{ln \left(p\right)}{ln \left(2\right)}\right) + \left(1 - p\right) \left(\frac{ln \left(1 - p\right)}{ln \left(2\right)}\right)\\
                            C\left(p\right) & = & \left(\frac{1}{ln \left(2\right)}\right) \left(ln \left(2\right) + p ln \left(p\right) + \left(1 - p\right) ln \left(1 - p\right)\right)\\
                            C\left(p\right) & = & \left(\frac{1}{ln \left(2\right)}\right) \left(ln \left(2\right) + p ln \left(p\right) + ln \left(1 - p\right) - p ln \left(1 - p\right)\right)\\
                \frac{dC\left(p\right)}{dp} & = & \left(\frac{1}{ln \left(2\right)}\right) \left(1 + ln \left(p\right) - (\frac{1}{\left(1 - p\right)}) - \left(ln \left(1 - p\right) - (\frac{p}{\left(1 - p\right)})\right)\right)\\
                                            & = & \left(\frac{1}{ln \left(2\right)}\right) \left(1 + ln \left(p\right) - (\frac{1}{\left(1 - p\right)} - ln \left(1 - p\right) + \left(\frac{p}{\left(1 - p\right)}\right)\right)\\
                                            & = & \left(\frac{1}{ln \left(2\right)}\right) \left(ln \left(p\right) - ln \left(1 - p\right) + \left(\frac{p}{\left(1 - p\right)}\right) - \left(\frac{1}{\left(1 - p\right)}\right)\right)\\
                                            & = & \left(\frac{1}{ln \left(2\right)}\right) \left(1 + ln \left(p\right) - ln \left(1 - p\right) + \left(\frac{\left(p - 1\right)}{\left(1 - p\right)}\right)\right)\\
                                            & = & \left(\frac{1}{ln \left(2\right)}\right) \left(1 + ln \left(p\right) - ln \left(1 - p\right) - 1\right)\\
                                            & = & \left(\frac{1}{ln \left(2\right)}\right) \left(ln \left(p\right) - ln \left(1 - p\right)\right)
            \end{eqnarray}

            An example output from the {\it tsshannon}\/ program
            appears in Figure~\ref{shannonexample}.

            \begin{figure}[ht]
                \begin{center}
                    \begin{minipage}[t]{0.45\textwidth}
                        \center{\fbox{$\input{../utilities/test/tscoin.tslogreturns-p.tex}$}}
                        \caption[Example output of the {\it
                            tslogreturns}\/ program]{Example output of
                            the {\it tslogreturns}\/ program, the
                            input was produced by the {\it tscoin}\/
                            program, with a Shannon probability of
                            0.6, as shown in Figure~\ref{coinexample}
                            in Section~\ref{tscoinexample}.}
                        \label{logreturnsexample}
                    \end{minipage}
                    \hfill
                    \begin{minipage}[t]{0.45\textwidth}
                        \center{\fbox{$\input{../utilities/test/tscoin.tslogreturns-p.tsshannon.tex}$}}
                        \caption[Example output of the {\it
                            tsshannon}\/ program]{Example output of
                            the {\it tsshannon}\/ program, the input
                            was produced by the {\it tslogreturns}\/
                            program, as shown in
                            Figure~\ref{logreturnsexample}, which was
                            derived from the output of the {\it
                            tscoin}\/ program, with a Shannon
                            probability of 0.6, and is shown in
                            Figure~\ref{coinexample} in
                            Section~\ref{tscoinexample}.}
                        \label{shannonexample}
                    \end{minipage}
                \end{center}
            \end{figure}

        \subsection{tsshannonmax}
            \subidx{programs}{tsshannonmax}
            \subidx{tsshannonmax}{program}
            \subidx{time series}{maximum Shannon probability}
            \subidx{maximum Shannon probability}{time series}

            Source tsshannonmax.c, for calculating unfair returns of a
            time series, as a function of Shannon probability. The
            input time series is presumed to have a Brownian
            distribution. The main function of this program is
            regression scenario verification---given an empirical time
            series, speculative market pro forma performance can be
            analyzed, as a function of Shannon probability. The
            cumulative sum process is Brownian in nature.

            \subidx{maximization}{golden method}
            To find the maximum returns, the ``golden'' method of
            minimization is used.  As a reference on the ``golden''
            method of minimization, see~\cite[pp. 298]{Press}.

            The input file structure is a text file consisting of
            records, in temporal order, one record per time series
            sample.  Blank records are ignored, and comment records
            are signified by a `\#' character as the first non white
            space character in the record. Data records must contain
            at least one field, which is the data value of the sample,
            but may contain many fields---if the record contains many
            fields, then the first field is regarded as the sample's
            time, and the last field as the sample's value at that
            time.

            An example output from the {\it tsshannonmax}\/ program
            appears in Figure~\ref{shannonmaxexample}.

        \subsection{tsfraction}
            \subidx{programs}{tsfraction}
            \subidx{tsfraction}{program}
            \subidx{time series}{incremental change}
            \subidx{incremental change}{time series}

            Source tsfraction.c, for finding the fraction of change in
            a time series. The value of a sample in the time series is
            subtracted from the previous sample in the time series,
            and divided by the value of the previous sample. The
            fraction time series is printed to stdout.

            The input file structure is a text file consisting of
            records, in temporal order, one record per time series
            sample.  Blank records are ignored, and comment records
            are signified by a `\#' character as the first non white
            space character in the record. Data records must contain
            at least one field, which is the data value of the sample,
            but may contain many fields---if the record contains many
            fields, then the first field is regarded as the sample's
            time, and the last field as the sample's value at that
            time.

            An example output from the {\it tsfraction}\/ program
            appears in Figure~\ref{fractionexample}.

            \begin{figure}[ht]
                \begin{center}
                    \begin{minipage}[t]{0.45\textwidth}
                        \epsfxsize=1.0\linewidth
                        \epsffile{../utilities/test/tscoin.tsshannonmax.eps}
                        \caption[Example output of the {\it
                            tsshannonmax}\/ program]{Example output of
                            the {\it tsshannonmax}\/ program, using
                            the file produced by the {\it tscoin}\/
                            program, with a Shannon probability of
                            0.6, which is shown in
                            Figure~\ref{coinexample} in
                            Section~\ref{tscoinexample}.}
                        \label{shannonmaxexample}
                    \end{minipage}
                    \hfill
                    \begin{minipage}[t]{0.45\textwidth}
                        \epsfxsize=1.0\linewidth
                        \epsffile{../utilities/test/tscoin.tsfraction.eps}
                        \caption[Example output of the {\it
                            tsfraction}\/ program]{Example output of
                            the {\it tsfraction}\/ program, the input
                            was produced by the {\it tscoin}\/
                            program, with a Shannon probability of
                            0.6, as shown in Figure~\ref{coinexample}
                            in Section~\ref{tscoinexample}.  The {\it
                            tsfraction}\/ program produces a time
                            series of the ``wins'' or ``losses'' in
                            the game, which in this case is $\pm f =
                            \pm (2P - 1) = \pm (2 \cdot 0.6 - 1) = \pm
                            0.2$.}
                        \label{fractionexample}
                    \end{minipage}
                \end{center}
            \end{figure}

        \subsection{tsrms}
            \subidx{programs}{tsrms}
            \subidx{tsrms}{program}
            \subidx{time series}{root mean square}
            \subidx{root mean square}{time series}

            Source tsrms.c, for taking the root mean square of a time
            series.  The value of a sample in the time series is
            squared and added to the cumulative sum of squares to make
            a new time series. The new time series is printed to
            stdout.

            The input file structure is a text file consisting of
            records, in temporal order, one record per time series
            sample.  Blank records are ignored, and comment records
            are signified by a `\#' character as the first non white
            space character in the record. Data records must contain
            at least one field, which is the data value of the sample,
            but may contain many fields---if the record contains many
            fields, then the first field is regarded as the sample's
            time, and the last field as the sample's value at that
            time.

            An example output from the {\it tsrms}\/ program appears
            in Figure~\ref{rmsexample}.

        \subsection{tslsq}
            \subidx{Newton---Raphson}{iterated solution}
            \subidx{iterated solution}{Newton---Raphson}
            \subidx{searching for solutions}{methodology}
            \subidx{programs}{tslsq}
            \subidx{tslsq}{program}
            \subidx{time series}{least squares fit}
            \subidx{least squares fit}{time series}
            \subidx{time series}{exponential least squares fit}
            \subidx{exponential least squares fit}{time series}

            Source tslsq.c, for making a least squares fit time series
            from a time series.

            The form of the best fit is $b + at$, for linear least
            squares fit, $e^{b + at}$, $w^{x + t}$, or $2^{y + zt}$
            for exponential least squares fit, $c / {(1 + e^{-(b +
            at)})}$ for the logistic least squares fit, $\sqrt{b +
            at}$ for the square root fit, $\ln (b + at)$ for the
            natural logarithmic fit, and and $(b + at)^{2}$ for the
            square law fit.

            The input file structure is a text file consisting of
            records, in temporal order, one record per time series
            sample.  Blank records are ignored, and comment records
            are signified by a `\#' character as the first non white
            space character in the record. Data records must contain
            at least one field, which is the data value of the sample,
            but may contain many fields---if the record contains many
            fields, then the first field is regarded as the sample's
            time, and the last field as the sample's value at that
            time.  Uses Newton---Raphson method for an iterative
            solution for the probability, $p$.

            As a reference on Newton---Raphson Method of root finding,
            see~\cite[pp. 270]{Press}.

            An example output from the {\it tslsq}\/ program appears
            in Figure~\ref{lsqexample}.

            \begin{figure}[ht]
                \begin{center}
                    \begin{minipage}[t]{0.45\textwidth}
                        \epsfxsize=1.0\linewidth
                        \epsffile{../utilities/test/tscoin.tsfraction.tsrms.eps}
                        \caption[Example output of the {\it tsrms}\/
                            program]{Example output of the {\it
                            tsrms}\/ program, the input was produced
                            by the {\it tsfraction}\/ program, shown
                            in Figure~\ref{fractionexample}, which
                            used the output of the {\it tscoin}\/
                            program, with a Shannon probability of
                            0.6, and is shown in
                            Figure~\ref{coinexample} in
                            Section~\ref{tscoinexample}.}
                        \label{rmsexample}
                    \end{minipage}
                    \hfill
                    \begin{minipage}[t]{0.45\textwidth}
                        \epsfxsize=1.0\linewidth
                        \epsffile{../utilities/test/tscoin.tsfraction.tslsq.eps}
                        \caption[Example output of the {\it tslsq}\/
                            program]{Example output of the {\it
                            tslsq}\/ program, the input was produced
                            by the {\it tsfraction}\/ program, shown
                            in Figure~\ref{fractionexample}, which
                            used the output of the {\it tscoin}\/
                            program, with a Shannon probability of
                            0.6, and is shown in
                            Figure~\ref{coinexample} in
                            Section~\ref{tscoinexample}.}
                        \label{lsqexample}
                    \end{minipage}
                \end{center}
            \end{figure}

        \subsection{tsnormal}
            \subidx{programs}{tsnormal}
            \subidx{tsnormal}{program}
            \subidx{time series}{standard deviation}
            \subidx{standard deviation}{time series}
            \subidx{time series}{mean}
            \subidx{mean}{time series}
            \subidx{time series}{average}
            \subidx{average}{time series}
            \subidx{time series}{histogram}
            \subidx{histogram}{time series}
            \subidx{time series}{frequency plot}
            \subidx{frequency plot}{time series}

            Source tsnormal.c, for making a histogram or frequency
            plot of a time series.

            The input file structure is a text file consisting of
            records, in temporal order, one record per time series
            sample.  Blank records are ignored, and comment records
            are signified by a `\#' character as the first non white
            space character in the record. Data records must contain
            at least one field, which is the data value of the sample,
            but may contain many fields---if the record contains many
            fields, then the first field is regarded as the sample's
            time, and the last field as the sample's value at that
            time.

            An example output from the {\it tsnormal}\/ program
            appears in Figure~\ref{normalexample}.

        \subsection{tschangewager}
            \subidx{programs}{tschangewager}
            \subidx{tschangewager}{program}
            \subidx{time series}{unfair returns, changing}
            \subidx{unfair returns, changing}{time series}

            Source tschangewager.c, for changing the unfair returns of
            a time series. The idea is to change the returns of a time
            series which is weighted unfairly, by changing the
            increments by a constant factor.  The main function of
            this program is regression scenario verification---given
            an empirical time series, and a ``wager'' fraction,
            speculative market pro forma performance can be
            analyzed. The input time series is assumed to be
            cumulative sum with fractional or Brownian
            characteristics.

            The input file structure is a text file consisting of
            records, in temporal order, one record per time series
            sample.  Blank records are ignored, and comment records
            are signified by a `\#' character as the first non white
            space character in the record. Data records must contain
            at least one field, which is the data value of the sample,
            but may contain many fields---if the record contains many
            fields, then the first field is regarded as the sample's
            time, and the last field as the sample's value at that
            time.

            An example output from the {\it tschangewager}\/ program
            appears in Figure~\ref{changewagerexample}.

            \begin{figure}[ht]
                \begin{center}
                    \begin{minipage}[t]{0.45\textwidth}
                        \epsfxsize=1.0\linewidth
                        \epsffile{../utilities/test/tsfBm.tsnormal.eps}
                        \caption[Example output of the {\it
                           tsnormal}\/ program] {Example output of the
                           {\it tsnormal}\/ program, using a simulated
                           Hurst coefficient of 0.5 as simulated by
                           the {\it tsfBm}\/ program.}
                        \label{normalexample}
                    \end{minipage}
                    \hfill
                    \begin{minipage}[t]{0.45\textwidth}
                        \epsfxsize=1.0\linewidth
                        \epsffile{../utilities/test/tscoin.tschangewager.eps}
                        \caption[Example output of the {\it
                            tschangewager}\/ program] {Example output
                            of the {\it tschangewager}\/ program,
                            using the file produced by the {\it
                            tscoin}\/ program, with a Shannon
                            probability {\it tscoin}\/ program, with a
                            Shannon probability of 0.6, which is shown
                            in Figure~\ref{coinexample} in
                            Section~\ref{tscoinexample}.  The wager
                            was reduced by 50\%.}
                        \label{changewagerexample}
                    \end{minipage}
                \end{center}
            \end{figure}

        \subsection{tsavg}
            \subidx{programs}{tsavg}
            \subidx{tsavg}{program}
            \subidx{time series}{average}
            \subidx{average}{time series}

            Source tsavg.c, for taking the average of a time series.
            The value of a sample in the time series is added to the
            cumulative sum of the samples to make a new time series by
            dividing the cumulative sum by the number of samples, for
            each sample. The new time series is printed to stdout.

            The input file structure is a text file consisting of
            records, in temporal order, one record per time series
            sample.  Blank records are ignored, and comment records
            are signified by a `\#' character as the first non white
            space character in the record. Data records must contain
            at least one field, which is the data value of the sample,
            but may contain many fields---if the record contains many
            fields, then the first field is regarded as the sample's
            time, and the last field as the sample's value at that
            time.

            An example output from the {\it tsavg}\/ program appears
            in Figure~\ref{avgexample}.

        \subsection{tssample}
            \subidx{programs}{tssample}
            \subidx{tssample}{program}
            \subidx{time series}{sampling}
            \subidx{sampling}{time series}

            Source tssample.c, for sampling a time series.  The value
            of a sample in the time series is printed to stdio only if
            it is a multiple of the specified interval.

            The input file structure is a text file consisting of
            records, in temporal order, one record per time series
            sample.  Blank records are ignored, and comment records
            are signified by a `\#' character as the first non white
            space character in the record. Data records must contain
            at least one field, which is the data value of the sample,
            but may contain many fields---if the record contains many
            fields, then the first field is regarded as the sample's
            time, and the last field as the sample's value at that
            time.

            An example output from the {\it tssample}\/ program
            appears in Figure~\ref{sampleexample}.

            \begin{figure}[ht]
                \begin{center}
                    \begin{minipage}[t]{0.45\textwidth}
                        \epsfxsize=1.0\linewidth
                        \epsffile{../utilities/test/tscoin.tsfraction.tsavg.eps}
                        \caption[Example output of the {\it tsavg}\/
                            program]{Example output of the {\it
                            tsavg}\/ program, the input was produced
                            by the {\it tsfraction}\/ program, shown
                            in Figure~\ref{fractionexample}, which
                            used the output of the {\it tscoin}\/
                            program, with a Shannon probability of
                            0.6, and is shown in
                            Figure~\ref{coinexample} in
                            Section~\ref{tscoinexample}.}
                        \label{avgexample}
                    \end{minipage}
                    \hfill
                    \begin{minipage}[t]{0.45\textwidth}
                        \epsfxsize=1.0\linewidth
                        \epsffile{../utilities/test/tscoin.tssample.eps}
                        \caption[Example output of the {\it
                            tssample}\/ program]{Example output of the
                            {\it tssample}\/ program, sampling every
                            other record. The input was produced by
                            the {\it tscoin}\/ program, shown in
                            Figure~\ref{coinexample}, in
                            Section~\ref{tscoinexample}, with a
                            Shannon probability of 0.6.}
                        \label{sampleexample}
                    \end{minipage}
                \end{center}
            \end{figure}

        \subsection{tsXsquared}
            \subidx{programs}{tsXsquared}
            \subidx{tsXsquared}{program}
            \subidx{time series}{sampling}
            \subidx{sampling}{time series}

            Source tsXsquared.c, for taking the Chi---Square of two
            time series, the first file contains the observed values,
            the second contains the expected values.

            The input file structures are text files consisting of
            records, in temporal order, one record per time series
            sample.  Blank records are ignored, and comment records
            are signified by a `\#' character as the first non white
            space character in the record. Data records must contain
            at least one field, which is the data value of the sample,
            but may contain many fields---if the record contains many
            fields, then the first field is regarded as the sample's
            time, and the last field as the sample's value at that
            time.

            An example output from the {\it tsXsquared}\/ program
            appears in Figure~\ref{xsquaredexample}.

            \begin{figure}[ht]
                \begin{center}
                    \begin{minipage}[t]{0.9\textwidth}
                        \center{\fbox{\input{../utilities/test/tsbrownian.tsderivative.tsnormal.tsXsquared.tex}}}
                        \caption[Example output of the {\it
                            tsXsquared}\/ program]{Example output of
                            the {\it tsXsquared}\/ program, the input
                            was produced by the {\it tsnormal}\/
                            program, as shown in
                            Figure~\ref{derivativeexample}, which was
                            derived from the output of the {\it
                            tsbrownian}\/ program with 1500 records as
                            input.}
                        \label{xsquaredexample}
                    \end{minipage}
                \end{center}
            \end{figure}

        \subsection{tsavgwindow}
            \subidx{programs}{tsavgwindow}
            \subidx{tsavgwindow}{program}

            Source tsavgwindow.c, for taking the average of a time
            series.  The value of a sample in the time series added to
            the cumulative sum of the samples to make a new time
            series by dividing the cumulative sum by the number of
            samples, for each sample. The new time series is printed
            to stdout.

            The input file structure is a text file consisting of
            records, in temporal order, one record per time series
            sample.  Blank records are ignored, and comment records
            are signified by a `\#' character as the first non white
            space character in the record. Data records must contain
            at least one field, which is the data value of the sample,
            but may contain many fields---if the record contains many
            fields, then the first field is regarded as the sample's
            time, and the last field as the sample's value at that
            time.

            An example output from the {\it tsavgwindow}\/ program
            appears in Figure~\ref{avgwindowexample}.

        \subsection{tsrmswindow}
            \subidx{programs}{tsrmswindow}
            \subidx{tsrmswindow}{program}

            Source tsrmswindow.c, is for taking the root mean square
            of a time series.  The square of a value of a sample in
            the time series added to the cumulative sum of the square
            of the samples to make a new time series by dividing the
            cumulative sum of the square of the samples by the number
            of samples, for each sample. The new time series is
            printed to stdout.

            The input file structure is a text file consisting of
            records, in temporal order, one record per time series
            sample.  Blank records are ignored, and comment records
            are signified by a `\#' character as the first non white
            space character in the record. Data records must contain
            at least one field, which is the data value of the sample,
            but may contain many fields---if the record contains many
            fields, then the first field is regarded as the sample's
            time, and the last field as the sample's value at that
            time.

            An example output from the {\it tsrmswindow}\/ program
            appears in Figure~\ref{rmswindowexample}.

            \begin{figure}[ht]
                \begin{center}
                    \begin{minipage}[t]{0.45\textwidth}
                        \epsfxsize=1.0\linewidth
                        \epsffile{../utilities/test/tscoin.tsfraction.tsavgwindow.eps}
                        \caption[Example output of the {\it
                            tsavgwindow}\/ program]{Example output of
                            the {\it tsavgwindow}\/ program, the input
                            was produced by the {\it tsfraction}\/
                            program, shown in
                            Figure~\ref{fractionexample}, which used
                            the output of the {\it tscoin}\/ program,
                            with a Shannon probability of 0.6, and is
                            shown in Figure~\ref{coinexample} in
                            Section~\ref{tscoinexample}.}
                        \label{avgwindowexample}
                    \end{minipage}
                    \hfill
                    \begin{minipage}[t]{0.45\textwidth}
                        \epsfxsize=1.0\linewidth
                        \epsffile{../utilities/test/tscoin.tsfraction.tsrmswindow.eps}
                        \caption[Example output of the {\it
                            tsrmswindow}\/ program]{Example output of
                            the {\it tsrmswindow}\/ program, the input
                            was produced by the {\it tsfraction}\/
                            program, shown in
                            Figure~\ref{fractionexample}, which used
                            the output of the {\it tscoin}\/ program,
                            with a Shannon probability of 0.6, and is
                            shown in Figure~\ref{coinexample} in
                            Section~\ref{tscoinexample}.}
                        \label{rmswindowexample}
                    \end{minipage}
                \end{center}
            \end{figure}

        \subsection{tsshannonwindow}
            \subidx{programs}{tsshannonwindow}
            \subidx{tsshannonwindow}{program}
            \subidx{time series}{Shannon probability}
            \subidx{Shannon probability}{time series}

            Source tsshannonwindow.c, for finding the windowed Shannon
            probability of a time series.  The Shannon probability is
            calculated by the following method:

            \begin{enumerate}

                \item For each sample in the time series:

                \begin{enumerate}

                    \item Find the value of the sample's normalized
                    increment by subtracting the previous value of the
                    time series from the current value of the time
                    series, and then dividing this value of the
                    increment by the previous value in the time
                    series, (note that this is similar to the
                    procedure used by the program {\it tsfraction}\/).

                    \item Find the running value of the root mean
                    square of a window of the normalized increments,
                    (note that this is similar to the procedure used
                    by the program {\it tsrmswindow}\/).

                    \item Find the running value of the average of a
                    window of the normalized increments, (note that
                    this is similar to the procedure used by the
                    program {\it tsavgwindow}\/).

                \end{enumerate}

                \item Compute the Shannon probability of the windows
                by eight methods:

                \begin{enumerate}

                    \item using the formula:

                    \begin{equation}
                        P = \frac{\frac{avg}{rms} + 1}{2}
                    \end{equation}

                    \noindent which is derived in
                    Chapter~\ref{general}, in Equation~\ref{fopt3}.

                    \item using the formula:

                    \begin{equation}
                        P = \frac{rms + 1}{2}
                    \end{equation}

                    \noindent which is derived in
                    Chapter~\ref{general}, by combining
                    Equations~\ref{optimumequation} and~\ref{rms}.

                    \item using the formula:

                    \begin{equation}
                        P = \frac{\sqrt{avg} + 1}{2}
                    \end{equation}

                    \noindent which is derived in
                    Chapter~\ref{general}, by combining
                    Equations~\ref{avgts}, and,~\ref{optimumequation}.

                    \item by taking the absolute value of the
                    normalized increments and using the formula:

                    \begin{equation}
                        P = \frac{abs + 1}{2}
                    \end{equation}

                    \noindent which is derived in
                    Appendix~\ref{tutorial}, in
                    Equation~\ref{bet_optimum5}.

                    \item counting the up movements in the window of
                    the time series, and considering adjacent elements
                    from the time series with equal magnitude as an up
                    movement.

                    \item counting the up movements in the window of
                    the time series, and considering adjacent elements
                    from the time series with equal magnitude as a
                    down movement.

                    \subidx{Newton---Raphson}{iterated solution}
                    \subidx{iterated solution}{Newton---Raphson}
                    \item finding an exponential least squares fit of
                    the values of the time series in a window, and
                    iteratively calculating the Shannon probability
                    from the least squares fit variable using
                    Newton---Raphson method for finding the roots of a
                    function.

                    \item finding the logarithmic returns of the
                    values of the time series in a window, and
                    iteratively calculating the Shannon probability
                    from the least squares fit variable using
                    Newton---Raphson method for finding the roots of a
                    function.

                \end{enumerate}

            \end{enumerate}

            Where $P$ is the Shannon probability, $avg$ is the running
            average of a window of the normalized increments, and,
            $rms$ is the running root mean square of a window of the
            increments.  The Shannon probability of the windows of the
            increments is a time series that is printed to stdout.

            The input file structure is a text file consisting of
            records, in temporal order, one record per time series
            sample.  Blank records are ignored, and comment records
            are signified by a `\#' character as the first non white
            space character in the record. Data records must contain
            at least one field, which is the data value of the sample,
            but may contain many fields---if the record contains many
            fields, then the first field is regarded as the sample's
            time, and the last field as the sample's value at that
            time.

            As a reference on Newton---Raphson Method of root finding,
            see~\cite[pp. 270]{Press}.

            \noindent Note: The derivation for exponential least
            squares fit is:

            \begin{enumerate}

                \item input the value of the time series for each time
                interval, $value(t)$, and store the logarithm of the
                value, ie.:

                \begin{equation}
                    y\left(t\right) = \ln \left(value\left(t\right)\right)
                \end{equation}

                \item compute the least squares fit to $y(t), a + bt$,
                then:

                \begin{equation}
                    \ln \left(y\left(t\right)\right) = b + at
                \end{equation}

                \item exponentiate the values in $y(t)$:

                \begin{eqnarray}
                    fit \left(t\right) & = & e^{b} \cdot e^{at}\\
                                       & = & e^{b + at}
                \end{eqnarray}

            \end{enumerate}

            \noindent where $fit (t)$ is the least squares exponential
            fit.

            \noindent Note: The derivation for exponential least
            squares fit is:

            \begin{eqnarray}
                y\left(t\right) & = & e^{k1 + k2t}\\
                s\left(t\right) & = & \ln \left(\frac{e^{k1 + k2t}}{e^{k1 + k2 \left(t - 1\right)}}\right)\\
                                & = & \ln \left(e^{k1 + k2t - k1 - k2t + k2}\right)\\
                                & = & \ln \left(e^{k2}\right)\\
                                & = & k2
            \end{eqnarray}

            \noindent And for the binary least squares fit, letting $k
            = k2$:

            \begin{eqnarray}
                      y\left(t\right) & = & e^{k1 + k2t}\\
                      s\left(t\right) & = & \ln \left(\frac{e^{k1 + k2t}}{e^{k1 + k2 \left(t - 1\right)}}\right)\\
                                      & = & \ln \left(e^{k1 + k2t - k1 - k2t + k2}\right)\\
                                      & = & \ln \left(e^{k2}\right)\\
                                      & = & k2\\
                               e^{xt} & = & 2^{kt}\\
                                a^{t} & = & 2^{kt}\\
                                    a & = & 2^{k}\\
                 k \ln \left(2\right) & = & \ln \left(a\right)\\
                                    k & = & \frac{\ln \left(a\right)}{\ln \left(2\right)}
            \end{eqnarray}

            \noindent Note: The derivation for calculating the Shannon
            probability, given the Shannon information capacity, where
            the information capacity is the exponent derived from the
            least squares fit to the values of the time series,
            divided by the natural logarithm of
            two. See~\cite[pp. 128, 151]{Schroeder}.  Uses
            Newton-Raphson method for an iterative solution for the
            probability, p.

            \begin{eqnarray}
                            C\left(p\right) & = & 1 + p ln_2 \left(p\right) + \left(1 - p\right) ln_2 \left(1 - p\right)\\
                            C\left(p\right) & = & 1 + p \left(\frac{ln \left(p\right)}{ln \left(2\right)}\right) + \left(1 - p\right) \left(\frac{ln \left(1 - p\right)}{ln \left(2\right)}\right)\\
                            C\left(p\right) & = & \left(\frac{1}{ln \left(2\right)}\right) \left(ln \left(2\right) + p ln \left(p\right) + \left(1 - p\right) ln \left(1 - p\right)\right)\\
                            C\left(p\right) & = & \left(\frac{1}{ln \left(2\right)}\right) \left(ln \left(2\right) + p ln \left(p\right) + ln \left(1 - p\right) - p ln \left(1 - p\right)\right)\\
                \frac{dC\left(p\right)}{dp} & = & \left(\frac{1}{ln \left(2\right)}\right) \left(1 + ln \left(p\right) - (\frac{1}{\left(1 - p\right)}) - \left(ln \left(1 - p\right) - (\frac{p}{\left(1 - p\right)})\right)\right)\\
                                            & = & \left(\frac{1}{ln \left(2\right)}\right) \left(1 + ln \left(p\right) - (\frac{1}{\left(1 - p\right)} - ln \left(1 - p\right) + \left(\frac{p}{\left(1 - p\right)}\right)\right)\\
                                            & = & \left(\frac{1}{ln \left(2\right)}\right) \left(ln \left(p\right) - ln \left(1 - p\right) + \left(\frac{p}{\left(1 - p\right)}\right) - \left(\frac{1}{\left(1 - p\right)}\right)\right)\\
                                            & = & \left(\frac{1}{ln \left(2\right)}\right) \left(1 + ln \left(p\right) - ln \left(1 - p\right) + \left(\frac{\left(p - 1\right)}{\left(1 - p\right)}\right)\right)\\
                                            & = & \left(\frac{1}{ln \left(2\right)}\right) \left(1 + ln \left(p\right) - ln \left(1 - p\right) - 1\right)\\
                                            & = & \left(\frac{1}{ln \left(2\right)}\right) \left(ln \left(p\right) - ln \left(1 - p\right)\right)
            \end{eqnarray}

            An example output from the {\it tsshannonwindow}\/ program
            appears in Figure~\ref{shannonwindowexample}.

            \begin{figure}[ht]
                \begin{center}
                    \begin{minipage}[t]{0.9\textwidth}
                        \center{\fbox{\input{../utilities/test/tscoin.tsshannonwindow-a-b-c-d-e-f-g-h.tex}}}
                        \caption[Example output of the {\it
                            tsshannonwindow}\/ program]{Example output
                            of the {\it tsshannonwindow}\/ program,
                            the input was produced by the the {\it
                            tscoin}\/ program, with a Shannon
                            probability of 0.6, and is shown in
                            Figure~\ref{coinexample} in
                            Section~\ref{tscoinexample}.}
                        \label{shannonwindowexample}
                    \end{minipage}
                \end{center}
            \end{figure}

        \subsection{tspole}
            \subidx{programs}{tspole}
            \subidx{tspole}{program}

            Source tspole.c, is for single pole low pass filtering of a
            time series. The single pole low pass filter is
            implemented from the following discrete time equation:

            \begin{equation}
                v_{n + 1} = I \cdot k2 + v_{n} \cdot k1
            \end{equation}

            \noindent where $I$ is the value of the current sample in
            the time series, $v_{n}$ are the value of the output time
            series, and $k1$ and $k2$ are constants determined from
            the following equations:

            \begin{equation}
                k1 = e^{-2 \cdot p \cdot \pi}
            \end{equation}

            \noindent and

            \begin{equation}
                k2 = 1 - k1
            \end{equation}

            \noindent where $p$ is a constant that determines the
            frequency of the pole-a value of unity, the default,
            places the pole at the sample frequency of the time
            series.

            The input file structure is a text file consisting of
            records, in temporal order, one record per time series
            sample.  Blank records are ignored, and comment records
            are signified by a `\#' character as the first non white
            space character in the record. Data records must contain
            at least one field, which is the data value of the sample,
            but may contain many fields---if the record contains many
            fields, then the first field is regarded as the sample's
            time, and the last field as the sample's value at that
            time.

            This program is based on~\cite[pp. 11]{Conover}.

            An example output from the {\it tspole}\/ program appears
            in Figure~\ref{poleexample}.

        \subsection{tsdft}
            \subidx{programs}{tsdft}
            \subidx{tsdft}{program}
            \idx{Fourier analysis}

            Source tsdft.c, is for taking the Discrete Fourier
            Transform (power spectrum) of a time series.

            The input file structure is a text file consisting of
            records, in temporal order, one record per time series
            sample.  Blank records are ignored, and comment records
            are signified by a `\#' character as the first non white
            space character in the record. Data records must contain
            at least one field, which is the data value of the sample,
            but may contain many fields---if the record contains many
            fields, then the first field is regarded as the sample's
            time, and the last field as the sample's value at that
            time.

            Note: the algorithm used in this program is a modified
            version of the program dft.c, written and~\copyright~1985
            Nicholas B. Tufillaro.

            An example output from the {\it tsdft}\/ program appears
            in Figure~\ref{dftexample}.

            \begin{figure}[ht]
                \begin{center}
                    \begin{minipage}[t]{0.45\textwidth}
                        \epsfxsize=1.0\linewidth
                        \epsffile{../utilities/test/tscoin.tspole-p0.01.eps}
                        \caption[Example output of the {\it tspole}\/
                            program]{Example output of the {\it
                            tspole}\/ program, the input was produced
                            by the {\it tscoin}\/ program, with a
                            Shannon probability of 0.6, and is shown
                            in Figure~\ref{coinexample} in
                            Section~\ref{tscoinexample}. The pole
                            frequency was set at $\frac{sample
                            frequency}{100}$.}
                        \label{poleexample}
                    \end{minipage}
                    \hfill
                    \begin{minipage}[t]{0.45\textwidth}
                        \epsfxsize=1.0\linewidth
                        \epsffile{../utilities/test/tscoin.tsdft-s.eps}
                        \caption[Example output of the {\it tsdft}\/
                            program]{Example output of the {\it
                            tsdft}\/ program, the input was produced
                            by the {\it tscoin}\/ program, with a
                            Shannon probability of 0.6, and is shown
                            in Figure~\ref{coinexample} in
                            Section~\ref{tscoinexample}. A plot of
                            $\frac{1}{f^2}$ is superimposed on the
                            plot.}
                        \label{dftexample}
                    \end{minipage}
                \end{center}
            \end{figure}

        \subsection{tsmath}
            \subidx{programs}{tsmath}
            \subidx{tsmath}{program}

            Source tsmath.c, for for performing arithmetic operations
            on each element in a time series. The resultant time
            series is printed to stdio.

            The input file structure is a text file consisting of
            records, in temporal order, one record per time series
            sample.  Blank records are ignored, and comment records
            are signified by a `\#' character as the first non white
            space character in the record. Data records must contain
            at least one field, which is the data value of the sample,
            but may contain many fields---if the record contains many
            fields, then the first field is regarded as the sample's
            time, and the last field as the sample's value at that
            time.

            An example output from the {\it tsmath}\/ program appears
            in Figure~\ref{mathexample}.

        \subsection{tsdeterministic}
            \subidx{programs}{tsdeterministic}
            \subidx{tsdeterministic}{program}

            Source tsdeterministic.c, is for determining if a time
            series was created by a deterministic mechanism. The idea
            is place each element of a time series in an array
            structure that contains the element and the next element
            in the time series, and then sort the array. The array is
            output and may be plotted. For example, using the program
            {\it tsdlogistic}\/ to make a discrete time series of the
            logistic, (quadratic function,) with the command
            ``tsdlogistic -a 4 -b -4 1000 > XXX'' and then using this
            program on the output file, XXX, will result in a plot of
            a parabola. See~\cite[pp. 745]{Peitgen}.

            The input file structure is a text file consisting of
            records, in temporal order, one record per time series
            sample.  Blank records are ignored, and comment records
            are signified by a `\#' character as the first non white
            space character in the record. Data records must contain
            at least one field, which is the data value of the sample,
            but may contain many fields---if the record contains many
            fields, then the first field is regarded as the sample's
            time, and the last field as the sample's value at that
            time.

            An example output from the {\it tsdeterministic}\/ program
            appears in Figure~\ref{deterministicexample}.

            \begin{figure}[ht]
                \begin{center}
                    \begin{minipage}[t]{0.45\textwidth}
                        \epsfxsize=1.0\linewidth
                        \epsffile{../utilities/test/tscoin.tsmath-l.eps}
                        \caption[Example output of the {\it tsmath}\/
                            program]{Example output of the {\it
                            tsmath}\/ program, taking the logarithm of
                            the file produced by the {\it tscoin}\/
                            program, with a Shannon probability of
                            0.6, which is shown in
                            Figure~\ref{coinexample} in
                            Section~\ref{tscoinexample}.}
                        \label{mathexample}
                    \end{minipage}
                    \hfill
                    \begin{minipage}[t]{0.45\textwidth}
                        \epsfxsize=1.0\linewidth
                        \epsffile{../utilities/test/tsdlogistic.tsdeterministic.eps}
                        \caption[Example output of the {\it
                            tsdeterministic}\/ program]{Example output
                            of the {\it tsdeterministic}\/ program,
                            the input was produced by the {\it
                            tsdlogistic}\/ program, using the command
                            ``tsdlogistic -a 4 -b -1 1000 >
                            filename,'' which is shown in
                            Figure~\ref{dlogisticexample} in
                            Section~\ref{tsdlogisticexample}.}
                        \label{deterministicexample}
                    \end{minipage}
                \end{center}
            \end{figure}

        \subsection{tsstatest}
            \subidx{programs}{tsstatest}
            \subidx{tsstatest}{program}
            \subidx{time series}{statistical estimate}
            \subidx{statistical estimate}{time series}

            Source tsstatest.c, for making a statistical estimation of
            a time series. The number of samples, given the maximum
            error estimate, and the confidence level required is
            computed for both the standard deviation, and the mean.

            The input file structure is a text file consisting of
            records, in temporal order, one record per time series
            sample.  Blank records are ignored, and comment records
            are signified by a `\#' character as the first non white
            space character in the record. Data records must contain
            at least one field, which is the data value of the sample,
            but may contain many fields---if the record contains many
            fields, then the first field is regarded as the sample's
            time, and the last field as the sample's value at that
            time.

            Consider the following formula for determination of the
            Shannon Probability, $P$, of an equity market time series,
            using the average and root mean square of the normalized
            increments, $avg$, and, $rms$, respectively, by rearranging
            Equation~\ref{fopt3}:

                \begin{equation}
                    P = \frac{\frac{avg}{rms} + 1}{2}
                \end{equation}

            \noindent which is useful in the determination of the optimal
            fraction of capital, $f$, to invest in a stock, from
            Equation~\ref{optimumequation}:

                \begin{equation}
                   f = 2P - 1
                \end{equation}

            The objective is to estimate how large the data set has to be
            for determining $P$ to a given accuracy, possibly using
            statistical estimates of how many data points are required for
            a given confidence level that the error is less than a
            specific value.

            Suppose we have a confidence level, $0 < c < 1$, that a value
            is within, plus or minus, an error level, $e$. What this
            means, for example if $c = 0.9$, and $e = 0.1$, is that for
            $90\%$ of the cases, the value will be within the limits of
            $\pm e$, or, $5\%$ of the time, on the average, it will be
            less than $-e$, and $5\%$ of the time more than $+e$.

            The error level for $avg$, $e_{avg}$,for a given confidence
            level, will be:

                \begin{equation}
                    e_{avg} = k \frac{rms}{\sqrt{n}}
                \end{equation}

            \noindent where $n$ is the number of records in the data set,
            and $k$ is a function involving a normal distribution. The
            error level for $rms$, for the same given confidence level,
            will be:

                \begin{equation}
                    e_{rms} = k \frac{rms}{\sqrt{2n}}
                \end{equation}

            \noindent where $k$ is identical in both cases. Also, the
            number of records required for a given error level would be:

                \begin{equation}
                    n_{avg} = \left(\frac{(rms \cdot k)}{e_{rms}}\right)^2
                \end{equation}

            \noindent and

                \begin{equation}
                    n_{rms} = \frac{1}{2} \left(\frac{(rms \cdot k)}{e_{rms}}\right)^2
                \end{equation}

            \noindent where $k$ is the same as above.

            For equity market indices, a typical value for $rms$ would be
            $0.01$, and $0.0003$ for $avg$. This is probably typical for
            many stocks, however, high gain stocks, in a ``bull'' market
            can have an $rms$ of $0.04$, and an $avg$ of $0.005$.

            The value of $k$ can be determined from standard statistical
            tables, as shown in table~\ref{CLVSSL1}, where $k = $ sigma
            level, for a confidence level, $c$.

            \begin{small}
                \begin{table}[ht]
                    \begin{center}
                        \caption[Confidence Level vs. $\sigma$ Level]{Confidence Level vs. $\sigma$ Level.}
                        \begin{tabular}{|l|l|} \hline
                            Confidence Level, $c$ & $\sigma$ level \\ \hline
                            (\%) & \hspace{0.01in} \\ \hline
                            50      &    0.67 \\
                            68.27   &    1.00 \\
                            80      &    1.28 \\
                            90      &    1.64 \\
                            95      &    1.96 \\
                            95.45   &    2.00 \\
                            99      &    2.58 \\
                            99.73   &    3.00 \\ \hline
                        \end{tabular}
                        \label{CLVSSL1}
                    \end{center}
                \end{table}
            \end{small}

            Note that for a given confidence level:

            \begin{eqnarray}
            \frac{avg}{rms} & = & \frac{avg \pm k \frac{rms}{\sqrt{n}}}{rms \pm k \frac{rms}{\sqrt{2n}}} \\
                            & = & \frac{\frac{avg}{rms} \pm k \frac{1}{\sqrt{n}}}{1 \pm k \frac{1}{4\sqrt{n}}}
            \end{eqnarray}

            Now, consider the specific example of $avg$ and $rms$ for an
            exponential function. In this specific case, $avg = rms$, and
            $\frac{avg}{rms} = 1$. Since $k$ is assumed to be a function
            of a normally distributed random variable, the error in the
            ratio $\frac{avg}{rms}$ as a function of the data set size,
            {n}, can be found by superposition, and adding the
            contributing error values as a function of $n$ for both $rms$
            and $avg$ root mean square, or:

                \begin{equation}
                    \sqrt{1^2 + \left(\frac{1}{4}\right)^2} = 1.030776406
                \end{equation}

            \noindent or:

                \begin{equation}
                    \frac{avg}{rms} \sim \frac{avg}{rms} \pm 1.03 \frac{1}{\sqrt{n}} k \sim \frac{avg}{rms} \pm \frac{1}{\sqrt{n}} k
                \end{equation}

            \noindent where $k$ is determined from the table, above. In
            this specific case, where $avg = rms$:

                \begin{equation}
                    \frac{avg}{rms} \sim \frac{avg}{rms} \left(1 \pm \frac{1}{\sqrt{n}} k\right)
                \end{equation}

            An interpretation of what this means is that, given a data set
            size, $n$, and a confidence level of, say $90\%$, then $90\%$
            of the time, our measurements of $\frac{avg}{rms}$, would fall
            within an error level of $\pm 1.64 \frac{1}{\sqrt{n}}$, ie.,
            $5\%$ of the time it would be greater than the error value,
            and $5\%$ of the time, it would be lower than the error
            value. In general, the concern is with the lower error value
            since from the equation:

                \begin{equation}
                    P = \frac{\frac{avg}{rms} + 1}{2}
                \end{equation}

            \noindent (at least in this specific case where $avg = rms$,)
            that a $90\%$ confidence level would imply that there is a
            $5\%$ chance of the real value $\frac{avg}{rms}$ being zero is
            where:

                \begin{equation}
                    \frac{k}{\sqrt{n}} = 1
                \end{equation}

            \noindent or:

                \begin{equation}
                    \frac{1.64}{\sqrt{n}} = 1
                \end{equation}

            \noindent or $n = 2.6896 \sim 3$.

            What this means is that, if we repeat the experiment of
            finding $3$ records in a row that have $rms = avg$, with
            neither equal to zero, many times, that we would loose money
            in $5\%$ of the cases, making the measured Shannon
            probability, $P$, unity, and the estimated Shannon
            probability, $0.95$, eg., we should consider the Shannon
            probability as $0.95$ in this specific case---ie., it would be
            ill advised to invest all of the capital in such a scenario,
            since, sooner or later, all of the capital would be lost, (on
            average, by the 20'th game.)

            This implies a simple methodology. Measure $avg$ and $rms$,
            and compute the Shannon probability. Decease that probability
            by a factor---ie., one minus the confidence level, divided by
            two---that the wager could be a loosing proposition, based on
            the estimates that $avg$ could be zero, (which is a function
            of the confidence level, and the number of records in the data
            set.) This, conceivably, could provide a quantitative estimate
            on the number of records required in a data set.

            Note that if $\frac{avg}{rms}$ is measured at $0.9$, then:

                \begin{equation}
                    \frac{1.64}{\sqrt{n}} = 0.9
                \end{equation}

            \noindent for the same confidence level of $0.9$, or

                \begin{equation}
                    n = 3.32
                \end{equation}

            \noindent and:

            \begin{small}
                \begin{table}[ht]
                    \begin{center}
                        \caption[Shannon Probability vs. Data Set Size]{Shannon Probability vs. Data Set Size.}
                        \begin{tabular}{|r|r|r|r|} \hline
                            $\frac{avg}{rms}$ & $n$ & $P_{measured}$ & $P$\\ \hline
                            1.0  &   2.7  & 1.00  &  0.95 \\
                            0.9  &   3.3  & 0.95  &  0.90 \\
                            0.8  &   4.2  & 0.90  &  0.86 \\
                            0.7  &   5.5  & 0.85  &  0.81 \\
                            0.6  &   7.5  & 0.80  &  0.76 \\
                            0.5  &  10.8  & 0.75  &  0.71 \\
                            0.4  &  16.8  & 0.70  &  0.67 \\
                            0.3  &  29.9  & 0.65  &  0.62 \\
                            0.2  &  67.2  & 0.60  &  0.57 \\
                            0.1  & 268.9  & 0.55  &  0.52 \\
                            0.05 & 1075.8 & 0.53  &  0.50 \\ \hline
                        \end{tabular}
                        \label{SPVSDSS1}
                    \end{center}
                \end{table}
            \end{small}

            for the same confidence level $0.9$. What the table means is
            that if you have a stock price time series of $67$ records,
            then the minimum measured Shannon probability must be at least
            $0.6$---and the wagering strategy should use the Shannon
            probability of $0.57$---and the minimum number of records used
            to measure $avg$ and $rms$ is $67$. Additionally, a stock time
            series with a Shannon probability of $0.53$ should be measured
            using not less than $1076$ records, and no wager should be
            made, unless the measurements involve substantially more than
            $1076$ records. In general, the Shannon probability of almost
            all stock time series fall, inclusively, in this range. $67$
            business days is, approximately, $13.4$ weeks, or little more
            than a calendar quarter.  $1076$ business days is slightly
            longer than four calendar years.

            Note that~\cite[pp. 83]{Peters:CAOITCM}
            referencing~\cite[pp. 179]{Feder}, the claim is made that
            2500 records is the minimum size of the data set for using
            fractal analytical methodologies. Note that a data set of
            this size would have, with an $\frac{avg}{rms}$ of
            $0.5$---which is ``typical'' for a stock time series, a
            Shannon probability error level that is approximately
            $1\%$, since it lies between $2$ and $3$ sigma, and $c$
            would be approximately $0.99$. This would seem to be
            consistent with the empirical arguments of both Peters and
            Feder, although Peters implies that less could be used if
            the system being analyzed is ``chaotic'' in nature, and
            one ``cycle'' of the system's, apparently, ``strange
            attractor'' is less than $2500$ time units. This analysis
            would seem to be consistent with the observations of these
            authors, provided that it is a requirement that the
            measured Shannon probability be used to calculate the
            optimum wager fraction.

            What this analysis would tend to suggest is that, although
            Feder's and Peter's arguments seem to be confirmed, that there
            may, also, be other viable solutions for data sets, (or
            fragments thereof,) that are very much smaller, provided that
            the measured Shannon probability of the data set, or segment,
            is sufficiently large---for example, a stock that has a time
            series fragment that has $5$ out of $6$ upward movements may
            prove to be a viable investment opportunity at a measured
            Shannon probability that is greater than $0.85$, ($\frac{5}{6}
            =$ a Shannon probability of $0.833 \sim 0.85$,) if played at a
            Shannon probability as high as $0.8$, but no higher.

            For example, using a Shannon probability, $P$, of $0.51$
            for the {\it tscoins}\/ program, to provide an input
            fractal time series for the {\it tsstatest}\/ program, and
            iterating, indicates that for a standard deviation of
            $0.020000$, with a confidence level of $0.960784$ that the
            error did not exceed $0.020000$, $3$ samples would be
            required.

            Since the Shannon probability is calculated directly from
            the standard deviation, (ie., $rms$ = root mean square of
            the normalized increments,) the maximum error can be
            calculated:

            \begin{equation}
                \frac{0.5}{0.51} = 0.980392157
            \end{equation}

            which means that a confidence level of $0.960784314$ that
            the error level in the standard deviation is less than
            $0.02$ because standard deviation $= rms = 0.02 - 0.02 =
            0$, which would correspond to a Shannon probability, $P$,
            of $0.5$, and since half the errors outside the range of
            $0.02$ would be negative, (and the other half positive,)
            the confidence level required would be $1 - ((1 -
            0.980392157) \cdot 2)$.

            What this means is that $((1 - 0.960784314) / 2) \cdot
            100$ percent of the time, the actual $rms$ value will be
            sufficiently small to make $P$ equal to, or less than
            $0.5$. This means that $P$ must be decreased by
            $1.960784300$ percent. The reasoning is that after many
            iterations, the measured $P$ would be too small by
            $1.90784300$\% of the time, on average, making the
            measured $P$, over all of the iterations, $0.5$.

            This suggests a dynamic rule: do not wager unless the
            Shannon probability, $P$, is strictly greater than $0.51$,
            as measured on strictly more than $3$ time
            units. Interestingly, the Hurst Coefficient, as measured
            by the {\it tshurst}\/ program, graph of a random walk,
            Brownian motion, or fractional Brownian motion fractals
            indicates that there is significant near term correlations
            for $4$ or less time units. This suggests a dynamic
            trading methodology for equities.

            Similar reasoning would indicate that using a value of $P
            = 0.6$ for the {\it tscoins}\/ and {\it tsfraction}\/
            programs to provide input to the {\it tsstatest}\/ program
            with a confidence level of $0.8$, and an error of $0.12$,
            (ie., 10\% of the time the value of $P$ would be less than
            $0.9 \cdot 0.6 = 0.54$, where $0.2 - 0.12 = 0.08$, and
            $0.54 = \frac{0.08 + 1}{2}$,) would require a minimum of
            $3$ records. The fraction of capital wagered should be $2
            \cdot 0.54 - 1 = 0.08$.

            To review what has been presented so far, we really are
            not confident that we know the value of the Shannon
            probability, $P$, until we have sufficiently many records,
            $n$. One way of addressing this issue is to wait to make a
            wager until we do. But this strategy has an ``opportunity
            cost,'' since, approximately $50$\% of the time, we would
            not have made an investment when we should have. Note that
            since investing in equities is not a $100$\% assured
            proposition, we only invest a fraction of our capital,
            $f$, where $f = 2P - 1$. Since investing with a data set
            size that is insufficient, ie., $n$ is too small, lowers
            the probability of the wins, the Shannon probability, $P$,
            will have to be lowered to maintain the optimum wager
            fraction of the capital. We can compute the value that the
            Shannon probability, $P$, must be lowered to account for
            this.

            The relationship between the Shannon probability, $P$, and
            the root mean square of the normalized increments of a
            time series, $rms$, is:

            \begin{equation}
                P = \frac{rms + 1}{2}
            \end{equation}

            Let the error, $e$, in $rms$ created by an insufficient
            data set size be:

            \begin{equation}
                e = rms - rms'
            \end{equation}

            \noindent where $0 \leq rms' \leq rms$. This means that
            although $rms$ was measured it could be as low as
            $rms'$. The confidence level that $rms$ is not less than
            $rms'$ can be found by statistical estimate. The Shannon
            probability, $P'$, associated with $rms'$ is:

            \begin{equation}
                P' = \frac{rms' + 1}{2}
            \end{equation}

            $P'$ is the Shannon probability if the root mean square
            value of the normalized increments of the time series is
            $rms'$.

            Since we want to alter the measured Shannon probability,
            $P$, to accommodate the error created by a insufficient
            data set size, we multiply $P$ by the confidence level
            that the real value of $P$ is not less than $P'$, or the
            confidence level, $C$, is:

            \begin{equation}
                C = \frac{P'}{P}
            \end{equation}

            The reasoning is that a value of $C$, say $0.9$, means
            that the root mean square value of the increments could be
            below the measured value, $rms$, by an amount $e$ for
            $5$\% of the time, and above $rms$ by an amount $e$ for
            $5$\% of the time, so that:

            \begin{equation}
                P' = CP
            \end{equation}

            \noindent Substituting:

            \begin{equation}
                CP = \frac{rms' + 1}{2}
            \end{equation}

            \noindent and solving for $rms'$:

            \begin{equation}
                rms' = 2CP - 1
            \end{equation}

            \noindent or:

            \begin{equation}
                e = rms - \left(2CP - 1\right) = rms - 2CP + 1
            \end{equation}

            \noindent and substituting for $rms$, where $rms = 2P -
            1$:

            \begin{equation}
                e  = 2P - 1 - 2CP + 1 = 2P - 2CP = 2P\left(1 - C\right)
            \end{equation}

            \noindent and substituting $P' = CP$:

            \begin{equation}
                e = 2P - 2P' = 2\left(P - P'\right)
            \end{equation}

            $C$ now has to be adjusted because we are only concerned
            with the values of $rms'$ that are less than $rms$, where:

            \begin{equation}
                c = 1 - 2\left(1 - C\right) = 1 - 2 + 2C = 2C - 1
            \end{equation}

            \noindent but since $C = P' / P$:

            \begin{equation}
                c = \frac{2P'}{P} - 1
            \end{equation}

            \noindent or we have:

            \begin{equation}
                e = 2\left(P - P'\right)
            \end{equation}

            \noindent and:

            \begin{equation}
                c = \frac{2P'}{P} - 1
            \end{equation}

            \noindent which are the two general equations for use of
            this program for trading equities.

            Making a plot of these equations, of $P'$ vs. $n$ for
            various $P$ presents an interesting conjecture. The graph
            can be crudely approximated by a single pole filter, with
            a pole at $0.033$, ie., using the program {\it tscoins}\/
            with a -p 0.6 argument to simulate an equity value time
            series, and the program {\it tsinstant}\/, with the -s
            option, to calculate the instantaneous Shannon probability
            of the time series, followed by the program {\it tspole}\/
            with a -p 0.033 argument, would output, approximately,
            $P'$. The $P'$ tends to under wager for $t \le 7$, and
            over wager for $t \ge 0.7$. The approximation is simple,
            but crude. Interestingly, using the program {\it
            tshurst}\/ on the same time series indicates that there is
            good correlation for $t \le 5$, and if this temporal range
            is of interest, this simple solution may prove adequate
            for non-rigorous requirements. Additionally, perhaps using
            the {\it tsmath}\/ program, the output of the {\it
            tspole}\/ program could have $0.5$ subtracted, multiplied
            by, say, $0.85$, and then the $0.5$ re-added to extend the
            usefulness to approximately $100$ business days. The
            accuracy over this range is approximately $\pm 0.01$ out
            of $0.55$. Naturally, after very many days, for example,
            if $P = 0.6$, $P'$ would still be $0.585$, creating a long
            term error in $rms$ of $0.2 - 0.17 = 0.03$. Note that the
            error created in the exponential growth of the capital
            would be $0.04 - 0.0289$. A substantial long term error.
            Alternately, perhaps a recursive feed-forward technique
            could be implemented that would allow the pole frequency
            to be selected for far term compatibility with the
            statistical estimate, while at the same time approximating
            the near term. Naturally, this, also, should not be
            considered a substitute for statistical estimates, but
            using statistical estimates would probably require a
            recursive procedure, and that is a formidable proposition.

            This program will require finding the value of the normal
            function, given the standard deviation. The method used is
            to use Romberg/trapezoid integration to numerically solve
            for the value.

            This program will require finding the functional inverse
            of the normal, ie., Gaussian, function. The method used is
            to use Romberg/trapezoid integration to numerically solve
            the equation:

            \begin{equation}
                F\left(x\right) = \int_{0}^{x} \frac{1}{2\pi} e^{\frac{-t^{2}}{2}} dt + 0.5
            \end{equation}

            \noindent which has the derivative:

            \begin{equation}
                f\left(x\right) = \frac{1}{2\pi} e^{\frac{-x^{2}}{2}}
            \end{equation}

            \noindent Since $F(x)$ is known, and it is desired to find
            $x$,

            \begin{equation}
                F\left(x\right) - \int_{0}^{x} \frac{1}{2\pi} e^{\frac{-t^{2}}{2}} dt + 0.5 = P\left(x\right) = 0
            \end{equation}

            \noindent and the Newton-Raphson method of finding roots
            would be:

            \begin{equation}
                P_{n + 1} = P_{n} - \frac{P\left(x\right)}{f\left(x\right)}
            \end{equation}

            An example output from the {\it tsstatest}\/ program
            appears in Figure~\ref{statestexample}.

            \begin{figure}[ht]
                \begin{center}
                    \begin{minipage}[t]{\textwidth}
                        \center{\fbox{\parbox{0.9\textwidth}{\input{../utilities/test/tsfBm-h0.5.tsstatest-f0.1-c0.9-i.tex}}}}
                        \caption[Example output of the {\it
                            tsstatest}\/ program]{Example output of
                            the {\it tsstatest}\/ program, the input
                            was produced by the the {\it tsfBm}\/
                            program, with a Hurst coefficient of
                            0.5. The {\it tsstatest}\/ parameters are
                            for a confidence level of 90\%, with an
                            error estimate of $\pm$ 10\%, on the
                            distribution of the normalized
                            increments.}
                        \label{statestexample}
                    \end{minipage}
                \end{center}
            \end{figure}

        \subsection{tsshannonaggregate}
            \subidx{programs}{tsshannonaggregate}
            \subidx{tsshannonaggregate}{program}

            Source tsshannonaggregate.c, aggregate Shannon probability
            of many concurrent Shannon probabilities.

            Consider gambling on two unfair coin tossing games, at the
            same time, one game having a Shannon probability of
            $0.55$, and the other having a Shannon probability of
            $0.65$. Assuming that the coins in both games are tossed
            concurrently for each iteration of the game, the
            combinatorics of the possible outcomes of wins and losses
            in each iteration are:

            \begin{center}
                \begin{tabular}{|c|c|c|c|} \hline
                    outcomes & probability & fraction & average\\ \hline
                    $ll$: & $0.157500$ & $-0.400000$ & $ -0.063000$\\
                    $wl$: & $0.192500$ & $-0.200000$ & $ -0.038500$\\
                    $lw$: & $0.292500$ & $ 0.200000$ & $  0.058500$\\
                    $ww$: & $0.357500$ & $ 0.400000$ & $  0.143000$\\ \hline
                \end{tabular}
            \end{center}

            \noindent where $l$ is a loss, and $w$ is a win, and the
            probability is calculated by multiplying the individual
            probabilities of a loss or win for the respective coins,
            ie., for both coins to win, the probability would be $0.55
            \cdot 0.65 = 0.3575$.  ($1 - P$ is used for the
            probability of a loss for each coin.) The fraction is the
            fraction of capital waged on an individual game, and is
            computed as optimal, from the equation $2P - 1$, where $P$
            is the Shannon probability of the individual unfair coin
            and is either $0.55$ or $0.65$. The average is computed as
            the product of the probability and the fraction.

            What this means is that $35.75$\% of the time, a win-win
            outcome will be observed in the iterated games, and
            $15.75$\% of the time, a lose-lose outcome will be
            observed. The amount won in the win-win scenario will be
            the sum of the fractions wagered on each coin, which is
            $(2 \cdot 0.55 - 1) + (2 \cdot 0.65 - 1) = 0.1 + 0.3 =
            0.4$. The product of this fraction and probability is the
            contribution over many plays to the capital do to this
            outcome. Summing these averages for the different outcomes
            is the average over many plays of the capital growth by
            playing both games, and is numerically identical to the
            sum of the average of the normalized increments of both
            games.

            Since the average and root mean square of the normalized
            increments are related by:

            \begin{equation}
                rms = sqrt (average)
            \end{equation}

            \noindent squaring the average will be the root mean
            square of the normalized increments, or:

            \begin{center}
                \begin{tabular}{|c|c|c|} \hline
                    Average & rms & Shannon probability\\ \hline
                    0.100000 & 0.316228 & 0.658114\\ \hline
                \end{tabular}
            \end{center}

            \noindent where the Shannon probability, P, is computed
            by:

            \begin{equation}
                P = \frac{rms + 1}{2} = \frac{1.316228}{2} = 0.658114
            \end{equation}

            The implication is that the two concurrent unfair coin
            tossing games could be ``modeled'' as a single game with a
            Shannon probability of $0.658114$.

            Although it is generally more expedient just to sum, root
            mean square, the individual root mean square of the
            normalized increments of each game, (where $f = rms = 2P -
            1$,) and then compute the Shannon probability by:

            \begin{eqnarray}
                P & = & \frac{\sqrt{\left(\left(2 \cdot 0.55\right) - 1\right)^2 + \left(\left(2 \cdot 0.65\right) - 1\right)^2} + 1}{2}\\
                  & = & \frac{\sqrt{0.1^2 + 0.3^2} + 1}{2}\\
                  & = & \frac{\sqrt{0.01 + 0.09} + 1}{2}\\
                  & = & \frac{\sqrt{0.1} + 1}{2}\\
                  & = & \frac{0.316227766 + 1}{2}\\
                  & = & \frac{1.316227766}{2}\\
                  & = & 0.658113883
            \end{eqnarray}

            \noindent this program does it with combinatorics.

            An example output from the {\it tsshannonaggregate}\/
            program appears in Figure~\ref{shannonaggregateexample}.

            \begin{figure}[ht]
                \begin{center}
                    \begin{minipage}[t]{\textwidth}
                        \center{\fbox{\parbox{0.9\textwidth}{\input{../utilities/test/tsshannonaggregate-p_0.55_0.65.tex}}}}
                        \caption[Example output of the {\it
                            tsshannonaggregate}\/ program]{Example
                            output of the {\it tsshannonaggregate}\/
                            program, with verbose print option, and
                            Shannon probabilities of $0.55$ and
                            $0.65$.}
                        \label{shannonaggregateexample}
                    \end{minipage}
                \end{center}
            \end{figure}

        \subsection{tsunfraction}
            \subidx{programs}{tsunfraction}
            \subidx{tsunfraction}{program}

            Source tsunfraction.c, is for making a cumulative sum of
            the fraction of change in a time series. The value of a
            sample in the time series is multiplied by the running
            cumulative sum of the time series, and added to the
            running sum of the time series. The resultant time series
            is printed to stdout. (This program is the inverse of the
            {\it tsfraction}\/ program.) Note that:


                \begin{center}
                    tsfraction data | tsunfraction \\
                \end{center}


            \noindent does nothing. An interesting application of this
            program is:

                \begin{center}
                    tsgaussian -t 10000 | tsmath -t -m 0.01 | tsmath -t -a 0.0003 | tsunfraction
                \end{center}

            \noindent which would manufacture a data file with
            statistics that are similar to equity market indices.

            The input file structure is a text file consisting of
            records, in temporal order, one record per time series
            sample.  Blank records are ignored, and comment records
            are signified by a `\#' character as the first non white
            space character in the record. Data records must contain
            at least one field, which is the data value of the sample,
            but may contain many fields---if the record contains many
            fields, then the first field is regarded as the sample's
            time, and the last field as the sample's value at that
            time.

            An example output from the {\it tsunfraction}\/ program
            appears in Figure~\ref{unfractionexample}.

        \subsection{tsinstant}
            \subidx{programs}{tsinstant}
            \subidx{tsinstant}{program}
            \idx{Shannon probability}
            \idx{average of normalized increments}
            \idx{root mean square of normalized increments}
            \subidx{Shannon probability}{instantaneous computation of}
            \subidx{average of normalized increments}{instantaneous computation of}
            \subidx{root mean square of normalized increments}{instantaneous computation of}
            \subidx{instantaneous computation}{Shannon probability}
            \subidx{instantaneous computation}{average of normalized increments}
            \subidx{instantaneous computation}{root mean square of normalized increments}
            \idx{normalized increments}
            \idx{time series}

            Source tsinstant.c, for finding the instantaneous fraction
            of change in a time series. The value of a sample in the
            time series is subtracted from the previous sample in the
            time series, and divided by the value of the previous
            sample. For Brownian motion, random walk fractals, the
            absolute value of the instantaneous fraction of change is
            also the root mean square of the instantaneous fraction of
            change\footnote{The absolute value of the normalized
            increments, when averaged, is related to the root mean
            square of the increments by a constant. If the normalized
            increments are a fixed increment, the constant is
            unity. If the normalized increments have a Gaussian
            distribution, the constant is $\approx 0.8$ depending on
            the accuracy of of ``fit'' to a Gaussian
            distribution.}. Squaring this value is the average of the
            instantaneous fraction of change, and adding unity to the
            absolute value of the instantaneous fraction of change,
            and dividing by two, is the Shannon probability of the
            instantaneous fraction of change. The values are printed
            to stdout.

            For fractional Brownian motion time series, substantial
            filtering will be required of the output time series. The
            programs {\it tspole}\/ and {\it tsavgwindow}\/ may be
            used, perhaps in a cascade fashion, to implement a
            filtering technique, which potentially could be used in an
            adaptive computational system. Markov techniques may also
            be applicable. Note that in fractal time series, the short
            term correlation, say less than three time units as a
            typical value, is quite high---this can be verified by the
            {\it tshurst}\/ program, eg., filtering, to find the
            average value, over a few time units, may be an
            advantageous strategy in adaptive computational control
            systems.

            The input file structure is a text file consisting of
            records, in temporal order, one record per time series
            sample.  Blank records are ignored, and comment records
            are signified by a `\#' character as the first non white
            space character in the record. Data records must contain
            at least one field, which is the data value of the sample,
            but may contain many fields---if the record contains many
            fields, then the first field is regarded as the sample's
            time, and the last field as the sample's value at that
            time.

            An example output from the {\it tsinstant}\/ program
            appears in Figure~\ref{instantexample}.

            \begin{figure}[ht]
                \begin{center}
                    \begin{minipage}[t]{0.45\textwidth}
                        \epsfxsize=1.0\linewidth
                        \epsffile{../utilities/test/tscoin.tsfraction.tsunfraction.eps}
                        \caption[Example output of the {\it
                            tsunfraction}\/ program]{Example output of
                            the {\it tsunfraction}\/ program,
                            reconstructing the file produced by the
                            {\it tscoin}\/ program, with a Shannon
                            probability of 0.6, which is shown in
                            Figure~\ref{coinexample} in
                            Section~\ref{tscoinexample}, after the
                            program {\it tsfraction}\/ was used to
                            construct the normalized increments.}
                        \label{unfractionexample}
                    \end{minipage}
                    \hfill
                    \begin{minipage}[t]{0.45\textwidth}
                        \epsfxsize=1.0\linewidth
                        \epsffile{../utilities/test/tscoin.tsinstant.eps}
                        \caption[Example output of the {\it
                            tsinstant}\/ program]{Example output of
                            the {\it tsinstant}\/ program, the input
                            file was produced by the {\it tscoin}\/
                            program, with a Shannon probability of
                            0.6, which is shown in
                            Figure~\ref{coinexample} in
                            Section~\ref{tscoinexample}.}
                        \label{instantexample}
                    \end{minipage}
                \end{center}
            \end{figure}

        \subsection{tsrunlength}
            \subidx{programs}{tsrunlength}
            \subidx{tsrunlength}{program}
            \idx{run length}
            \idx{zero free intervals}
            \subidx{time series}{zero free intervals}

            Source tsrunlength.c, is for finding the run lengths of
            zero free intervals in a time series, which is assumed to
            be a Brownian fractal.  The value of each sample in the
            time series is stored, and the run length to a like value
            in the time series is stored. A histogram of the number of
            run lengths of each run length value is printed to stdout
            as tab delimited columns of run length value, positive run
            lengths, negative run lengths, and the sum of both
            positive and negative run lengths, followed by the
            cumulative sum of the positive run lengths, the cumulative
            sum of negative run lengths, and the cumulative sum of
            both positive and negative run lengths.

            The idea is to create a run length structure, that tallies
            how many time intervals a run length was either positive
            or negative, for each element in the time series. When a
            run length transition is made, (ie., when the value of the
            time series has {\it passed}\/ through the value of the
            time series when the run length structure was created,
            from a positive or negative direction,) then the run
            length is tallied into histogram arrays, and the structure
            removed.  See~\cite[pp. 260]{Schroeder}

            As approximations for the probability, $p$, of the run
            lengths, for $t \gg 1$, $p = \frac{1}{2 \cdot
            x^{(\frac{3}{2})}}$, which can be integrated for the
            cumulative probability, $P$, for $t \gg 1$, $P =
            \frac{1}{\sqrt{t}}$. For $t \approx 1$, $P = erf
            (\frac{1}{\sqrt{t}})$.

            Note: there is an issue with this methodology---a run
            length is not considered complete until the value is {\it
            passed}, so, for example, a square wave function input
            will never be tallied, ie., a $1$ to $-1$ to $1$ to $-1$
            to $2$ sequence is a negative run length of $3$ time
            units.

            The input file structure is a text file consisting of
            records, in temporal order, one record per time series
            sample.  Blank records are ignored, and comment records
            are signified by a `\#' character as the first non white
            space character in the record. Data records must contain
            at least one field, which is the data value of the sample,
            but may contain many fields---if the record contains many
            fields, then the first field is regarded as the sample's
            time, and the last field as the sample's value at that
            time.

            An example output from the {\it tsrunlength}\/ program
            appears in Figures~\ref{runlengthexample1}
            and~\ref{runlengthexample2}.

            \begin{figure}[ht]
                \begin{center}
                    \begin{minipage}[t]{0.45\textwidth}
                        \epsfxsize=1.0\linewidth
                        \epsffile{../utilities/test/tsfractional.tsrunlength.cut-f1.4.eps}
                        \caption[Example output of the {\it
                            tsrunlength}\/ program]{Example output of
                            the {\it tsrunlength}\/ program, using
                            100,000 records produced by the {\it
                            tsfractional}\/program. This is a plot of
                            the run length distribution..}
                        \label{runlengthexample1}
                    \end{minipage}
                    \hfill
                    \begin{minipage}[t]{0.45\textwidth}
                        \epsfxsize=1.0\linewidth
                        \epsffile{../utilities/test/tsfractional.tsrunlength.cut-f1.7.eps}
                        \caption[Example output of the {\it
                            tsrunlength}\/ program]{Example output of
                            the {\it tsrunlength}\/ program, using
                            100,000 records produced by the {\it
                            tsfractional}\/ program.This is a plot of
                            the cumulative run length distribution.}
                        \label{runlengthexample2}
                    \end{minipage}
                \end{center}
            \end{figure}

        \subsection{tsrootmean}
            \subidx{programs}{tsrootmean}
            \subidx{tsrootmean}{program}
            \subidx{time series}{root mean}
            \subidx{root mean}{time series}

            Source tsrootmean.c for finding the root mean of a time
            series. The number of consecutive samples of like
            movements in the time series is tallied, and the resultant
            distribution is printed to stdout-a simple random walk
            fractal with a Gaussian/normal distributed increments
            would be the combinatorial probabilities, $0.5$, $0.25$,
            $0.125$, $0.625$, $\ldots$

            The input file structure is a text file consisting of
            records, in temporal order, one record per time series
            sample.  Blank records are ignored, and comment records
            are signified by a `\#' character as the first non white
            space character in the record. Data records must contain
            at least one field, which is the data value of the sample,
            but may contain many fields---if the record contains many
            fields, then the first field is regarded as the sample's
            time, and the last field as the sample's value at that
            time.

            An example output from the {\it tsrootmean}\/ program
            appears in Figure~\ref{rootmeanexample}.

        \subsection{tsrunmagnitude}
            \subidx{programs}{tsrunmagnitude}
            \subidx{tsrunmagnitude}{program}
            \idx{run length magnitude}

            Source tsrunmagnitude.c is for finding the magnitude of
            the run lengths in a time series. The value of each sample
            in the time series is stored, and subtracted from all
            other values in the time series, each point being tallied
            root mean square. The magnitude deviation is printed to
            stdout.

            The input file structure is a text file consisting of
            records, in temporal order, one record per time series
            sample.  Blank records are ignored, and comment records
            are signified by a `\#' character as the first non white
            space character in the record. Data records must contain
            at least one field, which is the data value of the sample,
            but may contain many fields---if the record contains many
            fields, then the first field is regarded as the sample's
            time, and the last field as the sample's value at that
            time.

            An example output from the {\it tsrunmagnitude}\/ program
            appears in Figure~\ref{runmagnitudeexample}.

            \begin{figure}[ht]
                \begin{center}
                    \begin{minipage}[t]{0.45\textwidth}
                        \epsfxsize=1.0\linewidth
                        \epsffile{../utilities/test/tsfBm.tsrootmean.eps}
                        \caption[Example output of the {\it
                            tsrootmean}\/ program]{Example output of
                            the {\it tsrootmean}\/ program, using
                            simulated Hurst coefficients of 0.0, 0.1,
                            0.3, 0.5, 0.7, 0.9, and 1.0, as simulated
                            by the {\it tsfBm}\/
                            program.}
                        \label{rootmeanexample}
                    \end{minipage}
                    \hfill
                    \begin{minipage}[t]{0.45\textwidth}
                        \epsfxsize=1.0\linewidth
                        \epsffile{../utilities/test/tsfBm.tsrunmagnitude.eps}
                        \caption[Example output of the {\it
                            tsrunmagnitude}\/ program]{Example output
                            of the {\it tsrunmagnitude}\/ program,
                            using simulated Hurst coefficients of 0.0,
                            0.1, 0.3, 0.5, 0.7, 0.9, and 1.0, as
                            simulated by the {\it tsfBm}\/ program.}
                        \label{runmagnitudeexample}
                    \end{minipage}
                \end{center}
            \end{figure}

        \subsection{tsshannonvolume}
            \subidx{programs}{tsshannonvolume}
            \subidx{tsshannonvolume}{program}

            Note: Conceptually, this program is used to ``adjust'' the
            Shannon probability of a stock by considering the volumes
            of trade in a time interval.  Unfortunately, the results
            were not encouraging, and the concept was abandoned. It is
            left in the program inventory for future reference.

            Tsshannonvolume.c, is for finding the fundamental Shannon
            probability of a time series, given a stocks value, and
            the number of shares traded, in each time interval.  The
            value of a sample in the time series is divided by the
            volume, and added to the cumulative sum of the samples,
            and the square of the value, after dividing by the volume,
            is added to the sum of the squares to make a new time
            series by dividing both the cumulative sum and the square
            root of the sum of the squares by the number of samples
            for each sample. The new time series is printed to stdout.
            The time series printed to stdout is a tab delimited table
            of:

            \begin{enumerate}

                \item The average of a normalized increment, $avg$,
                and is computed by:

                \begin{equation}
                    avg = \frac{v_{t} - v_{t - 1}}{v_{t - 1}} \cdot \frac{1}{N}
                \end{equation}

                \noindent where $N$ is the trading volume, at time
                $t$.

                \item The root mean square of the normalized
                increment, $rms$, and is computed by:

                \begin{equation}
                    rms = \sqrt{\left(\frac{v_{t} - v_{t - 1}}{v_{t - 1}}\right)^{2} \cdot \frac{1}{N}}
                \end{equation}

                \noindent where $N$ is the trading volume, at time
                $t$.

                \item The Shannon probability, $P$, as computed by:

                \begin{equation}
                    P = \frac{\frac{avg}{rms} + 1}{2}
                \end{equation}

                \item The Shannon probability, $P$, as computed by:

                \begin{equation}
                    P = \frac{\sqrt{avg} + 1}{2}
                \end{equation}

                \item The Shannon probability, $P$, as computed by:

                \begin{equation}
                    P = \frac{rms + 1}{2}
                \end{equation}

            \end{enumerate}

            The input file structure is a text file consisting of
            records, in temporal order, one record per time series
            sample.  Blank records are ignored, and comment records
            are signified by a `\#' character as the first non white
            space character in the record. Data records must contain
            at least two fields, which is the data value of the
            sample, followed by the volume of the sample, but may
            contain many more fields-if the record contains many more
            fields, then the first field is regarded as the sample's
            time, and the next to the last field the value, with the
            last field as the sample's volume at that time.

            Note that since the average of the normalized increments
            of a time sampled time series goes up linearly on the
            number of samples in a sampled interval, and the root mean
            square of the normalized increments goes up with the
            square root of the of the number of samples in a sampled
            interval, it would be reasonable to assume that that the
            average of the normalized increments would go up linearly
            with the trading volume of a stock, and the root mean
            square would to go up with the square root of the trading
            volume.

            If we consider capital, $V$, invested in a savings
            account, and calculate the growth of the capital over
            time:

            \begin{equation}
                V_{t} = V_{t - 1} \left(1 + a_{t}\right)
                \label{summaryappb1}
            \end{equation}

            \noindent where $a_{t}$ is the interest rate at time $t$,
            (usually a constant\footnote{For example, if $a = 0.06$,
            or $6$\%, then at the end of the first time interval the
            capital would have increased to $1.06$ times its initial
            value. At the end of the second time interval it would be
            $\left(1.06\right)^{2}$, and so on. What
            Equation~\ref{summaryappb1} states is that the way to get the
            value, $V$ in the next time interval is to multiply the
            current value by $1.06$. Equation~\ref{summaryappb1} is
            nothing more than a ``prescription,'' or a process to make
            an exponential, or ``compound interest'' mechanism. In
            general, exponentials can always be constructed by
            multiplying the current value of the exponential by a
            constant, to get the next value, which in turn, would be
            multiplied by the same constant to get the next value, and
            so on. Equation~\ref{summaryappb1} is nothing more than a
            construction of $V\left(t\right) = e^{kt}$ where $k =
            \ln\left(1 + a\right)$. The advantage of representing
            exponentials by the ``prescription'' defined in
            Equation~\ref{summaryappb1} is analytical expediency. For
            example, if you have data that is an exponential, the
            parameters, or constants, in Equation~\ref{summaryappb1} can
            be determined by simply reversing the ``prescription,''
            ie., subtracting the previous value, (at time $t - 1$,)
            from the current value, and dividing by the previous value
            would give the exponentiating constant, $\left(1 +
            a_{t}\right)$. This process of reversing the
            ``prescription'' is termed calculating the ``normalized
            increments.'' (Increments are simply the difference
            between two values in the exponential, and normalized
            increments are this difference divided by the value of the
            exponential.)  Naturally, since one usually has many data
            points over a time interval, the values can be averaged
            for better precision---there is a large mathematical
            infrastructure dedicated to precision enhancement, for
            example, least squares approximation to the normalized
            increments, and statistical estimation.}.) In equities,
            $a_{t}$ is not constant, and varies---perhaps being
            negative at certain times, (meaning that the value of the
            equity decreased.)  This fluctuation in an equity's value
            can be represented by modifying $a_{t}$ in
            Equation~\ref{summaryappb1}:

            \begin{equation}
                a_{t} = f_{t} F_{t}
                \label{summaryappb2}
            \end{equation}

            \noindent where the product $f_{t} \cdot F_{t}$ is the
            fluctuation in the equity's value at time $t$.

            An equity's value, over time, is similar to a simple
            tossed coin game~\cite[pp. 128]{Schroeder}, where $f_{t}$
            is the fraction of a gambler's capital wagered on a toss
            of the coin, at time $t$, and $F_{t}$ is a random
            variable\footnote{``Random variable'' means that the
            process, $F_{t}$, is random in nature, ie., there is no
            possibility of determining what the next value will
            be. However, $F_{t}$ can be analyzed using statistical
            methods~\cite[pp. 163]{Feder},~\cite[pp. 128]{Schroeder}. For
            example, $F_{t}$ typically has a Gaussian distribution for
            equity values~\cite[pp. 249]{Crownover}, in which case the
            it is termed a ``fractional Brownian motion,'' or simply a
            ``fractal'' process. In the case of a single tossed coin,
            it is termed ``fixed increment fractal,'' ``Brownian,'' or
            ``random walk'' process. In any case, determination of the
            statistical characteristics of $F_{t}$ are the essence of
            analysis. Fortunately, there is a large mathematical
            infrastructure dedicated to the subject. For example,
            $F_{t}$ could be verified as having a Gaussian
            distribution using Chi---Square techniques. Frequently, it
            is convenient, from an analytical standpoint, to ``model''
            $F_{t}$ using a mathematically simpler
            process~\cite[pp. 128]{Schroeder}. For example, multiple
            iterations of tossing a coin can be used to approximate a
            Gaussian distribution, since the distribution of many
            tosses of a coin is binomial---which if the number of
            tosses is sufficient will represent a Gaussian
            distribution to within any required
            precision~\cite[pp. 144]{Schroeder},~\cite[pp. 154]{Feder}.},
            signifying whether the game was a win, or a loss, ie.,
            whether the gambler's capital increased or decreased, and
            by how much. The amount the gambler's capital increased or
            decreased is $f_{t} \cdot F_{t}$. In general, $F_{t}$ is a
            function of a random variable, with an average, over time,
            of $avg_{f}$, and a root mean square value, $rms_{f}$, of
            unity. Note that for simple, time invariant, compound
            interest, $F_{t}$ has an average and root mean square,
            both being unity, and $f_{t}$ is simply the interest rate,
            which is assumed to be constant. For a simple, single coin
            game, $F_{t}$ is a fixed increment, (ie., either $+1$ or
            $-1$,) random generator. From an analytical perspective,
            it would be advantageous to measure the the statistical
            characteristics of the generator. Substituting
            Equation~\ref{summaryappb2} into
            Equation~\ref{summaryappb1}\footnote{Equation~\ref{summaryappb3}
            is interesting in many other respects. For example, adding
            a single term, $m \cdot V_{t - 1}$, to the equation
            results in $V_{t} = V_{t - 1} \left(1 + f_{t} F_{t} + m
            \cdot V_{t - 1}\right)$ which is the ``logistic,'' or `S'
            curve equation, (formally termed the ``discreet time
            quadratic equation,'') and has been used successfully in
            many unrelated fields such as manufacturing operations,
            market and economic forecasting, and analyzing disease
            epidemics~\cite[pp. 131]{Modis}. There is continuing
            research into the application of an additional
            ``non-linear'' term in Equation~\ref{summaryappb3} to model
            equity value non-linearities. Although there have been
            modest successes, to date, the successes have not proved
            to be exploitable in a systematic
            fashion~\cite[pp. 133]{Peters:CAOITCM}. The reason for the
            interest is that the logistic equation can exhibit a wide
            variety of behaviors, among them, ``chaotic.''
            Interestingly, chaotic behavior is mechanistic, but not
            ``long term'' predictable into the future. A good example
            of such a system is the weather. It is an important
            concept that compound interest, the logistic function, and
            fractals are all closely related.}:

            \begin{equation}
                V_{t} = V_{t - 1} \left(1 + f_{t} F_{t}\right)
                \label{summaryappb3}
            \end{equation}

            \noindent and subtracting $V_{t - 1}$ from both sides:

            \begin{equation}
                V_{t} - V_{t - 1} = V_{t - 1} \left(1 + f_{t} F_{t}\right) - V_{t - 1}
                \label{summaryappb4}
            \end{equation}

            \noindent and dividing both sides by $V_{t - 1}$:

            \begin{equation}
                \frac{V_{t} - V_{t - 1}}{V_{t - 1}} = \frac{V_{t - 1} \left(1 + f_{t} F_{t}\right) - V_{t - 1}}{V_{t - 1}}
                \label{summaryappb5}
            \end{equation}

            \noindent and combining:

            \begin{equation}
                \frac{V_{t} - V_{t - 1}}{V_{t - 1}} = \left(1 + f_{t} F_{t}\right) - 1 =  f_{t} F_{t}
                \label{summaryappb6}
            \end{equation}

            We now have a ``prescription,'' or process, for
            calculating the characteristics of the random process that
            determines an equity's value.  That process is, for each
            unit of time, subtract the value of the of the equity at
            the previous time from the value of the equity at the
            current time, and divide this by the value of the equity
            at the previous time. The root mean square\footnote{In
            this section, ``root mean square'' is used to mean the
            variance of the normalized increments. In Brownian motion
            fractals, this is computed by $\sigma_total^{2} =
            \sigma_1^{2} + \sigma_2^{2} + \cdots$ However, in many
            fractals, the variances are not calculated by adding the
            squares, (ie., a power of $2$,) of the values---the power
            may be ``fractional,'' ie., $3 / 2$ instead of $2$, for
            example~\cite[pp. 130]{Schroeder},~\cite[pp. 178]{Feder}. However,
            as a first order approximation, the variances of the
            normalized increments of equity values can successfully be
            added root mean square~\cite[kpp. 250]{Crownover}. The so
            called ``Hurst'' coefficient, which can be measured,
            determines the process to be used. The Hurst coefficient
            is range of the equity values over a time interval,
            divided by the standard deviation of the values over the
            interval, and its determination is commonly called ``$R /
            S$'' analysis. As pointed out in~\cite[pp. 157]{Schroeder}
            the errors committed in such simplified assumptions can be
            significant---however, for analysis of equities, squaring
            the variances seems to be a reasonable simplification.}
            of these values are the root mean square of the random
            process. The average of these values are the average of
            the random process, $avg_{f}$. The root mean square of
            these values can be calculated by any convenient means,
            and will be represented by $rms$. The average of these
            values can be found by any convenient means, and will be
            represented by $avg$\footnote{For example, many
            calculators have averaging and root mean square
            functionality, as do many spreadsheet
            programs---additionally, there are computer source codes
            available for both. See the programs {\it tsrms}\/ and
            {\it tsavg}\/. The method used is not
            consequential.}. Therefore, if $f_{t} = f$, and does not
            vary over time:

            \begin{equation}
                rms = f
                \label{summaryappb7}
            \end{equation}

            \noindent which, if there are sufficiently many samples,
            is a metric of the equity value's ``volatility,'' and:

            \begin{equation}
                avg = f \cdot F_{t}
                \label{summaryappb9}
            \end{equation}

            \noindent and if there are sufficiently many samples, the
            average of $F_{t}$ is simply $avg_{f}$, or:

            \begin{equation}
                avg = f \cdot avg_{f}
                \label{summaryappb10}
            \end{equation}

            \noindent which is a metric on the equity value's rate of
            ``growth.''  Note that this is the ``effective'' compound
            interest rate from Equation~\ref{summaryappb1}.

            Equations~\ref{summaryappb7} and~\ref{summaryappb10} are
            important equations, since they can be used in portfolio
            management. For example, Equation~\ref{summaryappb7}
            states that the volatility of the capital invested in many
            equities, simultaneously, is calculated as the root mean
            square of the individual volatility of the
            equities. Equation~\ref{summaryappb10} states that the
            growths in the same equity values add together
            linearly\footnote{There are significant implications do to
            the fact that equity volatilities are calculated root mean
            square. For example, if capital is invested in $N$ many
            equities, concurrently, then the volatility of the capital
            will be $\frac{1}{\sqrt{N}} \cdot rms$ of an individual
            equity's volatility, $rms$, provided all the equites have
            similar statistical characteristics. But the growth in the
            capital will be unaffected, ie., it would be statistically
            similar to investing all the capital in only one
            equity. What this means is that capital, or portfolio,
            volatility can be minimized without effecting portfolio
            growth---ie., volatility risk can addressed.  Further, it
            does not make any difference, as far as portfolio value
            growth is concerned, whether the individual equities are
            invested in concurrently, or serially, ie., if one
            invested in $10$ different equities for $100$ days,
            concurrently, or one could invest in only one equity, for
            $10$ days, and then the next equity for the next $10$
            days, and so on. The capital growth would have the same
            characteristics for both agendas. (Note that the
            concurrent agenda is superior since the volatility of the
            capital will be the root mean square of the individual
            equity volatilities divided by the square root of the
            number of equities.  In the serial agenda, the volatility
            of the capital will be simply the root mean square of the
            individual equity volatilities.)  Almost all equity
            wagering strategies will consist of optimizing variations
            on combinations of serial and concurrent agendas. There
            are further applications.  For example,
            Equation~\ref{summaryappb6} could be modified by dividing
            both the normalized increments, and the square of the
            normalized increments by the daily trading volume. The
            quotient of the normalized increments divided by the
            trading volume is the instantaneous growth, $avg_{f}$, of
            the equity, on a per-share basis. Likewise, the square
            root of the square of the normalized increments divided by
            the daily trading volume is the instantaneous root mean
            square, $rms_{f}$, of the equity on a per-share basis,
            ie., its instantaneous volatility of the equity. (Note
            that these instantaneous values are the statistical
            characteristics of the equity on a per-share bases,
            similar to a coin toss, and not on time.) Additionally, it
            can be shown that the range---the maximum minus the
            minimum---of an equity's value over a time interval will
            increase with the square root of of the size of the
            interval of time~\cite[pp. 178]{Feder}. Also, it can be
            shown that the number of expected stock value ``high and
            low'' transitions scales with the square root of time,
            meaning that the probability of an equity value ``high or
            low'' exceeding a given time interval is proportional to
            the square root of the time
            interval~\cite[pp. 153]{Schroeder}.}.

            Dividing Equation~\ref{summaryappb10} by
            Equation~\ref{summaryappb7} results in the two $f$'s
            canceling, or:

            \begin{equation}
                \frac{avg}{rms} = avg_{f}
                \label{summaryappb11}
            \end{equation}

            There may be analytical advantages to ``model'' $avg_{f}$
            as a simple tossed coin game, (either played with a single
            coin, or multiple coins, ie., many coins played at one
            time, or a single coin played many times\footnote{Here the
            ``model'' is to consider two black boxes, one with a stock
            ``ticker'' in it, and the other with a casino game of a
            tossed coin in it. One could then either invest in the
            equity, or, alternatively, invest in the tossed coin game
            by buying many casino chips, which constitutes the
            starting capital for the tossed coin game. Later, either
            the equity is sold, or the chips ``cashed in.'' If the
            statistics of the equity value over time is similar to the
            statistics of the coin game's capital, over time, then
            there is no way to determine which box has the equity, or
            the tossed coin game. The advantage of this model is that
            gambling games, such as the tossed coin, have a large
            analytical infrastructure, which, if the two black boxes
            are statistically the same, can be used in the analysis of
            equities. The concept is that if the value of the equity,
            over time, is statistically similar to the coin game's
            capital, over time, then the analysis of the coin game can
            be used on equity values. Note that in the case of the
            equity, the terms in $f_{t} \cdot F_{t}$ can not be
            separated. In this case, $f = rms$ is the fraction of the
            equity's value, at any time, that is ``at risk,'' of being
            lost, ie., this is the portion of a equity's value that is
            to be ``risk managed.''  This is usually addressed through
            probabilistic methods, as outlined below in the discussion
            of Shannon probabilities, where an optimal wagering
            strategy is determined. In the case of the tossed coin
            game, the optimal wagering strategy is to bet a fraction
            of the capital that is equal to $f = rms = 2P -
            1$~\cite[pp. 128, 151]{Schroeder}, where $P$ is the
            Shannon probability. In the case of the equity, since $f =
            rms$ is not subject to manipulation, the strategy is to
            select equities that closely approximate this
            optimization, and the equity's value, over time, on the
            average, would increase in a similar fashion to the coin
            game. The growth of either investment would be equal to
            $avg = rms^{2}$, on average, for each iteration of the
            coin game, or time unit of equity investment. This is an
            interesting concept from risk management since it
            maximizes the gain in the capital, while, simultaneously,
            minimizing risk exposure to the capital.}.)  The number of
            wins minus the number of losses, in many iterations of a
            single coin tossing game would be:

            \begin{equation}
                P - \left(1 - P\right) = 2P - 1
            \end{equation}

            \noindent where P is the probability of a win for the
            tossed coin. (This probability is traditionally termed,
            the ``Shannon probability'' of a win.)  Note that from the
            definition of $F_{t}$ above, that $P = avg_{f}$. For a
            fair coin, (ie., one that comes up with a win 50\% of the
            time,) $P = 0.5$, and there is no advantage, in the long
            run, to playing the game. However, if $P > 0.5$, then the
            optimal fraction of capital wagered on each iteration of
            the single coin tossing game, $f$, would be $2P - 1$. Note
            that if multiple coins were used for each iteration of the
            game, we would expect that the volatility of the gambler's
            capital to increase as the square root of the number of
            coins used, and the growth to increase linearly with the
            number of coins used, irregardless of whether many coins
            were tossed at once, or one coin was tossed many times,
            (ie., our random generator, $F_{t}$ would assume a
            binomial distribution---and if the number of coins was
            very large, then $F_{t}$ would assume, essentially, a
            Gaussian distribution.) Many equities have a Gaussian
            distribution for the random process, $F_{t}$. It may be
            advantageous to determine the Shannon probability to
            analyze equity investment strategies. From
            Equation~\ref{summaryappb11}:

            \begin{equation}
                \frac{avg}{rms} = avg_{f} = 2P - 1
                \label{summaryappb12}
            \end{equation}

            \noindent or:

            \begin{equation}
                \frac{avg}{rms} + 1 = 2P
                \label{summaryappb13}
            \end{equation}

            \noindent and:

            \begin{equation}
                P = \frac{\frac{avg}{rms} + 1}{2}
                \label{summaryappb14}
            \end{equation}

            \noindent where only the average and root mean square of
            the normalized increments need to be measured, using the
            ``prescription'' or process outlined above.

            Interestingly, what Equation~\ref{summaryappb12} states is
            that the ``best'' equity investment is not, necessarily,
            the equity that has the largest average growth,
            $avg_{f}$. The best equity investment is the equity that
            has the largest growth, while simultaneously having the
            smallest volatility. In point of fact, the optimal
            decision criteria is to choose the equity that has the
            largest {\it ratio}\/ of growth to volatility, where the
            volatility is measured by computing the root mean square
            of the normalized increments, and the growth is computed
            by averaging the normalized increments.

            We now have a ``first order prescription'' that enables us to
            analyze fluctuations in equity values, although we have not
            explained why equity values fluctuate. For a formal
            presentation on the subject, see the bibliography
            in~\cite{Arthur:CIEAFM} which, also, offers non-mathematical
            insight into the explanation.

            Consider a very simple equity market, with only two people
            holding equities.  Equity value ``arbitration'' (ie., how
            equity values are determined,) is handled by one person
            posting (to a bulletin board,) a willingness to sell a
            given number of stocks at a given price, to the other
            person. There is no other communication between the two
            people. If the other person buys the stock, then that is
            the value of the stock at that time. Obviously, the other
            person will not buy the stock if the price posted is too
            high---even if ownership of the stock is desired. For
            example, the other person could simply decide to wait in
            hopes that a favorable price will be offered in the
            future. So the stock seller must not post a price that the
            other person would consider too high, and the other person
            would not buy at the price if it is reasoned that the
            seller's pricing strategy will be to lower the offering
            price in the future, which would be a reasonable deduction
            if the posted price is considered too high. What this
            means is that the seller must consider not only the
            behavior of the other person, but what the other person
            thinks the seller's behavior will be, ie., the seller must
            base the pricing strategy on the seller's pricing
            strategy.  Such convoluted logical processes are termed
            ``self referential,'' and the implication is that the
            market can never operate in a consistent fashion that can
            be the subject of deductive
            analysis~\cite[pp. 101]{Penrose}\footnote{Penrose,
            referencing Russell's paradox, presents a very good
            example of logical contradiction in a self-referential
            system. Consider a library of books. The librarian notes
            that some books in the library contain their titles, and
            some do not, and wants to add two index books to the
            library, labeled ``A'' and ``B,'' respectively; the ``A''
            book will contain the list of all of the titles of books
            in the library that contain their titles; and the ``B''
            book will contain the list of all of the titles of the
            books in the library that do not contain their
            titles. Now, clearly, all book titles will go into either
            the ``A'' book, or the ``B'' book, respectively, depending
            on whether it contains its title, or not.  Now, consider
            in which book, the ``A'' book or the ``B'' book, the title
            of the ``B'' book is going to be placed---no matter which
            book the title is placed, it will be contradictory with
            the rules. And, if you leave it out, the two books will be
            incomplete.)}. As pointed out
            by~\cite[Abstract]{Arthur:CIEAFM}, these types of
            indeterminacies pervade
            economics\footnote{\cite{Arthur:CIEAFM} cites the ``El
            Farol Bar'' problem as an example. Assume one hundred
            people must decide independently each week whether go to
            the bar. The rule is that if a person predicts that more
            than, say, 60 will attend, it will be too crowded, and the
            person will stay home; if less than 60 is predicted, the
            person will go to the bar. As trivial as this seems, it
            destroys the possibility of long-run shared, rational
            expectations.  If all believe {\it few}\/ will go, then
            {\it all}\/ will go, thus invalidating the
            expectations. And, if all believe {\it many}\/ will go,
            then {\it none}\/ will go, likewise invalidating those
            expectations.  Predictions of how many will attend depend
            on others' predictions, and others' predictions of others'
            predictions. Once again, there is no rational means to
            arrive at deduced {\it a-priori}\/ predictions. The
            important concept is that expectation formation is a
            self-referential process in systems involving many agents
            with incomplete information about the future behavior of
            the other agents. The problem of logically forming
            expectations then becomes ill-defined, and rational
            deduction, can not be consistent or complete. This
            indeterminacy of expectation-formation is by no means an
            anomaly within the real economy. On the contrary, it
            pervades all of economics and game
            theory~\cite{Arthur:CIEAFM}.}.

            What the two players do, in absence of a deductively
            consistent and complete theory of the market, is to rely
            on inductive reasoning. They form subjective expectations
            or hypotheses about how the market operates. These
            expectations and hypothesis are constantly formulated and
            changed, in a world that forms from others' subjective
            expectations. What this means is that equity values will
            fluctuate as the expectations and hypothesis concerning
            the future of equity values change\footnote{Interestingly,
            the system described is a stable system, ie., if the
            players have a hypothesis that changing equity positions
            may be of benefit, then the equity values will
            fluctuate---a self fulfilling prophecy. Not all such
            systems are stable, however. Suppose that one or both
            players suddenly discover that equity values can be
            ``timed,'' ie., there are certain times when equities can
            be purchased, and chances are that the equity values will
            increase in the very near future. This means that at
            certain times, the equites would have more value, which
            would soon be arbitrated away. Such a scenario would not
            be stable.}. The fluctuations created by these
            indeterminacies in the equity market are represented by
            the term $f_{t} F_{t}$ in Equation~\ref{summaryappb3}, and
            since there are many such indeterminacies, we would
            anticipate $F_t$ to have a Gaussian distribution.

            This is a rather interesting conclusion, since analyzing
            the actions of aggregately many ``agents,'' each operating
            on subjective hypothesis in a market that is deductively
            indeterminate, can result in a system that can not only be
            analyzed, but optimized.

            The only remaining derivation is to show that the optimal
            wagering strategy is, as cited above:

            \begin{equation}
                f = rms = 2P - 1
            \end{equation}

            \noindent where $f$ is the fraction of a gambler's capital
            wagered on each toss of a coin that has a Shannon
            probability, $P$, of winning.

            Following~\cite[pp. 450]{Reza}, consider that the gambler
            has a private wire into the future who places wagers on
            the outcomes of a game of chance. We assume that the side
            information which he receives has a probability, $P$, of
            being true, and of $1 - P$, of being false. Let the
            original capital of gambler be $V(0)$, and $V(n)$ his
            capital after the $n$'th wager. Since the gambler is not
            certain that the side information is entirely reliable, he
            places only a fraction, $f$, of his capital on each
            wager. Thus, subsequent to $n$ many wagers, assuming the
            independence of successive tips from the future, his
            capital is:

            \begin{equation}
                V\left(n\right) = \left(1 + f\right)^{w} \left(1 - f\right)^{l} V\left(0\right)
            \end{equation}

            \noindent where $w$ is the number of times he won, and $l
            = n - w$, the number of times he lost. These numbers are,
            in general, values taken by two random variables, denoted
            by $W$ and $L$. According to the law of large numbers:

            \begin{equation}
                \lim_{n\to\infty} \frac{1}{n} W = P
            \end{equation}

            \noindent and:

            \begin{equation}
                \lim_{n\to\infty} \frac{1}{n} L = q = 1 - P
            \end{equation}

            The problem with which the gambler is faced is the
            determination of $f$ leading to the maximum of the average
            exponential rate of growth of his capital. That is, he
            wishes to maximize the value of:

                \begin{equation}
                    G = \lim_{n\to\infty} \frac{1}{n} \ln \frac{V\left(n\right)}{V\left(0\right)}
                \end{equation}

            \noindent with respect to $f$, assuming a fixed original
            capital and specified $P$:

                \begin{equation}
                    G = \lim_{n\to\infty} \frac{W}{n} \ln \left(1 + f\right) + \frac{L}{n} \ln \left(1 - f\right)
                \end{equation}

            \noindent or:

                \begin{equation}
                    G = P \ln \left(1 + f\right) + q \ln \left(1 - f\right)
                \end{equation}

            \noindent which, by taking the derivative with respect to
            $f$, and equating to zero, can be shown to have a maxima
            when:

                \begin{equation}
                    \frac{dG}{df} = P\left(1 + f\right)^{P - 1} \left(1 - f\right)^{1 - P} - \left(1 - P\right)\left(1 - f\right)^{1 - P - 1} \left(1 + f\right)^{P} = 0
                \end{equation}

            \noindent combining terms:

                \begin{equation}
                    P\left(1 + f\right)^{P - 1} \left(1 - f\right)^{1 - P} - \left(1 - P\right) \left(1 - f\right)^{P} \left(1 + f\right)^{P} = 0
                \end{equation}

            \noindent and splitting:

                \begin{equation}
                    P\left(1 + f\right)^{P - 1} \left(1 - f\right)^{1 - P} = \left(1 - P\right) \left(1 - f\right)^{P} \left(1 + f\right)^{P}
                \end{equation}

            \noindent then taking the logarithm of both sides:

                \begin{equation}
                    \ln \left(P\right) + \left(P - 1\right) \ln \left(1 + f\right) + \left(1 - P\right) \ln \left(1 - f\right) = \ln \left(1 - P\right) - P \ln \left(1 - f\right) + P \ln \left(1 + f\right)
                \end{equation}

            \noindent and combining terms:

                \begin{equation}
                    \left(P - 1\right) \ln \left(1 + f\right) - P \ln \left(1 + f\right) + \left(1 - P\right) \ln \left(1 - f\right) + P \ln \left(1 - f\right) = \ln \left(1 - P\right) - \ln \left(P\right)
                \end{equation}

            \noindent or:

                \begin{equation}
                    \ln \left(1 - f\right) - \ln \left(1 + f\right) = \ln \left(1 - P\right) - \ln \left(P\right)
                \end{equation}

            \noindent and performing the logarithmic operations:

                \begin{equation}
                    \ln \left(\frac{1 - f}{1 + f}\right) = \ln \left(\frac{1 - P}{P}\right)
                \end{equation}

            \noindent and exponentiating:

                \begin{equation}
                    \frac{1 - f}{1 + f} = \frac{1 - P}{P}
                \end{equation}

            \noindent which reduces to:

                \begin{equation}
                    P\left(1 - f\right) = \left(1 - P\right) \left(1 + f\right)
                \end{equation}

            \noindent and expanding:

                \begin{equation}
                    P - Pf = 1 - Pf - P + f
                \end{equation}

            \noindent or:

                \begin{equation}
                    P = 1 - P + f
                \end{equation}

            \noindent and, finally:

                \begin{equation}
                    f = 2P - 1
                \end{equation}

        \subsection{tsshannonfundamental}
            \subidx{programs}{tsshannonfundamental}
            \subidx{tsshannonfundamental}{program}

            Source tsshannonfundamental.c, is for finding the
            fundamental Shannon probability of a time series, given a
            stocks value, and the number of shares traded.  The value
            of a sample in the time series is added to the cumulative
            sum of the samples, and the square of the value is added
            to the sum of the squares to make a new time series by
            dividing the cumulative sum by the number of samples, and
            the square root of the sum of the squares divided by the
            number of samples for each sample. The new time series is
            printed to stdout.

            The input file structure is a text file consisting of
            records, in temporal order, one record per time series
            sample.  Blank records are ignored, and comment records
            are signified by a `\#' character as the first non white
            space character in the record. Data records must contain
            at least two fields, which is the data value of the
            sample, followed by the volume of the sample, but may
            contain many more fields---if the record contains many
            more fields, then the first field is regarded as the
            sample's time, and the next to the last field the value,
            with the last field as the sample's volume at that time.

            Note that since the average of the normalized increments
            of a time sampled time series goes up linearly on the
            number of samples in a sampled interval, and the root mean
            square of the normalized increments go up with the square
            root of the of the number of samples in a sampled
            interval, it would be reasonable to assume that that the
            average of the normalized increments would go up linearly
            with the trading volume of a stock, and the root mean
            square to go up with the square root of the trading volume
            of a stock.

        \subsection{tsnumber}
            \subidx{programs}{tsnumber}
            \subidx{tsnumber}{program}

            Source tsnumber.c, is for numbering the records of a time
            series.  The new time series is printed to stdout.

            The input file structure is a text file consisting of
            records, in temporal order, one record per time series
            sample.  Blank records are ignored, and comment records
            are signified by a `\#' character as the first non white
            space character in the record. Data records must contain
            at least one field, which is the data value of the sample,
            but may contain many fields---if the record contains many
            fields, then the first field is regarded as the sample's
            time, and the last field as the sample's value at that
            time.

        \subsection{tsunshannon}
            \subidx{programs}{tsunshannon}
            \subidx{tsunshannon}{program}
            \subidx{time series}{Shannon probability}
            \subidx{Shannon probability}{time series}

            Source tsunshannon.c, is for calculating the Shannon
            information capacity, (and optimal gain,) given the
            Shannon probability. See~\cite[pp. 128, 151]{Schroeder}

            This program is the inverse of the {\it tsshannon}\/
            program, and solves the equation:

            \begin{equation}
                 C\left(p\right) = 1 + p ln_2 \left(p\right) + \left(1 - p\right) ln_2 \left(1 - p\right)
            \end{equation}

            \noindent where the optimal gain is calculated as
            $2^{C(p)}$, and $f$, the fraction of capital wagered, is
            $2p - 1$.

            \noindent From~\cite[pp.  151]{Schroeder}:

            $p = 0.55$, $2^{C(0.55)} = 0.005$, (probably a typo,
            meaning $1.005$) by this program, $2^{C(0.550000)} =
            2^{0.007226} = 1.005021$.

        \subsection{tskurtosis}
            \subidx{programs}{tskurtosis}
            \subidx{tskurtosis}{program}
            \subidx{time series}{kurtosis}
            \subidx{kurtosis}{time series}

            Source tskurtosis.c is for finding the coefficient of
            excess kurtosis of a time series.  The value of a sample
            in the time series is analyzed to find the running
            coefficient of excess kurtosis to make a new time
            series. The new time series is printed to stdout.

            The method used is described in~\cite[pp. 563]{Shelby}:

            \begin{equation}
                a_4 = \frac{m_4}{m_2^2}
            \end{equation}

            \noindent where:

            \begin{equation}
                m_r = \frac{1}{n} \sum_{i = 1}^n f_i(x_i)^r
            \end{equation}

            The input file structure is a text file consisting of
            records, in temporal order, one record per time series
            sample.  Blank records are ignored, and comment records
            are signified by a `\#' character as the first non white
            space character in the record. Data records must contain
            at least one field, which is the data value of the sample,
            but may contain many fields---if the record contains many
            fields, then the first field is regarded as the sample's
            time, and the last field as the sample's value at that
            time.

            An example output from the {\it tskurtosis}\/ program
            appears in Figure~\ref{kurtosisexample}.

        \subsection{tskurtosiswindow}
            \subidx{programs}{tskurtosiswindow}
            \subidx{tskurtosiswindow}{program}
            \subidx{time series}{kurtosis}
            \subidx{kurtosis}{time series}

            Source tskurtosiswindow.c is for finding the coefficient
            of excess kurtosis of a time series.  The value of a
            sample in the time series is analyzed to find the running
            coefficient of excess kurtosis to make a new time
            series. The new time series is printed to stdout.

            The method used is described in~\cite[pp. 563]{Shelby}:

            \begin{equation}
                a_4 = \frac{m_4}{m_2^2}
            \end{equation}

            \noindent where:

            \begin{equation}
                m_r = \frac{1}{n} \sum_{i = 1}^n f_i(x_i)^r
            \end{equation}

            The input file structure is a text file consisting of
            records, in temporal order, one record per time series
            sample.  Blank records are ignored, and comment records
            are signified by a `\#' character as the first non white
            space character in the record. Data records must contain
            at least one field, which is the data value of the sample,
            but may contain many fields---if the record contains many
            fields, then the first field is regarded as the sample's
            time, and the last field as the sample's value at that
            time.

            An example output from the {\it tskurtosiswindow}\/ program
            appears in Figure~\ref{kurtosiswindowexample}.

            \begin{figure}[ht]
                \begin{center}
                    \begin{minipage}[t]{0.45\textwidth}
                        \epsfxsize=1.0\linewidth
                        \epsffile{../utilities/test/tsfBm.tskurtosis.eps}
                        \caption[Example output of the {\it
                            tskurtosis}\/ program]{Example output of
                            the {\it tskurtosis}\/ program, using
                            simulated Hurst coefficients of 0.0, 0.1,
                            0.3, 0.5, 0.7, 0.9, and 1.0, as simulated
                            by the {\it tsfBm}\/ program.}
                        \label{kurtosisexample}
                    \end{minipage}
                    \hfill
                    \begin{minipage}[t]{0.45\textwidth}
                        \epsfxsize=1.0\linewidth
                        \epsffile{../utilities/test/tsfBm.tskurtosiswindow.eps}
                        \caption[Example output of the {\it
                            tskurtosis}\/ program]{Example output of
                            the {\it tskurtosis}\/ program, using
                            simulated Hurst coefficients of 0.0, 0.1,
                            0.3, 0.5, 0.7, 0.9, and 1.0, as simulated
                            by the {\it tsfBm}\/ program.}
                        \label{kurtosiswindowexample}
                    \end{minipage}
                \end{center}
            \end{figure}

        \subsection{tsgain}
            \subidx{programs}{tsgain}
            \subidx{tsgain}{program}
            \subidx{time series}{gain}
            \subidx{gain}{time series}

            Source tsgain.c is for finding the gain of a time series.
            The value of a sample in the time series added to the
            cumulative sum of the samples, and is squared and added to
            the cumulative sum of squares, the Shannon probability,
            $P$, calculated using:

            \begin{equation}
                P = \frac{\frac{avg}{rms} + 1}{2}
            \end{equation}

            \noindent where $rms$ is the root mean square of the
            marginal returns, and $avg$ is the average of the marginal
            returns, and the gain, $G$, calculated using:

            \begin{equation}
                G = \left(1 + rms\right)^P \cdot \left(1 - rms \right)^{P - 1}
            \end{equation}

            \noindent to make a new time series. The new time series
            is printed to stdout.

            The input file structure is a text file consisting of
            records, in temporal order, one record per time series
            sample.  Blank records are ignored, and comment records
            are signified by a `\#' character as the first non white
            space character in the record. Data records must contain
            at least one field, which is the data value of the sample,
            but may contain many fields---if the record contains many
            fields, then the first field is regarded as the sample's
            time, and the last field as the sample's value at that
            time.

            An example output from the {\it tsgain}\/ program
            appears in Figure~\ref{gainexample}.

        \subsection{tsgainwindow}
            \subidx{programs}{tsgainwindow}
            \subidx{tsgainwindow}{program}
            \subidx{time series}{gain}
            \subidx{gain}{time series}

            Source tsgainwindow.c is for finding the windowed gain of
            a time series.  The value of a sample in the time series
            added to the cumulative sum of the samples, and is squared
            and added to the cumulative sum of squares, the Shannon
            probability, $P$, calculated using:

            \begin{equation}
                P = \frac{\frac{avg}{rms} + 1}{2}
            \end{equation}

            \noindent where $rms$ is the root mean square of the
            marginal returns, and $avg$ is the average of the marginal
            returns, and the gain, $G$, calculated using:

            \begin{equation}
                G = \left(1 + rms\right)^P \cdot \left(1 - rms\right)^{P - 1}
            \end{equation}

            \noindent to make a new time series. The new time series
            is printed to stdout.

            The input file structure is a text file consisting of
            records, in temporal order, one record per time series
            sample.  Blank records are ignored, and comment records
            are signified by a `\#' character as the first non white
            space character in the record. Data records must contain
            at least one field, which is the data value of the sample,
            but may contain many fields---if the record contains many
            fields, then the first field is regarded as the sample's
            time, and the last field as the sample's value at that
            time.

            An example output from the {\it tsgainwindow}\/ program
            appears in Figure~\ref{gainwindowexample}.

            \begin{figure}[ht]
                \begin{center}
                    \begin{minipage}[t]{0.45\textwidth}
                        \epsfxsize=1.0\linewidth
                        \epsffile{../utilities/test/tscoin.tsgain.eps}
                        \caption[Example output of the {\it
                            tsgain}\/ program]{Example output of
                            the {\it tsgain}\/ program, using the
                            output of the {\it tscoin}\/ program, with
                            a Shannon probability of 0.6, which is
                            shown in Figure~\ref{coinexample} in
                            Section~\ref{tscoinexample}.}
                        \label{gainexample}
                    \end{minipage}
                    \hfill
                    \begin{minipage}[t]{0.45\textwidth}
                        \epsfxsize=1.0\linewidth
                        \epsffile{../utilities/test/tscoin.tsgainwindow.eps}
                        \caption[Example output of the {\it
                            tsgainwindow}\/ program]{Example output of
                            the {\it tsgainwindow}\/ program, using
                            the output of the {\it tscoin}\/ program,
                            with a Shannon probability of 0.6, which
                            is shown in Figure~\ref{coinexample} in
                            Section~\ref{tscoinexample}.}
                        \label{gainwindowexample}
                    \end{minipage}
                \end{center}
            \end{figure}

        \subsection{tsscalederivative}
            \subidx{programs}{tsscalederivative}
            \subidx{tsscalederivative}{program}
            \subidx{time series}{derivative}
            \subidx{derivative}{derivative}

            Source tsscalederivative.c, for taking the derivative of a
            time series.  The value of a sample in the time series is
            subtracted from the previous sample in the time
            series. The derivative time series is printed to stdout.

            The input file structure is a text file consisting of
            records, in temporal order, one record per time series
            sample.  Blank records are ignored, and comment records
            are signified by a '\#' character as the first non white
            space character in the record. Data records must contain
            at least two fields, which are the time followed by the
            data value of the sample at that time, but may contain
            many fields---if the record contains many fields, then the
            first field is regarded as the sample's time, and the last
            field as the sample's value at that time.

            An example output from the {\it tsscalederivative}\/ program
            appears in Figure~\ref{scalederivativeexample}.

        \subsection{tsrootmeanscale}
            \subidx{programs}{tsrootmeanscale}
            \subidx{tsrootmeanscale}{program}
            \subidx{time series}{root mean}
            \subidx{root mean}{time series}

            Source tsrootmeanscale.c is for finding the root mean of a
            time series, at different scales. The number of
            consecutive samples of like movements in the time series
            is tallied, at different scales, and the resultant value
            of the distribution, as calculated by using the first
            value in the distribution, the running mean of the
            distribution, and the least squares fit of the
            distribution, is printed to stdout-a simple random walk
            fractal with a Gaussian/normal distributed increments
            would be the combinatorial probabilities, $0.5$, $0.25$,
            $0.125$, $0.625$, $\ldots$

            The input file structure is a text file consisting of
            records, in temporal order, one record per time series
            sample.  Blank records are ignored, and comment records
            are signified by a `\#' character as the first non white
            space character in the record. Data records must contain
            at least one field, which is the data value of the sample,
            but may contain many fields---if the record contains many
            fields, then the first field is regarded as the sample's
            time, and the last field as the sample's value at that
            time.

            An example output from the {\it tsrootmeanscale}\/ program
            appears in Figure~\ref{rootmeanscaleexample}.

            \begin{figure}[ht]
                \begin{center}
                    \begin{minipage}[t]{0.45\textwidth}
                        \epsfxsize=1.0\linewidth
                        \epsffile{../utilities/test/tsfBm.tshurst.tsscalederivative.eps}
                        \caption[Example output of the {\it
                            tsscalederivative}\/ program]{Example
                            output of the {\it tsscalederivative}\/
                            program, using simulated Hurst
                            coefficients of 0.0, 0.1, 0.3, 0.5, 0.7,
                            0.9, and 1.0, as simulated by the {\it
                            tsfBm}\/ program, and the {\it tshurst}\/
                            program in Figure~\ref{hurstexample}.}
                        \label{scalederivativeexample}
                    \end{minipage}
                    \hfill
                    \begin{minipage}[t]{0.45\textwidth}
                        \epsfxsize=1.0\linewidth
                        \epsffile{../utilities/test/tsfBm.tsrootmeanscale.eps}
                        \caption[Example output of the {\it
                            tsrootmeanscale}\/ program]{Example output of
                            the {\it tsrootmeanscale}\/ program, using
                            simulated Hurst coefficients of 0.0, 0.1,
                            0.3, 0.5, 0.7, 0.9, and 1.0, as simulated
                            by the {\it tsfBm}\/
                            program.}
                        \label{rootmeanscaleexample}
                    \end{minipage}
                \end{center}
            \end{figure}

        \subsection{tskalman}
            \subidx{programs}{tskalman}
            \subidx{tskalman}{program}
            \subidx{time series}{average}
            \subidx{average}{time series}

            Source tskalman.c for taking the Kalman filtered average
            of a time series.  The $n$'th running Kalman filtered
            linear average, $A$, of a time series is calculated by:

            \begin{equation}
                A_{n} = \frac{n - 1}{n} A_{n - 1} + \frac{1}{n} a_{n}
            \end{equation}

            where $a$ is the $n$'th value in the time series.  The new
            time series of the running Kalman filtered average is
            printed to stdout.

            Note the similarity to the running average:

            \begin{equation}
                A_{n} = \frac{1}{n} \left(a_{1} + a_{2} + \cdot + a_{n}\right)
            \end{equation}

            The input file structure is a text file consisting of
            records, in temporal order, one record per time series
            sample.  Blank records are ignored, and comment records
            are signified by a `\#' character as the first non white
            space character in the record. Data records must contain
            at least one field, which is the data value of the sample,
            but may contain many fields---if the record contains many
            fields, then the first field is regarded as the sample's
            time, and the last field as the sample's value at that
            time.

            An example output from the {\it tskalman}\/ program appears
            in Figure~\ref{kalmanexample}.

        \subsection{tsroot}
            \subidx{programs}{tsroot}
            \subidx{tsroot}{program}
            \subidx{time series}{average}
            \subidx{average}{time series}

            Source tsroot.c is for finding the root of a time
            series. The range, as a function of time, is summed, for
            each and every point in the time series. For example, the
            output should be proportional to the $\sqrt{t}$ for a
            Brownian motion fractal.

            The input file structure is a text file consisting of
            records, in temporal order, one record per time series
            sample.  Blank records are ignored, and comment records
            are signified by a `\#' character as the first non white
            space character in the record. Data records must contain
            at least one field, which is the data value of the sample,
            but may contain many fields---if the record contains many
            fields, then the first field is regarded as the sample's
            time, and the last field as the sample's value at that
            time.

            An example output from the {\it tsroot}\/ program appears
            in Figure~\ref{rootexample}.

            \begin{figure}[ht]
                \begin{center}
                    \begin{minipage}[t]{0.45\textwidth}
                        \epsfxsize=1.0\linewidth
                        \epsffile{../utilities/test/tscoin.tsfraction.tskalman.eps}
                        \caption[Example output of the {\it tskalman}\/
                            program]{Example output of the {\it
                            tskalman}\/ program, the input was produced
                            by the {\it tsfraction}\/ program, shown
                            in Figure~\ref{fractionexample}, which
                            used the output of the {\it tscoin}\/
                            program, with a Shannon probability of
                            0.6, and is shown in
                            Figure~\ref{coinexample} in
                            Section~\ref{tscoinexample}.}
                        \label{kalmanexample}
                    \end{minipage}
                    \hfill
                    \begin{minipage}[t]{0.45\textwidth}
                        \epsfxsize=1.0\linewidth
                        \epsffile{../utilities/test/tscoin.tsroot-l.eps}
                        \caption[Example output of the {\it tsroot}\/
                            program]{Example output of the {\it
                            tsroot}\/ program, taking the logarithm of
                            the range of the file produced by the {\it
                            tscoin}\/ program, with a Shannon
                            probability of 0.6, which is shown in
                            Figure~\ref{coinexample} in
                            Section~\ref{tscoinexample}.}
                        \label{rootexample}
                    \end{minipage}
                \end{center}
            \end{figure}

    \section{Fractal Time Series Simulation Utilities}

        \idx{random deviate}
        \idx{normally distributed deviate}
        \idx{deviate, random}
        \idx{deviate, normally distributed}
        \idx{gamma function}
        \idx{Romberg integration}
        \idx{trapezoid iteration}
        \idx{polynomial interpolation}
        Note: these programs use the following functions from other
        references:

        \begin{description}

            \item[ran1:] which returns a uniform random deviate
            between 0.0 and 1.0. See~\cite[pp. 210]{Press},
            referencing Knuth.

            \item[gasdev:] which returns a normally distributed
            deviate with zero mean and unit variance, using ran1 () as
            the source of uniform deviates. See~\cite[pp. 217]{Press}.

            \item[gammln:] which returns the log of the results of the
            gamma function.  See~\cite[pp. 168]{Press}.

            \item[romberg:] which returns the integral of a function,
            using iterate (), and interpolate
            (). See~\cite[pp. 124]{Press}.

            \item[iterate:] which computes the n'th stage of
            refinement of an extended iterate rule using trapezoid
            iteration. See~\cite[pp. 120]{Press}.

            \item[interpolate:] which interpolates the y value for
            point x using polynomial
            interpolation. See~\cite[pp. 90]{Press}.

        \end{description}

        \subsection{tsbrownian}
            \subidx{programs}{tsbrownian}
            \subidx{tsbrownian}{program}
            \subidx{brown noise}{generation}
            \subidx{generation}{brown noise}
            \subidx{brown noise}{time series}
            \subidx{time series}{brown noise}
            \subidx{brown noise}{simulation}
            \subidx{simulation}{brown noise}

            Source tsbrownian.c, brownian noise generator---generates a
            time series.  The idea is to produce a $1/f$ squared power
            spectrum distribution by running a cumulative sum on white
            noise. See~\cite[pp. 128]{Schroeder}.

            An example output from the {\it tsbrownian}\/ program
            appears in Figure~\ref{brownianexample}.

        \subsection{tsblack}
            \subidx{programs}{tsblack}
            \subidx{rtsblack}{program}
            \subidx{black noise}{generation}
            \subidx{generation}{black noise}
            \subidx{black noise}{time series}
            \subidx{time series}{black noise}
            \subidx{black noise}{simulation}
            \subidx{simulation}{black noise}

            Source tsblack.c, black noise generator---generates a time
            series.  The idea is to produce a $1/f$ cubed power
            spectrum distribution by running a cumulative sum on pink
            noise which is made by running a cumulative sum on
            relaxation processes which are generated by a white noise
            generator. See~\cite[pp. 126]{Schroeder}.

            An example output from the {\it tsblack}\/ program appears
            in Figure~\ref{blackexample}.

            \begin{figure}[ht]
                \begin{center}
                    \begin{minipage}[t]{0.45\textwidth}
                        \epsfxsize=1.0\linewidth
                        \epsffile{../simulation/test/tsbrownian.tsnormal.eps}
                        \caption[Example output of the {\it
                            tsbrownian}\/ program]{Example output of
                            the {\it tsbrownian}\/ program, using 1500
                            records. This is a plot of the frequency
                            histogram.}
                        \label{brownianexample}
                    \end{minipage}
                    \hfill
                    \begin{minipage}[t]{0.45\textwidth}
                        \epsfxsize=1.0\linewidth
                        \epsffile{../simulation/test/tsblack.tsnormal.eps}
                        \caption[Example output of the {\it tsblack}\/
                            program]{Example output of the {\it
                            tsblack}\/ program, using 1500
                            records. This is a plot of the frequency
                            histogram.}
                        \label{blackexample}
                    \end{minipage}
                \end{center}
            \end{figure}

        \subsection{tsfractional}
            \subidx{programs}{tsfractional}
            \subidx{tsfractional}{program}
            \subidx{time series}{fractional}
            \subidx{fractional}{time series}
            \subidx{simulation}{fractional}
            \subidx{fractional}{simulation}

            Source tsfractional.c, fractional brownian noise
            generator---generates a time series.  The idea is to produce
            a $1/f$ squared power spectrum distribution by running a
            cumulative sum on a Gaussian power spectrum
            distribution. See~\cite[pp. 172]{Feder}.

            An example output from the {\it tsfractional}\/ program
            appears in Figure~\ref{fractionalexample}.

        \subsection{tsgaussian}
            \subidx{programs}{tsgaussian}
            \subidx{tsgaussian}{program}
            \subidx{time series}{Gaussian}
            \subidx{Gaussian}{time series}
            \subidx{Gaussian time series}{generation}
            \subidx{generation}{Gaussian time series}
            \subidx{Gaussian time series}{simulation}
            \subidx{simulation}{Gaussian time series}

            Source tsgaussian.c, Gaussian noise generator---generates a
            time series. The idea is to produce a Gaussian power
            spectrum distribution.

            An example output from the {\it tsgaussian}\/ program
            appears in Figure~\ref{gaussianexample}.

            \begin{figure}[ht]
                \begin{center}
                    \begin{minipage}[t]{0.45\textwidth}
                        \epsfxsize=1.0\linewidth
                        \epsffile{../simulation/test/tsfractional.tsnormal.eps}
                        \caption[Example output of the {\it
                            tsfractional}\/ program]{Example output of
                            the {\it tsfractional}\/ program, using
                            1500 records. This is a plot of the
                            frequency histogram.}
                        \label{fractionalexample}
                    \end{minipage}
                    \hfill
                    \begin{minipage}[t]{0.45\textwidth}
                        \epsfxsize=1.0\linewidth
                        \epsffile{../simulation/test/tsgaussian.tsnormal.eps}
                        \caption[Example output of the {\it
                            tsgaussian}\/ program]{Example output of
                            the {\it tsgaussian}\/ program, using 1500
                            records. This is a plot of the frequency
                            histogram.}
                        \label{gaussianexample}
                    \end{minipage}
                \end{center}
            \end{figure}

        \subsection{tswhite}
            \subidx{programs}{tswhite}
            \subidx{tswhite}{program}
            \subidx{white noise}{generation}
            \subidx{generation}{white noise}
            \subidx{white noise}{time series}
            \subidx{time series}{white noise}
            \subidx{white noise}{simulation}
            \subidx{simulation}{white noise}

            Source tswhite.c, white noise generator---generates a time
            series.  The idea is to produce a flat power spectrum
            distribution.

            An example output from the {\it tswhite}\/ program appears
            in Figure~\ref{whiteexample}.

        \subsection{tspink}
            \subidx{programs}{tspink}
            \subidx{tspink}{program}
            \subidx{pink noise}{generation}
            \subidx{generation}{pink noise}
            \subidx{pink noise}{time series}
            \subidx{time series}{pink noise}
            \subidx{pink noise}{simulation}
            \subidx{simulation}{pink noise}

            Source tspink.c, pink noise generator---generates a time
            series.  The idea is to produce a $1/f$ power spectrum
            distribution by running a cumulative sum on relaxation
            processes which are generated by a white noise
            generator. See~\cite[pp. 122]{Schroeder}.

            An example output from the {\it tspink}\/ program appears
            in Figure~\ref{pinkexample}.

            \begin{figure}[ht]
                \begin{center}
                    \begin{minipage}[t]{0.45\textwidth}
                        \epsfxsize=1.0\linewidth
                        \epsffile{../simulation/test/tswhite.tsnormal.eps}
                        \caption[Example output of the {\it tswhite}\/
                            program]{Example output of the {\it
                            tswhite}\/ program, using 1500
                            records. This is a plot of the frequency
                            histogram.}
                        \label{whiteexample}
                    \end{minipage}
                    \hfill
                    \begin{minipage}[t]{0.45\textwidth}
                        \epsfxsize=1.0\linewidth
                        \epsffile{../simulation/test/tspink.tsnormal.eps}
                        \caption[Example output of the {\it tspink}\/
                            program]{Example output of the {\it
                            tspink}\/ program, using 1500
                            records. This is a plot of the frequency
                            histogram.}
                        \label{pinkexample}
                    \end{minipage}
                \end{center}
            \end{figure}

        \subsection{tsunfairbrownian}
            \label{tsunfairbrownian}
            \subidx{programs}{tsunfairbrownian}
            \subidx{tsunfairbrownian}{program}
            \subidx{time series}{changing unfair weights}
            \idx{Shannon probability}

            Source tsunfairbrownian.c, unfair returns of a time
            series. The idea is to produce the returns of a time
            series which is weighted unfairly, by a Shannon
            probability, p, or alternately, a fraction of reserves to
            be wagered on each time increment. The input time series
            is presumed to have a Brownian distribution. The main
            function of this program is regression scenario
            verification---given an empirical time series, a Shannon
            probability, or a ``wager'' fraction, (which were probably
            derived from the program {\it tsshannon}\/,) speculative
            market pro forma performance can be analyzed. The
            cumulative sum process is Brownian in nature.

            The input file structure is a text file consisting of
            records, in temporal order, one record per time series
            sample.  Blank records are ignored, and comment records
            are signified by a `\#' character as the first non white
            space character in the record. Data records must contain
            at least one field, which is the data value of the sample,
            but may contain many fields---if the record contains many
            fields, then the first field is regarded as the sample's
            time, and the last field as the sample's value at that
            time.

            An example output from the {\it tsunfairbrownian}\/
            program appears in Figure~\ref{unfairbrownianexample}.

        \subsection{tscoin}
            \label{tscoinexample}
            \subidx{programs}{tscoin}
            \subidx{tscoin}{program}
            \subidx{time series}{coin game}
            \subidx{coin game}{time series}
            \subidx{generation}{coin game time series}
            \subidx{coin game time series}{generation}
            \subidx{simulation}{coin game time series}
            \subidx{coin game time series}{simulation}

            Source tscoin.c, brownian noise generator, with unfair
            bias, and cumulative sum---generates a time series.  The
            idea is to produce a $1/f$ squared power spectrum
            distribution by running a cumulative sum on white
            noise. The program accepts an unfair bias and a wager
            factor. See~\cite[pp. 128]{Schroeder}.

            \noindent The discreet time formula is:

            \begin{equation}
                x_t = x_{t - 1} + f \cdot R \cdot x_{t - 1}
            \end{equation}

            \noindent where f is the fraction of the capital to be
            wagered, and $R$ is a uniform random deviate between 0.0
            and 1.0, with the mean offset appropriately to provide a
            Shannon probability, $P$. For the logistic, function, the
            discreet time formula is:

            \begin{equation}
                x_t = x_{t - 1} + f \cdot R \cdot x_{t - 1} + n \cdot x_{t - 1}^2
            \end{equation}

            An example output from the {\it tscoin}\/ program appears
            in Figure~\ref{coinexample}.

            \begin{figure}[ht]
                \begin{center}
                    \begin{minipage}[t]{0.45\textwidth}
                        \epsfxsize=1.0\linewidth
                        \epsffile{../simulation/test/tscoin.tsunfairbrownian.eps}
                        \caption[Example output of the {\it
                            tsunfairbrownian}\/ program] {Example
                            output of the {\it tsunfairbrownian}\/
                            program, changing the wager fraction from
                            0.2 to 0.1. The original time series data
                            set was produced by the {\it tscoin}\/
                            program using a Shannon probability of
                            0.6, and is shown in
                            Figure~\ref{coinexample}, corresponding to
                            a wager fraction, $f$, of $f = 2P - 1 = 2
                            \cdot 0.6 - 1 = 0.2$.}
                        \label{unfairbrownianexample}
                    \end{minipage}
                    \hfill
                    \begin{minipage}[t]{0.45\textwidth}
                        \epsfxsize=1.0\linewidth
                        \epsffile{../simulation/test/tscoin.eps}
                        \caption[Example output of the {\it tscoin}\/
                            program] {Example output of the {\it
                            tscoin}\/ program, using a Shannon
                            probability of 0.6.}
                        \label{coinexample}
                    \end{minipage}
                \end{center}
            \end{figure}

        \subsection{tscoins}
            \subidx{Newton---Raphson}{iterated solution}
            \subidx{iterated solution}{Newton---Raphson}
            \subidx{searching for solutions}{methodology}
            \label{tscoinsexample}
            \subidx{programs}{tscoins}
            \subidx{tscoins}{program}
            \subidx{time series}{coin game}
            \subidx{coin game}{time series}
            \subidx{generation}{coin game time series}
            \subidx{coin game time series}{generation}
            \subidx{simulation}{coin game time series}
            \subidx{coin game time series}{simulation}

            Source tscoins.c, fractional brownian noise generator,
            with unfair bias, and cumulative sum---generates a time
            series.  The idea is to produce a $1/f$ squared power
            spectrum distribution by running a cumulative sum on a
            Gaussian power spectrum distribution. The program accepts
            an unfair bias and a wager factor.
            See~\cite[pp. 172]{Feder}.  Uses Newton---Raphson method
            for an iterative solution for the probability, $p$.

            \noindent The discreet time formula is:

            \begin{equation}
                x_t = x_{t - 1} + f \cdot R \cdot x_{t - 1}
            \end{equation}

            \noindent where f is the fraction of the capital to be
            wagered, and $R$ is a Gaussian function, with the mean
            offset appropriately to provide a Shannon probability,
            $P$. For the logistic function, the discreet time formula
            is:

            \begin{equation}
                x_t = x_{t - 1} + f \cdot R \cdot x_{t - 1} + n \cdot x_{t - 1}^2
            \end{equation}

            As a reference on Newton---Raphson Method of root finding,
            see~\cite[pp. 270]{Press}.

            The name, tscoins, was chosen since pitching many coins,
            at once, and counting the number of heads, many times,
            will approach a gaussian distribution, if the number of
            coins is large, and the number of times is
            large.~\cite[pp. 154]{Feder}

            \noindent The general outline of this program is:

            \begin{enumerate}

                \item given the Shannon probability, compute the
                abscissa value that divides the area under the normal
                curve, into two sections, such that the area to the
                left of the value, divided by the total area under the
                normal curve is the Shannon probability---a
                Newton-Raphson iterated approach using Romberg
                integration to find the area is used for this

                \item for each record:

                \begin{enumerate}

                    \item compute a gaussian distributed random
                    number

                    \item add the computed abscissa value to the
                    gaussian distributed number

                    \item multiply this number by the fraction of
                    cumulative sum to be wagered

                    \item multiply this number by the cumulative sum

                    \item add this number to the cumulative sum

                \end{enumerate}

            \end{enumerate}

            This program will require finding the value of the normal
            function, given the standard deviation. The method used is
            to use Romberg/trapezoid integration to numerically solve
            for the value.

            This program will require finding the functional inverse
            of the normal, ie., Gaussian, function. The method used is
            to use Romberg/trapezoid integration to numerically solve
            the equation:

            \begin{equation}
                F\left(x\right) = \int_{0}^{x} \frac{1}{2\pi} e^{\frac{-t^{2}}{2}} dt + 0.5
            \end{equation}

            \noindent which has the derivative:

            \begin{equation}
                f\left(x\right) = \frac{1}{2\pi} e^{\frac{-x^{2}}{2}}
            \end{equation}

            \noindent Since $F(x)$ is known, and it is desired to find
            $x$,

            \begin{equation}
                F\left(x\right) - \int_{0}^{x} \frac{1}{2\pi} e^{\frac{-t^{2}}{2}} dt + 0.5 = P\left(x\right) = 0
            \end{equation}

            \noindent and the Newton-Raphson method of finding roots
            would be:

            \begin{equation}
                P_{n + 1} = P_{n} - \frac{P\left(x\right)}{f\left(x\right)}
            \end{equation}

            An example output from the {\it tscoins}\/ program appears
            in Figure~\ref{coinsexample}. The distribution of the
            normalized increments appears in
            Figure~\ref{coinsexampleni}.

            \begin{figure}[ht]
                \begin{center}
                    \begin{minipage}[t]{0.45\textwidth}
                        \epsfxsize=1.0\linewidth
                        \epsffile{../simulation/test/tscoins.eps}
                        \caption[Example output of the {\it tscoins}\/
                            program] {Example output of the {\it
                            tscoins}\/ program, using a Shannon
                            probability of 0.6.}
                        \label{coinsexample}
                    \end{minipage}
                    \hfill
                    \begin{minipage}[t]{0.45\textwidth}
                        \epsfxsize=1.0\linewidth
                        \epsffile{../simulation/test/tscoins.tsfraction.tsnormal.eps}
                        \caption[Example output of the {\it tscoins}\/
                            program]{Example output of the {\it
                            tscoins}\/ program, normalized histogram
                            of the normalized increments of the time
                            series data shown in
                            Figure~\ref{coinsexample}. The area under
                            the two curves is identical.}
                        \label{coinsexampleni}
                    \end{minipage}
                \end{center}
            \end{figure}

        \subsection{tsfBm}
            \subidx{programs}{tsfBm}
            \subidx{tsfBm}{program}
            \subidx{fractional Brownian motion}{generation}
            \subidx{generation}{fractional Brownian motion}
            \subidx{time series}{fractional Brownian motion}
            \subidx{fractional Brownian motion}{time series}
            \subidx{simulation}{fractional Brownian motion}
            \subidx{fractional Brownian motion}{simulation}

            Source tsfBm.c, fractional brownian noise
            generator---generates a time series. The idea is to
            produce a programmable power spectrum
            distribution. See~\cite[pp. 211]{Peters:CAOITCM},
            or~\cite[pp. 173]{Feder}, referencing Mandelbrot and
            Wallis, 1969.

            Example outputs from the {\it tsfBm}\/ program appear in
            figures~\ref{normalexample},~\ref{hcalcexample},
            and~\ref{hurstexample}.

        \subsection{tslogistic}
            \subidx{programs}{tslogistic}
            \subidx{tslogistic}{program}
            \subidx{time series}{logistic function}
            \subidx{logistic function}{time series}
            \subidx{generation}{logistic function}
            \subidx{logistic function}{generation}
            \subidx{simulation}{logistic function}
            \subidx{logistic function}{simulation}

            Source tslogistic.c, logistic function
            generator---generates a time series.  The idea is to
            produce a function of the form $y(t) = c / {(1 + e^{-(at +
            b)})}$. See~\cite[pp. 100]{Morrison},
            or~\cite[pp. 230]{Modis}.

            An example output from the {\it tslogistic}\/ program
            appears in Figure~\ref{logisticexample}.

        \subsection{tsdlogistic}
            \label{tsdlogisticexample}
            \subidx{programs}{tsdlogistic}
            \subidx{tsdlogistic}{program}
            \subidx{logistic equation}{discreet}
            \subidx{discreet}{logistic equation}
            \subidx{logistic equation}{time series}
            \subidx{time series}{logistic equation}
            \subidx{logistic equation}{simulation}
            \subidx{simulation}{logistic equation}

            Source tsdlogistic.c, discreet logistic function
            generator---generates a time series.  The idea is to
            iterate the function $x(t) = x(t - 1) \cdot (a + b \cdot
            x(t - 1))$.  See~\cite[pp. 121]{Peters:CAOITCM}.

            \noindent As a simple set of examples:

            \begin{itemize}

                \item[] tsdlogistic -a 2 -b -2 100

                \item[] tsdlogistic -a 2.4 -b -2.4 100

                \item[] tsdlogistic -a 3 -b -3 100

                \item[] tsdlogistic -a 3.4495 -b -3.4495 100

                \item[] tsdlogistic -a 3.544 -b -3.544 100

                \item[] tsdlogistic -a 3.5688 -b -3.5688 100

                \item[] tsdlogistic -a 3.5696 -b -3.5696 100

                \item[] tsdlogistic -a 3.5699456 -b -3.5699456 100

            \end{itemize}

            An example output from the {\it tsdlogistic}\/ program
            appears in Figure~\ref{dlogisticexample}.

            \begin{figure}[ht]
                \begin{center}
                    \begin{minipage}[t]{0.45\textwidth}
                        \epsfxsize=1.0\linewidth
                        \epsffile{../simulation/test/tslogistic.eps}
                        \caption[Example output of the {\it
                            tslogistic}\/ program] {Example output of
                            the {\it tslogistic}\/ program, using the
                            command ``tslogistic 1 1 1 100 >
                            filename.''}
                        \label{logisticexample}
                    \end{minipage}
                    \hfill
                    \begin{minipage}[t]{0.45\textwidth}
                        \epsfxsize=1.0\linewidth
                        \epsffile{../simulation/test/tsdlogistic.eps}
                        \caption[Example output of the {\it
                            tsdlogistic}\/ program]{Example output of
                            the {\it tsdlogistic}\/ program, using the
                            command ``tsdlogistic -a 4 -b -1 1000 >
                            filename.''}
                        \label{dlogisticexample}
                    \end{minipage}
                \end{center}
            \end{figure}

        \subsection{tsstockwager}
            \subidx{programs}{tsstockwager}
            \subidx{tsstockwager}{program}
            \subidx{time series}{stock wager optimization}
            \subidx{stock wager optimization}{time series}
            \subidx{simulation}{stock wager optimization}
            \subidx{stock wager optimization}{simulation}

            Source tsstockwager.c, stock capital investment
            simulation. The idea is to simulate an optimal wagering
            strategy, dynamically determining the Shannon probability
            by counting the up movements in a stock's value in a
            window from the stock's value time series, and using this
            to compute the fraction of the total capital to be
            invested in the stock for the next iteration of the time
            series, which is $2P - 1$, where $P$ is the Shannon
            probability. See,~\cite[pp. 129, 151]{Schroeder}.  The
            assumption is that a stock's price time series could be
            modeled as a fixed increment fractal.

            The input file structure is a text file consisting of
            records, in temporal order, one record per time series
            sample.  Blank records are ignored, and comment records
            are signified by a `\#' character as the first non white
            space character in the record. Data records must contain
            at least one field, which is the data value of the sample,
            but may contain many fields---if the record contains many
            fields, then the first field is regarded as the sample's
            time, and the last field as the sample's value at that
            time.

            An example output from the {\it tsstockwager}\/ program
            appears in Figure~\ref{stockwagerexample}.

        \subsection{tsbinomial}
            \subidx{programs}{tsbinomial}
            \subidx{tsbinomial}{program}
            \subidx{time series}{binomial distribution}
            \subidx{binomial distribution}{time series}
            \subidx{generation}{binomial distribution time series}
            \subidx{binomial distribution time series}{generation}
            \subidx{simulation}{binomial distribution}
            \subidx{binomial distribution}{simulation}

            Source tsbinomial.c, is for generating binomial
            distribution noise, with unfair bias, and cumulative
            sum---generates a time series.  The idea is to produce a
            $1/f$ squared power spectrum distribution by running a
            cumulative sum on a binomial distribution. The program
            accepts a an unfair bias and a wager
            factor. See~\cite[pp. 154]{Feder}.

            This program is a modification of the program {\it
            tscoin}\/. The wager fraction is computed by first
            calculating the optimal wager fraction, $f = 2P - 1$,
            where $P$ is the Shannon probability, and $f$ is the
            optimal wager fraction, (which is the root mean square $=$
            standard deviation of the normalized increments of the
            time series,) and then reducing this value by the standard
            deviation of the binomial distribution, which is the
            square root of the number of elements in the distribution,
            ie., the root mean square of the normalized increments of
            the cumulative sum is the same as the standard deviation
            of the binomial distribution.  See~\cite[pp. 155]{Feder}.

            An example output from the {\it tsbinomial}\/ program
            appears in Figure~\ref{binomialexample}.

            \begin{figure}[ht]
                \begin{center}
                    \begin{minipage}[t]{0.45\textwidth}
                        \epsfxsize=1.0\linewidth
                        \epsffile{../simulation/test/tscoin.tsunfairbrownian.tsstockwager.eps}
                        \caption[Example output of the {\it
                            tsstockwager}\/ program]{Example output of
                            the {\it tsstockwager}\/ program, using
                            the {\it tscoin}\/ program with a Shannon
                            probability of 0.6, which is shown in
                            Figure~\ref{coinexample}. The program {\it
                            tsunfairbrownian}\/ was used to change the
                            wager of the output of the {\it tscoin}\/
                            program from 0.2 to 0.1. This file was
                            used as the input to the {\it
                            tsstockwager}\/ program, and is shown in
                            Figure~\ref{unfairbrownianexample}.}
                        \label{stockwagerexample}
                    \end{minipage}
                    \hfill
                    \begin{minipage}[t]{0.45\textwidth}
                        \epsfxsize=1.0\linewidth
                        \epsffile{../simulation/test/tsbinomial.tsnormal.eps}
                        \caption[Example output of the {\it
                            tsbinomial}\/ program]{Example output of
                            the {\it tsbinomial}\/ program, using 1500
                            records. This is a plot of the frequency
                            histogram.}
                        \label{binomialexample}
                    \end{minipage}
                \end{center}
            \end{figure}

        \subsection{tsunfairfractional}
            \subidx{programs}{tsunfairfractional}
            \subidx{tsunfairfractional}{program}
            \subidx{time series}{changing unfair weights}
            \subidx{Newton---Raphson}{iterated solution}
            \subidx{iterated solution}{Newton---Raphson}

            Note: Conceptually, this program is used to ``weight'' the
            returns of a time series with a Gaussian distribution,
            ie., produce a fractional Brownian motion time series, as
            opposed to a Brownian distribution which would produce a
            Brownian time series, as produced by the program {\it
            tsunfairbrownian}\/. Unfortunately, the precision of the
            results were not encouraging, and the concept was
            abandoned. It is left in the program inventory for future
            reference.

            Source tsunfairfractional.c, unfair returns of a time
            series. The idea is to produce the returns of a time
            series which is weighted unfairly, by a Shannon
            probability, p. The input time series is presumed to have
            a Gaussian distribution. The main function of this program
            is regression scenario verification---given an empirical
            time series, a Shannon probability, and a ``wager''
            fraction, (which were probably derived from the program
            {\it tsshannon}\/,) speculative market pro forma
            performance can be analyzed.  Uses Newton---Raphson method
            for an iterative solution for the inverse function of the
            normal function. Also iterates, using Romberg integration,
            to calculate the cumulative interval value of the normal
            function.

            The input file structure is a text file consisting of
            records, in temporal order, one record per time series
            sample.  Blank records are ignored, and comment records
            are signified by a `\#' character as the first non white
            space character in the record. Data records must contain
            at least one field, which is the data value of the sample,
            but may contain many fields---if the record contains many
            fields, then the first field is regarded as the sample's
            time, and the last field as the sample's value at that
            time.

            \subidx{Newton---Raphson}{iterated solution}
            \subidx{iterated solution}{Newton---Raphson}
            As a reference on Newton---Raphson Method of root finding,
            see~\cite[pp. 270]{Press}.

            As a reference on Romberg Integration,
            see~\cite[pp. 124]{Press}.

            As a reference on trapezoid iteration,
            see~\cite[pp. 120]{Press}.

            As a reference on polynomial interpolation,
            see~\cite[pp. 90]{Press}.

            An example output from the {\it tsunfairfractional}\/
            program appears in Figure~\ref{unfairfractionalexample}.

        \subsection{tsintegers}
            \subidx{programs}{tsintegers}
            \subidx{tsintegers}{program}
            \subidx{time series}{integration}
            \subidx{integration}{time series}

            Source tsintegers.c, integers function generator---generates
            a time series.

            An example output from the {\it tsintegers}\/ program
            appears in Figure~\ref{integersexample}.

            \begin{figure}[ht]
                \begin{center}
                    \begin{minipage}[t]{0.45\textwidth}
                        \epsfxsize=1.0\linewidth
                        \epsffile{../simulation/test/tscoin.tsunfairbrownian.tsunfairfractional.eps}
                        \caption[Example output of the {\it
                            tsunfairfractional}\/ program]{Example
                            output of the {\it tsunfairfractional}\/
                            program, using the {\it tscoin}\/ program
                            with a Shannon probability of 0.6, which
                            is shown in Figure~\ref{coinexample}. The
                            program {\it tsunfairbrownian}\/ was used
                            to change the wager of the output of the
                            {\it tscoin}\/ program from 0.2 to
                            0.1. This file was used as the input to
                            the {\it tsunfairfractional}\/ program,
                            and is shown in
                            Figure~\ref{unfairbrownianexample}.}
                        \label{unfairfractionalexample}
                    \end{minipage}
                    \hfill
                    \begin{minipage}[t]{0.45\textwidth}
                        \epsfxsize=1.0\linewidth
                        \epsffile{../simulation/test/tsintegers.eps}
                        \caption[Example output of the {\it
                            tsintegers}\/ program]{Example output of
                            the {\it tsintegers}\/ program, using 1500
                            records.}
                        \label{integersexample}
                    \end{minipage}
                \end{center}
            \end{figure}

        \subsection{tsshannonstock}
            \subidx{programs}{tsshannonstock}
            \subidx{tsshannonstock}{program}
            \subidx{time series}{stock wager optimization}
            \subidx{stock wager optimization}{time series}
            \subidx{simulation}{stock wager optimization}
            \subidx{stock wager optimization}{simulation}

            Source tsshannonstock.c, is for simulating the gains of a
            stock investment using Shannon
            probability. See~\cite[pp. 128, 151]{Schroeder}.

            \noindent the algorithm used is:
            \begin{itemize}

                \item Let $I(t)$ be the amount of capital at time $t$.

                \item Let $W(t)$ be the amount of the capital wagered
                at time $t$.

                \item Let $V(t)$ be the value of the stock at time
                $t$.

            \end{itemize}

            \noindent Let $f$ be the fraction of the capital, wagered
            at any time, and assumed not to be a function of time.

            \begin{equation}
                W\left(t\right) = f I\left(t - 1\right)
            \end{equation}

            \begin{equation}
                I\left(t\right) = I\left(t - 1\right) + W\left(t\right) \frac{V\left(t\right) - V\left(t - 1\right)}{V\left(t - 1\right)}
            \end{equation}

            \begin{equation}
                I\left(t\right) = I\left(t - 1\right) + f  I\left(t - 1\right) \frac{V\left(t\right) - V\left(t - 1\right)}{V\left(t - 1\right)}
            \end{equation}

            \begin{equation}
                \frac{I\left(t\right)}{I\left(t - 1\right)} = 1 + f \frac{V\left(t\right) - V\left(t - 1\right)}{V\left(t - 1\right)}
            \end{equation}

            If it is assumed that the stock's price time series can be
            represented as a Brownian noise fractal, then the optimum
            value of $f$ would be:

            \begin{equation}
                f = 2P - 1
            \end{equation}

            \noindent where $P$ is the Shannon probability of the time
            series, found by:

            \begin{equation}
                P = \frac{\frac{avg}{rms} + 1}{2}
            \end{equation}

            \noindent where avg is the average, and rms is the root
            mean square, of the normalized increments of the stock's
            price time series, which can be calculated by

            \begin{equation}
                \frac{V\left(t\right) - V\left(1 - 1\right)}{V\left(t - 1\right)}
            \end{equation}

            \noindent for each data point in the time series.

            \noindent Represented in pseudo code:

            \begin{enumerate}

                \item for each data point in the stock's price time
                series, find the normalized increment from the
                following equation:

                \begin{equation}
                    \frac{V\left(t\right) - V\left(1 - 1\right)}{V\left(t - 1\right)}
                \end{equation}

                \item calculate the average of all normalized
                increments in the stock's price time series by the
                following equation:

                \begin{equation}
                    avg = \frac{1}{n}\sum_{i = 0}^{n} \frac{V\left(t\right) - V\left(1 - 1\right)}{V\left(t - 1\right)}
                \end{equation}

                \item calculate the root mean square of all normalized
                increments in the stock's price time series by the
                following equation:

                \begin{equation}
                    rms^{2} = \frac{1}{n}\sum_{i = 0}^{n}\left(\frac{V\left(t\right) - V\left(1 - 1\right)}{V\left(t - 1\right)}\right)^{2}
                \end{equation}

                \item calculate the Shannon probability, $P$, by the
                following equation:

                \begin{equation}
                    P = \frac{\frac{avg}{rms} + 1}{2}
                \end{equation}

                \item calculate the optimal fraction of the capital to
                be wagered, $f$ , by the following equation:

                \begin{equation}
                   f = 2P - 1
                \end{equation}

                \item since the stock's price time series already has
                a value rms as the root mean square of the normalized
                increments, for the optimal wagering strategy, the
                fraction should be divided by rms to provide a
                multiplier:

                \begin{equation}
                   multiplier = \frac{f}{rms}
                \end{equation}

                \noindent so that:

                \begin{equation}
                    \frac{I\left(t\right)}{I\left(t - 1\right)} = 1 + multiplier \cdot \frac{V\left(t\right) - V\left(1 - 1\right)}{V\left(t - 1\right)}
                \end{equation}

            \end{enumerate}

            What this means is that if you have capital, (ie, a
            portfolio,) $I(t)$, the fraction of $I(t)$ that should be
            wagered with each iteration of the game, (ie., time unit,)
            would be twice the Shannon probability minus unity, where
            the capital, (or portfolio,) is the sum total of cash on
            hand, $C(t)$, and the current value of stocks held, $V(t)
            \cdot N$, where $N$ is the number of stocks held, or:

            \begin{equation}
               I\left(t\right) = C\left(t\right) + V\left(t\right) \cdot N
            \end{equation}

            It would be convenient, from a comparative standpoint, to
            let $I(0)$, the beginning capital, be the same as $V(0)$,
            the price of the stock at the beginning of the simulation,
            so that the wagering strategy can be compared to the price
            of the stock over time.

            \noindent $N$ will be adjusted for the next game, (time
            unit,) such that:

            \begin{equation}
                N\left(t + 1\right) = \frac{I\left(t\right) \cdot f}{V\left(t\right)}
            \end{equation}

            \noindent where, as above, $f$ is the fraction of capital,
            (portfolio,) to be wagered:

            \begin{equation}
                f = fraction = 2P - 1 = \left(2 \cdot shannon\right) - 1
            \end{equation}

            It would, additionally, for the simulation, be convenient,
            from an information-theoretic standpoint, to let $f$ be a
            fraction, (either larger or smaller,) of the root mean
            square value of the normalized increments of the stock's
            price time series, ie., let $f = F \cdot rms$, where $F$
            is a constant value, (usually around unity,) and $rms$ is
            the average of the root mean square value of the
            normalized increments of the stock's price time
            series. This would allow a comparison of the stock's
            price, over time, to the capital, over time, with a
            wagering strategy that is optimal for a stock price that
            is characterized as a Brownian motion fractal over time.

            The input file structure is a text file consisting of
            records, in temporal order, one record per time series
            sample.  Blank records are ignored, and comment records
            are signified by a `\#' character as the first non white
            space character in the record. Data records must contain
            at least one field, which is the data value of the sample,
            but may contain many fields---if the record contains many
            fields, then the first field is regarded as the sample's
            time, and the last field as the sample's value at that
            time.

            An example output from the {\it tsshannonstock}\/ program
            appears in Figure~\ref{shannonstockexample}.

        \subsection{tsmarket}
            \subidx{Newton---Raphson}{iterated solution}
            \subidx{iterated solution}{Newton---Raphson}
            \subidx{searching for solutions}{methodology}
            \subidx{programs}{tsmarket}
            \subidx{tsmarket}{program}
            \subidx{time series}{market simulation}
            \subidx{market simulation}{time series}

            Source tsmarket.c, is for market simulation by fractional
            brownian noise generation, with unfair bias, and
            cumulative sum---generates a time series.  The idea is to
            produce a $1/f$ squared power spectrum distribution for
            each company in an industrial market by running a
            cumulative sum on a Gaussian power spectrum
            distribution. The aggregate of all companies participating
            in the market is obtained by summing the production of the
            individual companies.  The program accepts an unfair bias
            and a wager factor, and the number of companies in the
            market. See~\cite[pp. 172]{Feder}.  Uses Newton---Raphson
            method for an iterative solution for the probability, $p$.

            As an example, consider the Semiconductor Industry
            Association (SIA,) historical time series, (see the
            directory~../markets/ic.namerica,) data for the
            integrated circuit marketplace in North America:

            \begin{itemize}

                \item From the program {\it tsshannonwindow}\/, the
                Shannon probability, $P = 0.758207$.

                \item From the programs {\it tsfraction}\/ and {\it
                tsrms}\/, the root mean square value of the normalized
                increments, $rms = 0.087396$.

                \item From the programs {\it tsfraction}\/ and {\it
                tsavg}\/, the average of the normalized increments,
                $avg = 0.045132$.

            \end{itemize}

            Interestingly, the optimal rms value would be $rms = 2P -
            1 = 0.516414$, if the SIA time series could be represented
            a Brownian fractal, (ie., represented as a gambler's
            capital time series in an unfair coin toss
            game. See~\cite[pp. 128]{Schroeder}.)

            \noindent For this analysis, it is assumed that:

            \begin{enumerate}

                \item Each company acts independently, and will
                receive cash flow from the market.

                \item Some of this cash flow will be diverted into new
                product manufacturing, development, etc., which in
                turn will go back into the market, which in turn will
                create cash flow, and so on-but there is a random
                element in this process.

                \item Analysis of the SIA graph yields that it is
                probably a fractal, (fractional Brownian variety,)
                with a fairly accurate distribution of the normalized
                increments that appears to be Gaussian in nature, a
                range that appears to increase with the square root of
                time, and an exponential curvature. These are
                indicative of system that can be modeled by as a
                gambler's capital in an unfair coin toss game, or
                Brownian fractal.

            \end{enumerate}

            To analyze the SIA time series, it is interesting to note
            that the avg is $0.045132$, which would be the sum total
            of the average of all companies in the market. If the
            individual companies are assumed to be operating
            optimally, (and all identical,) then the $rms$ would be
            the square root of the $avg$, which is $0.212442934$.
            This would be the amount ``wagered'' in each iteration of
            the unfair coin game, (which is a Brownian fractal,) and
            the Shannon probability would be $0.212442934 = 2P - 1$,
            or $P = 0.606221467$.

            \noindent Using the program {\it tsmarket}\/:

            \noindent tsmarket -p 0.6 -c n 2500 > data

            \noindent The variable n was altered to approximate the
            statistical data of the SIA numbers. The best seems to be
            with $n = 5$:

            \begin{itemize}

                \item[] from tsshannonwindow, $P = 0.744495$

                \item[] from tsfraction and tsrms, $rms = 0.102880$

                \item[] from tsfraction and tsavg, $avg = 0.050307$

            \end{itemize}

            \noindent which compares favorably, to about $\pm 5\%$,
            with the original SIA numbers:

            \begin{itemize}

                \item[] from tsshannonwindow, $P = 0.758207$

                \item[] from tsfraction and tsrms, $rms = 0.087396$

                \item[] from tsfraction and tsavg, $avg = 0.045132$

            \end{itemize}

            which would tend to indicate that the constituent
            companies in the aggregate are operating optimally, and
            that the measurements on the aggregate sum of the market,
            ie., the SIA numbers, would indicate a higher Shannon
            probability, $P$, and a smaller root mean square value of
            the normalized increments, rms.

            \noindent The reason is as follows:

            \begin{enumerate}

                \item Consider a market that is supplied by a single
                company. The time series for the market could be
                represented, at least statistically, as an unfair coin
                tossing game, (see the {\it tscoins}\/ program,) with
                each time unit of manufacturing going into the
                marketplace, the marketplace returning cash to the
                company's P \& L, which is distributed to the
                company's operations to manufacture more product, and
                so on. But there is an element of randomness in this
                process that represents the aggregate of customer
                desires and market forces-this is assumed be a central
                limit phenomena, ie., it can be represented as a
                random variable with a normal, (Gaussian,)
                distribution. Note, that like the gambler, the
                company's operations managers are continually wagering
                on the future-and each wager may, or may not prove to
                be a successful.  It is further assumed that the
                company will commit capital to enhancing its market
                position, (ie., increase manufacturing capacity,
                develop new products, etc.,) and, as above, the
                decision to do so will contain an element of risk, and
                will sometimes work out, and sometimes not.

                \item Now consider that another company decides to
                participate in the marketplace-under the same
                scenario, above. If everything else is equal, we would
                expect the market, eventually, to be divided equally
                between the two companies, or each company would have
                half the market.  When the second company was added to
                the market, the first company's contribution to the
                marketplace was cut in half-and its root mean square
                value of its normalized increments contribution to the
                marketplace was also cut in half. The second company's
                contribution to the marketplace is the remaining one
                half, and its contribution to the root mean square
                value of its normalized increments is the same as the
                first company's. (The point is that the contributions
                to the marketplace add linearly, but the contribution
                of to the normalized increments of the marketplace add
                root mean square-so we would expect the root mean
                square value of the normalized increments to decrease
                when the number of participants in the marketplace
                changes from one to two-since the value of the
                normalized increments for each company is proportional
                to the contribution to its the market.)  Think of it
                as a Gaussian noise generator. If we cut the root mean
                square value (amplitude,) of the noise generator in
                one half, and add an identical noise generator, the
                resulting noise output of both generators will be the
                square root of two, divided by two.

                \item Or in general, the root mean square value of the
                normalized increments of a marketplace time series
                will be proportional to one over the square root of
                the number of companies in the market.

            \end{enumerate}

            \noindent The general outline of this program is:

            \begin{enumerate}

                \item given the Shannon probability, compute the
                abscissa value that divides the area under the normal
                curve, into two sections, such that the area to the
                left of the value, divided by the total area under the
                normal curve is the Shannon probability---a
                Newton-Raphson iterated approach using Romberg
                integration to find the area is used for this

                \item for each record:

                \begin{enumerate}

                    \item compute a gaussian distributed random
                    number

                    \item add the computed abscissa value to the
                    gaussian distributed number

                    \item multiply this number by the fraction of
                    cumulative sum to be wagered

                    \item add this number to the cumulative sum for
                    the company

                    \item add this number to the temporary aggregate
                    sum of the market

                \end{enumerate}

                \item add the temporary aggregate sum of the market to
                the aggregate sum of the market

            \end{enumerate}

            This program will require finding the value of the normal
            function, given the standard deviation. The method used is
            to use Romberg/trapezoid integration to numerically solve
            for the value.

            This program will require finding the functional inverse
            of the normal, ie., Gaussian, function. The method used is
            to use Romberg/trapezoid integration to numerically solve
            the equation:

            \begin{equation}
                F\left(x\right) = \int_{0}^{x} \frac{1}{2\pi} e^{\frac{-t^{2}}{2}} dt + 0.5
            \end{equation}

            \noindent which has the derivative:

            \begin{equation}
                f\left(x\right) = \frac{1}{2\pi} e^{\frac{-x^{2}}{2}}
            \end{equation}

            \noindent Since $F(x)$ is known, and it is desired to find
            $x$,

            \begin{equation}
                F\left(x\right) - \int_{0}^{x} \frac{1}{2\pi} e^{\frac{-t^{2}}{2}} dt + 0.5 = P\left(x\right) = 0
            \end{equation}

            \noindent and the Newton-Raphson method of finding roots
            would be:

            \begin{equation}
                P_{n + 1} = P_{n} - \frac{P\left(x\right)}{f\left(x\right)}
            \end{equation}

            An example output from the {\it tsmarket}\/ program
            appears in Figure~\ref{marketexample}.

            \begin{figure}[ht]
                \begin{center}
                    \begin{minipage}[t]{0.45\textwidth}
                        \epsfxsize=1.0\linewidth
                        \epsffile{../simulation/test/tsmarket.tsshannonstock.eps}
                        \caption[Example output of the {\it
                            tsshannonstock}\/ program]{Example output
                            of the {\it tsshannonstock}\/ program, the
                            input was produced by the {\it tsmarket}\/
                            program, with a Shannon probability of
                            0.6, and 5 companies participating in the
                            market, and is shown in
                            Figure~\ref{marketexample}.}
                        \label{shannonstockexample}
                    \end{minipage}
                    \hfill
                    \begin{minipage}[t]{0.45\textwidth}
                        \epsfxsize=1.0\linewidth
                        \epsffile{../simulation/test/tsmarket.eps}
                        \caption[Example output of the {\it
                            tsmarket}\/ program]{Example output of the
                            {\it tsmarket}\/ program with a Shannon
                            probability of 0.6 and 5 companies
                            participating in the market, using 500
                            records.}
                        \label{marketexample}
                    \end{minipage}
                \end{center}
            \end{figure}

        \subsection{tsstock}
            \subidx{programs}{tsstock}
            \subidx{tsstock}{program}
            \subidx{time series}{stock wager optimization}
            \subidx{stock wager optimization}{time series}
            \subidx{simulation}{stock wager optimization}
            \subidx{stock wager optimization}{simulation}

            Source tsstock.c, is for simulating the gains of a stock
            investment using Shannon probability.

            The input file structure is a text file consisting of
            records, in temporal order, one record per time series
            sample.  Blank records are ignored, and comment records
            are signified by a `\#' character as the first non white
            space character in the record. Data records must contain
            at least one field, which is the data value of the sample,
            but may contain many fields---if the record contains many
            fields, then the first field is regarded as the sample's
            time, and the last field as the sample's value at that
            time.

            A large mathematical infrastructure of analytical
            techniques and methodologies have been dedicated to the
            analysis of equity market time series.
            See,~\cite[pp. 249]{Crownover},~\cite[pp. 126]{Schroeder},~\cite{Peters:CAOITCM},~\cite[pp. 196,
            269, 273, 329]{Lewin},~\cite[pp. 195,
            214]{Casti:SFC},~\cite[pp. 82, 106, 102, 255,
            269]{Casti:C},~\cite[pp. 155]{Modis}.

            In addition, a large infrastructure of
            information---theoretic techniques have been suggested for
            optimal speculative wagering strategies in the equity
            markets, based, generally, on the suggested interpretation
            in the Kelly reference\footnote{``New Interpretation of
            Information Rate,'' J. L. Kelly, Jr., Bell System
            Technical Journal, Vol. 35, (July, 1956,) pp. 917.}.
            See,~\cite[pp. 270]{Pierce},~\cite[pp. 450]{Reza},~\cite[pp. 9]{Ash},~\cite[pp. 39]{Shannon},~\cite[pp. 155]{Klir},~\cite[pp. 151]{Schroeder}.

            This program is an investigation into whether a stock
            price time series could be modeled as a fractal Brownian
            motion time series, and, further, whether, a mechanical
            wagering strategy could be devised to optimize portfolio
            growth in the equity markets.

            Specifically, the paradigm is to establish an isomorphism
            between the fluctuations in a gambler's capital in the
            speculative unfair tossed coin game, as suggested
            in~\cite{Schroeder}, and speculative investment in the
            equity markets.  The advantage in doing this is that there
            is a large infrastructure in mathematics dedicated to the
            analysis and optimization of parlor games, specifically,
            the unfair tossed coin game. See Schroeder reference.

            Currently, there is a repository of historical price time
            series for stocks available at:

                \begin{center}
                    http://www.ai.mit.edu/stocks.html\\
                \end{center}

            \noindent that contains the historical price time series
            of many hundreds of stocks. The stock's prices are by
            close of business day, and are updated daily.

            The stock price history files in the repository are
            available via anonymous ftp, (ftp.ai.mit.edu,) and the
            programs {\it tsfraction}\/, {\it tsrms}\/, {\it tsavg}\/,
            and {\it tsnormal}\/ can be used to verify that, as a
            reasonable first approximation, stock prices can be
            represented as a fractional Brownian motion fractal, as
            suggested by Schroeder and Crownover.  (Note the
            assumption that, as a first approximation, a stock's price
            time series can be generated by independent increments.)

            This would tend to imply that there is an isomorphism
            between the underlying mechanism that produces the
            fluctuations in speculative stock prices and the the
            mechanism that produces the fluctuations in a gambler's
            capital that is speculating on iterations of an unfair
            tossed coin.

            If this is a reasonably accurate approximation, then the
            underlying mechanism of a stock's price time series can be
            analyzed, (by ``disassembling'' the time series,) and a
            wagering strategy, similar to that of the optimal wagering
            strategy in the iterated unfair coin tossing game, can be
            formulated to optimize equity market portfolio growth.

            As a note in passing, it is an important and subtile
            point, that there are ``operational'' differences in
            wagering on the iterated unfair coin game, and wagering on
            a stock. Specifically, in the coin game, a fraction of the
            gambler's capital is wagered on the speculative outcome of
            the toss of the coin, and, depending on whether the toss
            of the coin resulted in a win, (or a loss,) the wager is
            added to the gambler's capital, (or subtracted from it,)
            respectively.  However, in the speculative stock game, the
            gambler wagers on the anticipated {\it fluctuations}\/ of
            the stock's price, by purchasing the stock. The important
            difference is that the stock gambler does not win or loose
            an amount that was equal to the stock's price, (which was
            equivalent to the wager in the iterated unfair coin game,)
            but only the fluctuations of the stock's price, ie., it is
            an important concept that a portfolio's value (which has
            an investment in a stock,) and the stock's price do not,
            necessarily, ``track'' each other.

            In some sense, wagering on a stock is {\it not}\/ like a
            gambler wagering on the outcome of the toss of an unfair
            coin, but like wagering on the capital of the gambler that
            wagered on the outcome of the toss of an unfair coin. A
            very subtile difference, indeed.

            Note that the paradigm of the isomorphism between wagering
            on a stock and wagering in an unfair tossed coin game is
            that the graph, (ie., time series,) of the gambler's
            capital, who is wagering on the iterated outcomes of an
            unfair tossed coin, and the graph of a stock's price over
            time are statistically similar.

            If this is the case, at least in principle, it should be
            possible to ``dissect'' the time series of both ``games,''
            and determine the underlying statistical mechanism of
            both. Further, it should be, at least in principle,
            possible to optimize portfolio growth of speculative
            investments in the equity markets using
            information---theoretic entropic
            techniques. See,~\cite{Pierce},~\cite{Reza},
            and~\cite{Schroeder}.

            Under these assumptions, the amount of capital won or lost
            in each iteration of the unfair tossed coin game would be:

                \begin{equation}
                    V\left(t\right) - V\left(t - 1\right)
                \end{equation}

            \noindent for all data points in the gambler's capital
            time series.  This would correspond to the amount of money
            won or lost on each share of stock at each interval in the
            stock price time series.

            Likewise, the normalized increments of the gambler's
            capital time series can be obtained by subtracting the
            value of the gambler's capital in the last interval from
            the value of the gambler's capital in the current
            interval, and dividing by the value of the gambler's
            capital in the last interval:

                \begin{equation}
                    \frac{V\left(t\right) - V\left(t - 1\right)}{V\left(t - 1\right)}
                \end{equation}

            \noindent for all data points in the gambler's capital
            time series. This would correspond to the fraction of the
            gambler's capital that was won or lost on each iteration
            of the game, or, alternatively, the fraction that the
            stock price increased or decreased in each interval.

            The normalized increments are a very useful ``tool'' in
            analyzing time series data.  In the case of the unfair
            coin tossing game, the normalized increments are a
            ``graph,'' (or time series,) of the fraction of the
            capital that was won or lost, every iteration of the
            game. Obviously, in the unfair coin game, to win or lose,
            a wager had to be made, and the graph of the absolute
            value, or more appropriately, the root mean
            square\footnote{The absolute value of the normalized
            increments, when averaged, is related to the root mean
            square of the increments by a constant. If the normalized
            increments are a fixed increment, the constant is
            unity. If the normalized increments have a Gaussian
            distribution, the constant is $\approx 0.8$ depending on
            the accuracy of of ``fit'' to a Gaussian distribution.},
            of the normalized increments is the fraction of the
            capital that was wagered on each iteration of the game. As
            suggested in Schroeder, if an unfair coin has a chance,
            $P$, of coming up heads, (winning) and a chance $1 - P$,
            of coming up tails, (loosing,) then the optimal wagering
            strategy would be to wager a fraction, $f$, of the
            gambler's capital, on every iteration of the game, that
            is:

                \begin{equation}
                    f = 2P - 1
                \end{equation}

            This would optimize the exponential growth of the
            gambler's capital. Wagering more than this value would
            result in less capital growth, and wagering less than this
            value would result in less capital growth, over time. The
            variable $f$ is also equal to the root mean square of the
            normalized increments, $rms$, and the average, $avg$, of
            the normalized increments is the constant of the average
            exponential growth of the gambler's capital:

                \begin{equation}
                    C\left(t\right) = \left(1 + avg\right)^{t}
                \end{equation}

            \noindent where $C(t)$ is the gambler's capital. It can be
            shown that the formula for the probability, $P$, as a
            function of $avg$ and $rms$ is:

                \begin{equation}
                    P = \frac{\frac{avg}{rms} + 1}{2}
                \end{equation}

            \noindent where the empirical measurement of $avg$ and
            $rms$ are:

                \begin{equation}
                    avg = \frac{1}{n} \sum_{i = 0}^{n} \frac{V\left(t\right) - V\left(t - 1\right)}{V\left(t - 1\right)}
                \end{equation}

            \noindent and,

                \begin{equation}
                    rms^{2} = \frac{1}{n} \sum_{i = 0}^n \left(\frac{V\left(t\right) - V\left(t - 1\right)}{V\left(t - 1\right)}\right)^{2}
                \end{equation}

            \noindent respectively, (additionally, note that these
            formulas can be used to produce the running average and
            running root mean square, ie., they will work ``on the
            fly.'')

            The formula for the probability, $P$, will be true whether
            the game is played optimally, or not, ie., the game we are
            ``dissecting,'' may not be played with $f = 2P -
            1$. However, the formula for the probability, $P$:

                \begin{equation}
                    P' = \frac{rms + 1}{2}
                \end{equation}

            \noindent will be the same as $P$, only if the game is
            played optimally, (which, also, is applicable in ``on the
            fly'' methodologies.)

            Interestingly, the measurement, perhaps dynamically, (ie.,
            ``on the fly,'') of the average and root mean square of
            the normalized increments is all that is necessary to
            optimize the ``play of the game.'' Note that if $P'$ is
            smaller than $P$, then we need to increase $rms$, by
            increasing $f$, and, likewise, if $P'$ is larger than $P$,
            we need to decrease $f$. Thus, without knowing any of the
            underlying mechanism of the game, we can formulate a
            methodology for an optimal wagering strategy. (The only
            assumption being that the capital can be represented as an
            independent increment fractal---and, this too can, and
            should, be verified with meticulous application of fractal
            analysis using the programs {\it tsfraction}\/, {\it
            tsrms}\/, {\it tsavg}\/, and {\it tsnormal}.)

            At this point, it would seem that the optimal wagering
            strategy and analytical methodology used to optimize the
            growth of the gambler's capital in the the unfair tossed
            coin gain is well in hand. Unfortunately, when applying
            the methodology to the equity markets, one finds that, for
            almost all stocks, $P$ is greater than $P'$, perhaps
            tending to imply that in the equity markets, stocks are
            over priced.

            To illustrate a simple stock wagering strategy, suppose
            that analytical measurements are made on a stock's price
            time series, and it is found, conveniently, that $P = P'$,
            implying that $f = rms$, (after computing the normalized
            increments of the stock's price time series and
            calculating $avg$, $rms$, $P$, and $P'$.)  Note that in
            the optimized unfair coin tossing game, that wagering a
            fraction, $f = rms$, of the gambler's capital would
            optimize the exponential growth of the gambler's capital,
            and that the fluctuations, over time, of the gambler's
            capital would simply be the normalized increments of the
            gambler's capital. The root mean square of the
            fluctuations, over time, are the fraction of that the
            gambler's capital wagered, over time.  To achieve an
            optimal strategy when wagering on a stock, the objective
            would be that the normalized increments in the value of
            the portfolio, and the root mean square value of the
            normalized increments of the portfolio, also, satisfy the
            criteria, $f = rms$. Note that the fraction of the
            portfolio that is invested in the stock will have
            normalized increments that have a root mean square value
            that are the same as the root mean square value of the
            normalized increments of the stock.

            The issue is to determine the fraction of the stock
            portfolio that should be invested in the stock such that
            that fraction of the portfolio would be equivalent to the
            gambler wagering a fraction of the capital on a coin
            toss. It is important to note that the optimized wagering
            strategy used by the gambler, when wagering on the outcome
            of a coin toss, is to never wager the entire capital, but
            to hold some capital in reserve, and wager only a fraction
            of the capital---and in the optimum case this wager
            fraction is $f = rms$. In a stock portfolio, even though
            the investment is totally in stocks, it could be
            considered that some of this value is wagered, and the
            rest held in reserve. The amount wagered would be the root
            mean square of the normalized increments of the stocks
            price, and the amount held in reserve would be the
            remainder of the portfolio's value. (Note the
            paradigm---there is an isomorphism between the fluctuating
            gambler's capital in the unfair coin tossing game, and the
            fluctuating value of a stock portfolio.)  In the simple
            case where $P = P'$, the fraction of the portfolio value
            that should be invested in the stock is $f = $ root mean
            square of the stock's normalized increments, which would
            be the same as $f = 2P - 1$, where $P =
            \frac{\frac{avg}{rms} + 1}{2}$ or $P = \frac{rms + 1}{2}$.
            Note that the fluctuations in the value of the portfolio
            do to the fluctuations in the stocks price would be
            statistically similar to the fluctuations in the gambler's
            capital when playing the unfair coin tossing game.

            This also leads to a generality, where $P$ and $P'$ are not equal. If
            the root mean square of the normalized increments of the stock price
            time series are too small, say by a factor of $2$, then the fraction
            of the portfolio invested in the stock should be increased, by a
            factor of $2$ (in this example.) This would make the root mean square
            of the fluctuations in the value of the portfolio the same as the the
            root mean square of the fluctuations in the gambler's capital under
            similar statistical circumstances, (albeit with twice as much of the
            portfolio's equivalent ``cash reserves'' tied up in the investment in
            the stock.

            To calculate the ratio by which the fraction of the
            portfolio invested in a stock must be increased:

                \begin{equation}
                    P = \frac{\frac{avg}{rms} + 1}{2}
                \end{equation}

            \noindent and,

                \begin{equation}
                    f = 2P - 1 = rms
                \end{equation}

            \noindent and letting the measured $rms$ by $rms_{m}$,

                \begin{equation}
                    f = 2P - 1 = 2 \frac{\frac{avg_{m}}{rms_{m}} + 1}{2} - 1 = \frac{avg_{m}}{rms_{m}} = rms
                \end{equation}

            \noindent (Note that both of the values, $avg$ and $rms$,
            are functions of the probability, $P$, but their ratio is
            not.)

            \noindent and letting $F$ be the ratio by which the
            fraction of the portfolio invested in a stock must be
            increased to accommodate $P$ not being equal to $P'$:

                \begin{equation}
                    F = \frac{rms}{rms_{m}} = \frac{avg_{m}}{rms_{m}^{2}}
                \end{equation}

            \noindent and multiplying both sides of the equation by
            $f$, to get the fraction of the portfolio that should be
            invested in the stock while accommodating $P$ not being
            equal to $P'$:

                \begin{equation}
                    F \cdot f = \frac{avg_{m}}{rms_{m}^{2}} \cdot \frac{avg_{m}}{rms_{m}} = \frac{avg_{m}^{2}}{rms_{m}^{3}}
                \end{equation}

            \noindent which can be computed, dynamically, or ``on the
            fly,'' and where avg and rms are the average and root mean
            square of the normalized increments of the stock's price
            time series, and assuming that the stock's price time
            series is composed of independent increments, and can be
            represented as a fractional Brownian motion fractal.

            Representing such an algorithm in pseudo code:

                \begin{enumerate}

                    \item for each data point in the stock's price
                    time series, find the, possibly running,
                    normalized increment from the following equation:

                        \begin{equation}
                            \frac{V\left(t\right) - V\left(t - 1\right)}{V\left(t - 1\right)}
                        \end{equation}

                    \item calculate the, possibly running, average of
                    all normalized increments in the stock's price
                    time series by the following equation:

                        \begin{equation}
                            avg = \frac{1}{n} \sum_{i = 0}^{n} \frac{V\left(t\right) - V\left(t - 1\right)}{V\left(t - 1\right)}
                        \end{equation}

                    \item calculate the, possibly running, root mean
                    square of all normalized increments in the stock's
                    price time series by the following equation:

                        \begin{equation}
                            rms^{2} = \frac{1}{n} \sum_{i = 0}^n \left(\frac{V\left(t\right) - V\left(t - 1\right)}{V\left(t - 1\right)}\right)^{2}
                        \end{equation}

                    \item calculate the, possibly running, fraction of
                    the portfolio to be invested in the stock, $F
                    \cdot f$:

                        \begin{equation}
                            F \cdot f = \frac{avg_{m}^{2}}{rms_{m}{3}}
                        \end{equation}

                \end{enumerate}

            To reiterate what we have so far, consider a gambler,
            iterating a tossed unfair coin. The gambler's capital,
            over time, could be a represented as a Brownian fractal,
            on which measurements could be performed to optimize the
            gambler's wagering strategy. There is supporting evidence
            that stock prices can be ``modeled'' as a Brownian
            fractal, and it would seem reasonable that the
            optimization techniques that the gambler uses could be
            applied to stock portfolios. As an example, suppose that
            it is desired to invest in a stock. We would measure the
            average and root mean square of the normalized increments
            of the stock's price time series to determine a wagering
            strategy for investing in the stock. Suppose that the
            measurement yielded that the the the fraction of the
            capital to be invested, $f$, was $0.2$, (ie., a Shannon
            probability of $0.6$,) then we might invest the entire
            portfolio in the stock, and our portfolio would be modeled
            as $20\%$ of the portfolio would be wagered at any time,
            and $80\%$ would be considered as ``cash reserves,'' even
            though the $80\%$ is actually invested in the
            stock. Additionally, we have a metric methodology,
            requiring only the measurement of the average and root
            mean square of the increments of the stock price time
            series, to formulate optimal wagering strategies for
            investment in the stocks.  The assumption is that the
            stock's price time series is composed of independent
            increments, and can be represented as a fractional
            Brownian motion fractal, both of which can be verified
            through a metric methodology.

            Note the isomorphism. Consider a gambler that goes to a
            casino, buys some chips, then plays many iterations of an
            unfair coin tossing game, and then cashes in the
            chips. Then consider investing in a stock, and some time
            later, selling the stock. If the Shannon probability of
            the time series of the unfair coin tossing game is the
            same as the time series of the stock's value, then both
            ``games'' would be statistically similar. In point of
            fact, if the toss of the unfair coin was replaced with
            whether the stock price movement was up or down, then the
            two time series would be identical. The implication is
            that stock values can be modeled by an unfair tossed
            coin. In point of fact, stock values are, generally,
            fractional Brownian motion in nature, implying that the
            day to day fluctuations in price can be modeled with a
            time sampled unfair tossed coin game.

            There is an implication with the model. It would appear
            that the ``best'' portfolio strategy would be to
            continually search the stock market exchanges for the
            stock that has the largest value of the quotient of the
            average and root mean square of the normalized increments
            of the stock's price time series, (ie.,
            $\frac{avg}{rms}$,) and invest $100\%$ of the portfolio in
            that single stock. This is in contention with the concept
            that a stock portfolio should be ``diversified,'' although
            it is not clear that the prevailing concept of
            diversification has any scientific merit.

            To address the issue of diversification of stocks in a
            stock portfolio, consider the example where a gambler,
            tossing an unfair coin, makes a wager.  If the coin has a
            $60\%$ chance of coming up heads, then the gambler should
            wager $20\%$ of the capital on hand on the next toss of
            the coin. The remaining $80\%$ is kept as ``cash
            reserves.'' It can be argued that the cash reserves are
            not being used to enhance the capital, so the gambler
            should play multiple games at once, investing all of the
            capital, investing $20\%$ of the capital, in each of $5$
            games at once, (assuming that the coins used in each game
            have a probability of coming up heads $60\%$ of the
            time---note that the fraction of capital invested in each
            game would be different for each game if the probabilities
            of the coins were different, but could be measured by
            calculating the $\frac{avg}{rms}$ of each game.)

            Likewise, with the same reasoning, we would expect that
            stock portfolio management would entail measuring the
            quotient of the average and root mean square of the
            normalized increments of every stock's price time series,
            (ie., $\frac{avg}{rms}$,) choosing those stocks with the
            largest quotient, and investing a fraction of the
            portfolio that is equal to the this quotient. Note that
            with an $\frac{avg}{rms} = 0.1$, (corresponding to a
            Shannon probability of $0.55$---which is ``typical'' for
            the better performing stocks on the New York Stock
            Exchange,) we would expect the portfolio to be diversified
            into $10$ stocks, which seems consistent with the
            recommendations of those purporting diversification of
            portfolios. In reality, since most stocks in the United
            States exchanges, (at least,) seem to be ``over priced,''
            (ie., $P$ larger than $P'$,) it will take more capital
            than is available in the value of the portfolio to invest,
            optimally, in all of the stocks in the portfolio, (ie.,
            the fraction of the portfolio that has to be invested in
            each stock, for optimal portfolio performance, will sum to
            greater than $100\%$.) The interpretation, I suppose, in
            the model, is that at least a portion of the investment in
            each stock would be on ``margin,'' which is a relatively
            low risk investment, and, possibly, could be extended into
            a formal optimization of ``buying stocks on the margin.''

            The astute reader would note that the fractions of the
            portfolio invested in each stock was added linearly, when
            these values are really the root mean square of the
            normalized increments, implying that they should be added
            root mean square. The rationale in linear addition is that
            the Hurst Coefficient in the near term is near unity, and
            for the far term $0.5$. (By definition, this is the
            characteristic of a Brownian motion fractal process.)
            Letting the Hurst Coefficient be $H$, then the method of
            summing multiple processes would be:

            \begin{equation}
                V_{tot}^H = V_1^H + V_2^H + \cdots
            \end{equation}

            \noindent so in the far term, the values would be added
            root mean square, and in the near term, linearly. Note
            that this is also a quantitative definition of the terms
            ``near term'' and ``far term.''  Since the Hurst
            Coefficient plot is on a log-log scale, the demarcation
            between the two terms is where $1 - \ln (t) = 0.5 \cdot
            \ln (t)$, or when $\ln (t) = 2$, or $t = 7.389\ldots$ The
            important point is that the ``root mean square formula''
            used varies with time. For the near term, $H \approx 1$,
            and linear addition is used. For the far term, a root mean
            square summation process is used. (Note, also, that a far
            term $H$ of $0.5$ is unique to Brownian motion
            fractals. In general, it can be different than $0.5$. If
            it is larger than $0.5$, then it is termed fractional
            Brownian motion, depending on who is doing the defining.)

            There are some interesting implications to this near
            term/far term interpretation. First, the
            ``forecastability'' is better in the near term than far
            term-which could be interpreted as meaning that short term
            strategies would yield better portfolio performance than
            long term strategies---see
            \cite[pp. 83-84]{Peters:CAOITCM}.  Secondly, it can be
            used to optimize portfolio long term strategy. For
            example, suppose that a stock's Shannon probability is
            $0.52$, and all stocks in the portfolio have the same
            Shannon probability. This means that the portfolio should
            consist of $25$ stocks. However, in the long run, the
            portfolio would have a root mean square value of the
            square root of $25$ times $0.04$, or $0.2$. This would
            tend to imply that, on the average, over the long run, the
            stock portfolio would be one fifth of the total
            investments. Naturally, this ratio could be adjusted, over
            time, depending on the instantaneous value of the Shannon
            probabilities of all different investments, like bonds,
            metals, etc.

            This would imply that ``timing of the market'' would have
            to be initiated to adjust the ratio of investment in
            stocks. One of the implications of entropic theory is that
            this is impossible. However, as the Shannon probability of
            the various investments change, statistical estimation can
            be used to asses the statistical accuracy of these
            movements, and the ratios adjusted accordingly. This would
            tend to suggest that adaptive computational control system
            methodology would be an applicable alternative.

            As a note in passing, the average and root mean square of
            the normalized increments of a stock's price time series,
            $avg$ and $rms$, respectively, represent a qualitative
            metric of the stock. The average, $avg$, is an expression
            of the stock's growth in price, and the root mean square,
            $rms$, is a expression of the stock's price volatility. It
            would seem, incorrectly, at first glance that stocks
            should be selected that have high price growth, and low
            price volatility---however, it is a more complicated issue
            since $avg$ and $rms$ are interrelated, and not
            independent of each other.

            In the diversified portfolio, the ``volatilities'' of the
            individual stocks add root mean square to the volatility
            of the portfolio value, so, everything else being equal,
            we would expect that the volatility of the portfolio value
            to be about $\frac{1}{3}$ the volatility of the stocks
            that make up the portfolio. (The ratio $\frac{1}{3}$ came
            from the $\sqrt{\frac{1}{10}}$, which is about
            $\frac{1}{3}$.) (There is a qualification here, it is
            assumed that all stock price time series are made up of
            independent increments, and can be represented as a
            fractional Brownian motion fractal---note that this
            statement is not true if the time series is characterized
            as simple Brownian motion, like the gambler's capital in
            the unfair coin toss game---see~\cite[pp. 157]{Schroeder},
            for details.)  So, it can be supposed, if one desires
            maximum performance in a stock portfolio, then one should
            search the stock market exchanges for the stock that has
            the highest quotient of the average and root mean square
            of the normalized increments of stock price time series,
            and invest $100\%$ of the portfolio in that stock. As an
            alternative strategy, one could diversify the portfolio,
            investing in multiple stocks, and lower the portfolio
            volatility at the expense of lower portfolio performance.
            Arguments can probably be made for both strategies.

            As a note in passing, I have made the statement that, at
            least in the United States exchanges, stocks tend to be
            over priced.  The rationale behind the statement is as
            follows. If the stock's price time series represents an
            independent increment, fractional Brownian fractal, and if
            the stock's price performance is optimal, then the
            equation:

                \begin{equation}
                    f = 2P - 1
                \end{equation}

            \noindent where $P$ is the Shannon probability for the
            stock's price time series, and $f$ is the fraction of the
            capital wagered per game, (or unit time, and where the
            capital is the stock's price,) will represent fluctuations
            in the stock's price, since the symbol $f$ is also the
            root mean square value of the normalized increments of the
            stock's time series. Also, the absolute value of the time
            derivative of the stock's price time series is the
            fluctuations in the stocks price, ie., at any instant, if
            $V$ is the stock's price, then $fV$ will be the
            fluctuation in price, which is the derivative, $D =
            \frac{dV}{dt}$, or, $V = D / f$. In other words, the fair
            market value of the stock, in relation to the normalized
            increments of the stock's value, will be the derivative of
            the stock's price, divided by the root mean square of the
            normalized increments of the stock's price, which is also
            $f$\footnote{The absolute value of the normalized
            increments, when averaged, is related to the root mean
            square of the increments by a constant. If the normalized
            increments are a fixed increment, the constant is
            unity. If the normalized increments have a Gaussian
            distribution, the constant is $\approx 0.8$ depending on
            the accuracy of of ``fit'' to a Gaussian
            distribution.}. If the argument has merit, then, at least
            the stocks available from
            http://www.ai.mit.edu/stocks.html would seem to be over
            priced. (It is a straight forward shell programming
            exercise, using the programs {\it tsderivative}\/, {\it
            tsfraction}\/, {\it tsmath}\/, and {\it tsrms}\/, to
            verify this.)

            A final derivation, following~\cite{Reza}. Consider the
            case of a gambler with a private wire into the future who
            places wagers on the outcomes of a game of chance. We
            assume that the side information which he receives has a
            probability, $P$, of being true, and of $1 - P$, of being
            false. Let the original capital of gambler be $V(0)$, and
            $V(n)$ his capital after the $n$'th wager. Since the
            gambler is not certain that the side information is
            entirely reliable, he places only a fraction, $f$, of his
            capital on each wager. Thus, subsequent to $n$ many
            wagers, assuming the independence of successive tips from
            the future, his capital is:

                \begin{equation}
                    V\left(n\right) = \left(1 + f\right)^{w} \left(1 - f\right)^{l} V\left(0\right)
                \end{equation}

            \noindent where $w$ is the number of times he won, and $l
            = n - w$, the number of times he lost. These numbers are,
            in general, values taken by two random variables, denoted
            by $W$ and $L$. According to the law of large numbers:

                \begin{equation}
                    \lim_{n\to\infty} \frac{1}{n} W = P
                \end{equation}

            \noindent and:

                \begin{equation}
                    \lim_{n\to\infty} \frac{1}{n} L = q = 1 - P
                \end{equation}

            The problem with which the gambler is faced is the
            determination of $f$ leading to the maximum of the average
            exponential rate of growth of his capital. That is, he
            wishes to maximize the value of:

                \begin{equation}
                    G = \lim_{n\to\infty} \frac{1}{n} \ln \frac{V\left(n\right)}{V\left(0\right)}
                \end{equation}

            \noindent with respect to $f$, assuming a fixed original
            capital and specified $P$:

                \begin{equation}
                    G = \lim_{n\to\infty} \frac{W}{n} \ln \left(1 + f\right) + \frac{L}{n} \ln \left(1 - f\right)
                \end{equation}

            \noindent or:

                \begin{equation}
                    G = P \ln \left(1 + f\right) + q \ln \left(1 - f\right)
                \end{equation}

            \noindent which, by taking the derivative with respect to
            $f$, and equating to zero, can be shown to have a maxima
            when:

                \begin{equation}
                    \frac{dG}{df} = P\left(1 + f\right)^{P - 1} \left(1 - f\right)^{1 - P} - \left(1 - P\right)\left(1 - f\right)^{1 - P - 1} \left(1 + f\right)^{P} = 0
                \end{equation}

            \noindent combining terms:

                \begin{equation}
                    P\left(1 + f\right)^{P - 1} \left(1 - f\right)^{1 - P} - \left(1 - P\right) \left(1 - f\right)^{P} \left(1 + f\right)^{P} = 0
                \end{equation}

            \noindent and splitting:

                \begin{equation}
                    P\left(1 + f\right)^{P - 1} \left(1 - f\right)^{1 - P} = \left(1 - P\right) \left(1 - f\right)^{P} \left(1 + f\right)^{P}
                \end{equation}

            \noindent then taking the logarithm of both sides:

                \begin{equation}
                    \ln \left(P\right) + \left(P - 1\right) \ln \left(1 + f\right) + \left(1 - P\right) \ln \left(1 - f\right) = \ln \left(1 - P\right) - P \ln \left(1 - f\right) + P \ln \left(1 + f\right)
                \end{equation}

            \noindent and combining terms:

                \begin{equation}
                    \left(P - 1\right) \ln \left(1 + f\right) - P \ln \left(1 + f\right) + \left(1 - P\right) \ln \left(1 - f\right) + P \ln \left(1 - f\right) = \ln \left(1 - P\right) - \ln \left(P\right)
                \end{equation}

            \noindent or:

                \begin{equation}
                    \ln \left(1 - f\right) - \ln \left(1 + f\right) = \ln \left(1 - P\right) - \ln \left(P\right)
                \end{equation}

            \noindent and performing the logarithmic operations:

                \begin{equation}
                    \ln \left(\frac{1 - f}{1 + f}\right) = \ln \left(\frac{1 - P}{P}\right)
                \end{equation}

            \noindent and exponentiating:

                \begin{equation}
                    \frac{1 - f}{1 + f} = \frac{1 - P}{P}
                \end{equation}

            \noindent which reduces to:

                \begin{equation}
                    P\left(1 - f\right) = \left(1 - P\right) \left(1 + f\right)
                \end{equation}

            \noindent and expanding:

                \begin{equation}
                    P - Pf = 1 - Pf - P + f
                \end{equation}

            \noindent or:

                \begin{equation}
                    P = 1 - P + f
                \end{equation}

            \noindent and, finally:

                \begin{equation}
                    f = 2P - 1
                \end{equation}

            As a passing note, the methodology used in this derivation
            comes from information---theoretic concepts, formally
            called entropic principles, and is firmly entrenched
            branch of market and economic analysis.

            Continuing with the derivation of the methodology used
            herein, consider a gambler, wagering on the iterated
            outcomes of an unfair tossed coin game. A fraction, $f$,
            of the gambler's capital will be wagered on the outcome of
            each iteration of the unfair tossed coin, and if the coin
            comes up heads, with a probability, $P$, then the gambler
            wins the iteration, (and an amount equal to the wager is
            added to the gambler's capital,) and if the coin comes up
            tails, with a probability of $1 - P$, then the gambler
            looses the iteration, (and an amount of the wager is
            subtracted from the gambler's capital.)

            As a passing note, the iterations of a random variable, a
            flipped coin in this case, that are added together (ie.,
            to a cumulative sum,) the gambler's capital, in this case,
            are called ``fractal'' processes. The origins of the name
            are recent and obscure, but there are different varieties
            of fractal processes. In this case, since the distribution
            of the increments is either plus or minus one, it is
            called a Brownian motion fractal. If the distribution of
            the increments had a Gaussian, or normal distribution, it
            would be called a fractional Brownian motion
            fractal. (Typically distribution of the increments in a
            stock price time series fall someplace in between the
            two.) The analytical methodology of investigation into
            such matters is called ``fractal analysis,'' and that is
            what is going to be done here, in general, for the
            gambler's capital, which is a Brownian motion fractal.

            If we let the outcome of the first coin toss, (ie.,
            whether it came up as a win or a loss,) be $c(1)$ and the
            outcome of the second toss be $c(2)$, and so on, then the
            outcome of the $n$'th toss, $c(n)$, would be:

                \begin{equation}
                    C\left(n\right) = \left\{ \begin{array}{l l}
                                      win, & \mbox{with a probability of P}\\
                                      loose, & \mbox{with a probability of 1 - P}
                                      \end{array}
                    \right.
                \end{equation}

            \noindent for convenience, let a win to be represented by $+1$, and a
            loss by $-1$:

                \begin{equation}
                    C\left(n\right) = \left\{ \begin{array}{l l}
                                      +1, & \mbox{with a probability of P}\\
                                      -1, & \mbox{with a probability of 1 - P}
                                      \end{array}
                    \right.
                \end{equation}

            \noindent for the reason that when we multiply the wager,
            $f$, by $c(n)$, it will be a positive number, (ie., the
            wager will be added to the capital,) and for a loss, it
            will be a negative number, (ie., the wager will be
            subtracted from the capital.)  This is convenient, since
            the increment, by with the gambler's capital increased or
            decreased in the $n$'th iteration of the game is $f \cdot
            c(n)$.

            If we let $C(0)$ be the initial value of the gambler's
            capital, $C(1)$ be the value of the gambler's capital
            after the first iteration of the game, then:

                \begin{equation}
                    C\left(1\right) = C\left(0\right) \cdot \left(1 + c\left(1\right) \cdot f\left(1\right)\right)
                \end{equation}

            \noindent after the first iteration of the game, and:

                \begin{equation}
                    C\left(2\right) = C\left(0\right) \cdot \left(\left(1 + c\left(1\right) \cdot f\left(1\right)\right) \cdot \left(1 + c\left(2\right) \cdot f\left(2\right)\right)\right)
                \end{equation}

            \noindent after the second iteration of the game, and, in
            general, after the $n$'th iteration of the game:

                \begin{equation}
                    C\left(n\right) = C\left(0\right) \cdot \left(\left(1 + c\left(1\right) \cdot f\left(1\right)\right) \cdot \left(1 + c\left(2\right) \cdot f\left(2\right)\right) \cdot\hspace{0.1in} \cdots \hspace{0.1in} \cdot \left(1 + c\left(n\right) \cdot f\left(n\right)\right) \cdot \left(1 + c\left(n + 1\right) \cdot f\left(n + 1\right)\right)\right)
                \end{equation}

            For the normalized increments of the time series of the
            gambler's capital, it would be convenient to rearrange
            these formulas. For the first iteration of the game:

                \begin{equation}
                    C\left(1\right) - C\left(0\right) = C\left(0\right) \cdot \left(1 + c\left(1\right) \cdot f\left(1\right)\right) - C\left(0\right)
                \end{equation}

            \noindent or:

                \begin{equation}
                    \frac{C\left(1\right) - C\left(0\right)}{C\left(0\right)} = \frac{C\left(0\right) \cdot \left(1 + c\left(1\right) \cdot f\left(1\right)\right) - C\left(0\right)}{C\left(0\right)}
                \end{equation}

            \noindent and after reducing, the first normalized
            increment of the gambler's capital time series is:

                \begin{equation}
                    \frac{C\left(1\right) - C\left(0\right)}{C\left(0\right)} = \left(1 + c\left(1\right) \cdot f\left(1\right)\right) - 1 = c\left(1\right) \cdot f\left(1\right)
                \end{equation}

            \noindent and for the second iteration of the game:

                \begin{equation}
                C\left(2\right) = C\left(0\right) \cdot \left(\left(1 + c\left(1\right) \cdot f\left(1\right)\right) \cdot \left(1 + c\left(2\right) \cdot f\left(2\right)\right)\right)
                \end{equation}

            \noindent but $C(0) \cdot ((1 + c(1) \cdot f(1))$ is simply $C(1)$:

                \begin{equation}
                    C\left(2\right) = C\left(1\right) \cdot \left(1 + c\left(2\right) \cdot f\left(2\right)\right)
                \end{equation}

            \noindent or:

                \begin{equation}
                    C\left(2\right) - C\left(1\right) = C\left(1\right) \cdot \left(1 + c\left(2\right) \cdot f\left(2\right)\right) - C\left(1\right)
                \end{equation}

            \noindent which is:

                \begin{equation}
                    \frac{C\left(2\right) - C\left(1\right)}{C\left(1\right)} = \frac{C\left(1\right) \cdot \left(1 + c\left(2\right) \cdot f\left(2\right)\right) - C\left(1\right)}{C\left(1\right)}
                \end{equation}

            \noindent and after reducing, the second normalized
            increment of the gambler's capital time series is:

                \begin{equation}
                    \frac{C\left(2\right) - C\left(1\right)}{C\left(1\right)} = 1 + c\left(2\right) \cdot f\left(2\right) - 1 = c\left(2\right) \cdot f\left(2\right)
                \end{equation}

            \noindent and it should be obvious that the process can be
            repeated indefinitely, so, the $n$'th normalized increment
            of the gambler's capital time series is:

                \begin{equation}
                    \frac{C\left(n\right) - C\left(n - 1\right)}{C\left(n\right)} = c\left(n\right) \cdot f\left(n\right)
                \end{equation}

            Note that we can tell the fraction of the capital that the
            gambler wagered in the n'th iteration, it is simply the
            absolute value of the normalized increment for the
            iteration, $\left| c(n) \cdot f(n) \right|$, ie., $c(n)
            \cdot f(n)$ is what was won or lost in the $n$'th
            iteration, and removing $c(n) = \pm 1$, is the fraction of
            the wager. Another, more formal alternative, is to square
            the $n$'th normalized increment, (which, also, removes any
            negative sign,) and then take the square root of the
            square. Which leads to the formalization for the root mean
            square of the normalized increments, $rms$, (provided that
            $n$ is sufficiently large\footnote{The absolute value of
            the normalized increments, when averaged, is related to
            the root mean square of the increments by a constant. If
            the normalized increments are a fixed increment, the
            constant is unity. If the normalized increments have a
            Gaussian distribution, the constant is $\approx 0.8$
            depending on the accuracy of of ``fit'' to a Gaussian
            distribution.}):

                \begin{equation}
                    rms^{2} = \frac{1}{n} \sum_{i = 0}^n \left(\frac{C\left(t\right) - C\left(t - 1\right)}{C\left(t - 1\right)}\right)^{2}
                \end{equation}

            This is an important concept, since it shows that $rms =
            f$, or:

                \begin{equation}
                    rms^{2} = \frac{1}{n} \sum_{i = 0}^n f^{2} = \frac{1}{n} n \cdot f^{2} = f^{2}
                \end{equation}

            \noindent or, importantly:

                \begin{equation}
                    rms = f
                \end{equation}

            For the average, $avg$, of the normalized increments of
            the gambler's capital, consider that in an interval of $n$
            many iterations of the game, (provided that $n$ is
            sufficiently large,) there will be $P$ many wins, and $1 -
            P$ many losses, and since the gambler's capital increased
            by $+f$ for the wins, and $-f$ for the losses, or:

                \begin{equation}
                    avg = f \cdot \left[P - \left(1 - P\right)\right] = f \cdot \left(2P - 1\right)
                \end{equation}

            \noindent but since f = rms:

                \begin{equation}
                    avg = rms \cdot \left(2P - 1\right)
                \end{equation}

            \noindent or:

                \begin{equation}
                    \frac{avg}{rms} = 2P - 1
                \end{equation}

            \noindent and rearranging:

                \begin{equation}
                    2P = \frac{avg}{rms} + 1
                \end{equation}

            \noindent and solving for $P$:

                \begin{equation}
                    P = \frac{\frac{avg}{rms} + 1}{2}
                \end{equation}

            Which is the formula for the Shannon probability, $P$, as
            a function of the average and root mean square of the
            normalized increments of the gambler's capital, $avg$ and
            $rms$, respectively. It is an important concept that with
            the measurement of these two quantities, (and the metrics
            on these two quantities can be deduced dynamically, or
            ``on the fly,'') that an optimal wagering strategy, (or
            cash flow optimization,) can be formulated.

            It should be noted that this derivation is for analyzing a
            time series that is characterized as a Brownian motion
            fractal. A similar derivation can be used for time series
            that are characterized by fractional Brownian
            motion. However, the derivation is much more formidable,
            mathematically.

            As a matter of practical interest, the term ``provided
            that $n$ is sufficiently large'' needs to be
            qualified. Note that when the term ``running average'' or
            ``running root mean square'' is used, we really need to
            know how many iterations of coin tosses, $n$, are
            necessary to be considered ``sufficiently large.'' If we
            consider the formula:

                \begin{equation}
                    P = \frac{\frac{avg}{rms} + 1}{2}
                \end{equation}

            \noindent and noting that the Shannon probability, $P$,
            has a range $0 \leq P \leq 1$, and we are using a summing
            process for both the average, and root mean square of the
            normalized increments, then n would have to be $100$ to
            achieve a somewhat less than $1\%$ error in $P$. The
            reasoning is that if we sum $100$ ones, then the resultant
            sum would be $100$, and the next iteration that is to be
            added to the sum could create at most a $1\%$ error. The
            implication of this is that one should use a window of at
            least $100$ time units. (hours, days, weeks, or whatever
            is being used as a unit time in the time series being
            analyzed,) to achieve a $1\%$, or better uncertainty in
            $P$. In stock price performance analysis, this is a
            marginal accuracy, so a larger window size would be
            recommended. A more formal methodology would use the
            program {\it tsstatest}\/ to determine, precisely, the
            size of the data set required.

            As a few examples of using very simple programs to perform
            fractal metric analysis on stock time series:

                \begin{center}
                    tscoin -p 0.6 2500 \\
                \end{center}

            \noindent would generate a fractal time series
            characterized by optimal Brownian motion consisting of
            $2500$ records, and a Shannon probability, $P$, of $0.6$.

                \begin{center}
                    tscoins -p 0.6 2500 \\
                \end{center}

            \noindent would generate a fractal time series
            characterized by optimal fractional Brownian motion
            consisting of $2500$ records, and a Shannon probability,
            $P$, of $0.6$.

                \begin{center}
                    tscoins -p 0.6 -f 0.55 2500 \\
                \end{center}

            \noindent would generate a fractal time series
            characterized by non---optimal fractional Brownian motion
            consisting of $2500$ records, and a Shannon probability,
            $P$, of $0.6$, with a wagering fraction of $0.1$.

                \begin{center}
                    tscoins -p 0.6 -f 0.55 2500 | tsfraction \\
                \end{center}

            \noindent would generate the normalized increments of a
            fractal time series characterized by non---optimal
            fractional Brownian motion consisting of $2500$ records,
            and a Shannon probability, $P$, of $0.6$, with a wagering
            fraction of $0.1$.

                \begin{center}
                    tscoins -p 0.6 -f 0.55 2500 | tsfraction | tsavg -p \\
                    tscoins -p 0.6 -f 0.55 2500 | tsfraction | tsrms -p \\
                \end{center}

            \noindent would generate average and the root mean square
            of the normalized increments of a fractal time series
            characterized by non---optimal fractional Brownian motion
            consisting of $2500$ records, and a Shannon probability,
            $P$, of $0.6$, with a wagering fraction of $0.1$.

                \begin{center}
                    tsfraction my.stock | tsavg -p \\
                    tsfraction my.stock | tsrms -p \\
                \end{center}

            \noindent would measure the average and the root mean
            square of the normalized increments of the stock time
            series, my.stock.

            It would be convenient to consolidate the various programs
            into a single monolithic architecture for the analysis and
            simulation of wagering strategies of stock market time
            series.  It would, further, be convenient, from a
            comparative standpoint, to let value of the portfolio, at
            time zero, be the same as the price of a single stock at
            the beginning of the simulation, so that the portfolio
            value using the wagering strategy to invest in a single
            stock can be compared to the price of the stock, over
            time. To reiterate the previous concepts, suppose that the
            measurement yielded that the the the fraction of the
            capital to be invested, $f$, was $0.2$, (ie., a Shannon
            probability of $0.6$,) then we might invest the entire
            portfolio in the stock, and our portfolio would be modeled
            as $20\%$ of the portfolio would be wagered at any time,
            and $80\%$ would be considered as ``cash reserves,'' even
            though the $80\%$ is actually invested in the
            stock. Assume the following pseudo code:

                \vspace{0.25in}
                \begin{bf}
                    \begin{quotation}

                        \vspace{0.1in}\noindent calculate the average
                        and root mean square of the normalized
                        increments, $avg$ and $rms$, respectively

                        \vspace{0.1in}\noindent $capital = $ value of
                        stock at time $0$, (ie., the portfolio value
                        at time zero, is one share of stock)

                        \vspace{0.1in}\noindent multiplier $ =
                        \frac{avg^{2}}{rms^{3}} \cdot \frac{1}{f}$
                        (ie., the value of the multiplier, $F \cdot f$
                        in the derivations, by which the fraction of
                        the capital that is to be wagered must be
                        increased, ie., $F = m$ multiplier)

                        \vspace{0.1in}\noindent for each time
                        interval, (ie., for each increment in the time
                        series)

                        \begin{quotation}

                            \vspace{0.1in}\noindent if not the first
                            interval?, (ie., we need to calculate the
                            normalized increments, so the first
                            interval can not be used)

                            \begin{quotation}

                                \vspace{0.1in}\noindent $ capital =
                                lastcapital \cdot multiplier \cdot (1
                                + increment)$, (ie., this is the new
                                capital for today)

                                \vspace{0.1in}\noindent $lastcapital =
                                capital$, (ie., this is yesterday's
                                capital, tomorrow)

                            \end{quotation}

                        \end{quotation}

                    \end{quotation}
                \end{bf}
                \vspace{0.25in}

            \noindent where the increment is calculated by subtracting
            todays stock value from yesterday's stock value, and
            dividing by yesterday's stock value:

                \begin{equation}
                    increment = \frac{V\left(t\right) - V\left(t - 1\right)}{V\left(t - 1\right)}
                \end{equation}

            Note that:

                \begin{equation}
                    capital = lastcapital \cdot multiplier \cdot (1 + increment)
                \end{equation}

                \begin{equation}
                    capital = lastcapital \cdot multiplier \cdot \left(1 + \frac{V\left(t\right) - V\left(t - 1\right)}{V\left(t - 1\right)}\right)
                \end{equation}

                \begin{equation}
                    capital = lastcapital \cdot multiplier \cdot \left(1 + \frac{V\left(t\right)}{V\left(t - 1\right)} - 1\right)
                \end{equation}

                \begin{equation}
                    capital = lastcapital \cdot multiplier \cdot \frac{V\left(t\right)}{V\left(t - 1\right)}
                \end{equation}

            \noindent which, not surprisingly, if $multiplier = 1$,
            (ie., $P = P'$):

                \begin{equation}
                    capital = lastcapital \cdot \frac{V\left(t\right)}{V\left(t - 1\right)}
                \end{equation}

            \noindent meaning that the portfolio value would track the
            stock's value, as we would expect. Likewise, if
            $multiplier$ is greater than $1$, the portfolio value
            would linearly track the stock value, by a constant of
            proportionality, and the amount of the portfolio invested
            in the stock would be greater than the value of the
            portfolio, possibly indicating that the the remainder of
            the stock investment was purchased on margin. If the
            program is used to determine the fraction of the portfolio
            that is to be invested in a specific stock, then the
            fraction can be calculated from:

                \begin{equation}
                    f = 2P - 1
                \end{equation}

            \noindent and:

                \begin{equation}
                    fraction = multiplier \cdot f
                \end{equation}

            It would also be desirable to be able to automatically
            determine the number of stocks that should be held. The
            total capital invested in a stock is:

                \begin{equation}
                    capital \cdot multiplier
                \end{equation}

            \noindent and dividing this value by the current value of
            the stock will give the number of stocks that should be
            invested in.

            Note that the portfolio investment simulation model is
            very simple, and assumes perfect liquidity of the stock,
            (ie., as many as necessary can be bought or sold at
            exactly the day's closing price of the stock,) and that
            there are no transaction commissions.

            An example output from the {\it tsstock}\/ program appears
            in Figure~\ref{stockexample}.

            \begin{figure}[ht]
                \begin{center}
                    \begin{minipage}[t]{\textwidth}
                        \center{\fbox{\parbox{0.9\textwidth}{\input{../simulation/test/tscoin.tsstock-n.tex}}}}
                        \caption[Example output of the {\it
                            tsstock}\/ program]{Example output of the
                            {\it tsstock}\/ program, the input file
                            was produced by the {\it tscoin}\/
                            program, with a Shannon probability of
                            0.6, which is shown in
                            Figure~\ref{coinexample}.}
                        \label{stockexample}
                    \end{minipage}
                \end{center}
            \end{figure}

        \subsection{tsstocks}
            \subidx{programs}{tsstocks}
            \subidx{tsstocks}{program}
            \subidx{time series}{stock wager optimization}
            \subidx{stock wager optimization}{time series}
            \subidx{simulation}{stock wager optimization}
            \subidx{stock wager optimization}{simulation}

            Source tsstocks.c, is for simulating the optimal gains of
            multiple stock investments. The program decides which of
            all available stocks to invest in at any single time, by
            calculating the instantaneous Shannon probability of all
            stocks, and using an approximation to statistical
            estimation techniques to estimate the accuracy of the
            calculated Shannon probability.

            One of the implications of considering stock prices to
            have fractal characteristics, ie., random walk or Brownian
            motion, is that future prices can not be predicted from
            past stock price performance. The Shannon probability of a
            stock price time series is the likelihood that a stocks
            price will increase in the next time interval. It is
            typically $0.51$, on a day to day bases, (although,
            occasionally, it will be as high as $0.6$) What this
            means, for a typical stock, is that $51$\% of the time, a
            stock's price will increase, and $49$\% of the time it
            will decrease---and there is no possibility of determining
            which will occur---only the probability.

            However, another implication of considering stock prices
            to have fractal characteristics is that there are
            statistical optimizations to maximize portfolio
            performance. The Shannon probability, $P$, is related to
            the volatility of a stock's price, (measured as the root
            mean square of the normalized increments of the stock's
            price time series,) $rms$, by $rms = 2P - 1$. Also, the
            average of the normalized increments is the growth in the
            stock's price, and is equal to the square of the
            $rms$. Unfortunately, the measurements of $avg$ and $rms$
            must be made over a long period of time, to construct a
            very large data set for analytical purposes do to the
            necessary accuracy requirements. Statistical estimation
            techniques are usually employed to quantitatively
            determine the size of the data set for a given analytical
            accuracy.

            There are several techniques used to optimize stock
            portfolio performance. Since the volatility of an
            individual stock price, $rms$, is considered to have a
            Gaussian distribution, the volatilities add root mean
            square. What this means is that if the portfolio consists
            of $10$ stocks, concurrently, with each stock representing
            $10$\% of the portfolio, then the volatility of the
            portfolio will be decreased by a factor of the square root
            of $10$, (assuming all stocks are statistically
            identical.)  Further, since it is assumed that the stocks
            are statistically identical, the average growth of the
            stocks adds linearly in the portfolio, ie., it would not
            make any difference, from a portfolio growth standpoint,
            whether the portfolio consisted of $1$ stock, or $10$
            stocks.  This indicates that control of stock portfolio
            volatility can be an ``engineered solution.'' (In reality,
            of course, the stocks are not statistically identical, but
            the volatilities still add root mean square. The growth of
            the portfolio would be less, since it was not totally
            invested in the stock with the highest growth rate---this
            would be the cost of managing the volatility risk.)

            Now consider ``timing the market.'' If a stock's price has
            fractal characteristics, this is impossible, (at least
            more than $51$\% of the time, on average, for most
            stocks.) Attempting to do so, say by selling a stock for
            the speculative reason that the stocks price will decrease
            in the future, will result in selling a stock that $51$\%
            of the time would increase in value in the future, and
            $49$\% of the time would decrease in value. Of course,
            holding a stock would have the same probabilities, also.

            If a stock's price is fractal, it will, over time, exhibit
            price increases, and decreases, that have a range that is
            proportional to the square root of time, and a probable
            duration that is proportional to the reciprocal of the
            square root of time. In point of fact, measurements on
            these characteristics in stock pro forma for the past
            century offer compelling evidence that stock prices
            exhibit fractal characteristics.  These increases and
            decreases in stock price over time would lead to the
            intuitive presumption that a ``buy low and sell high''
            strategy could be implemented. Unfortunately, if stock
            prices are indeed fractal in nature, that is not the case,
            because no matter what time scale you use, the
            characteristics are invariant, (ie., on a time scale---be
            it by the tick, by the day, by the month, or by the
            year---the range and duration phenomena is still the same,
            ie., made up of ``long term'' increases and decreases,
            that have no predictive qualities, other than
            probabilistic.)

            The issue with attempting to ``time the market'' is that
            if you sell a stock to avoid an intuitively expected price
            decrease, (which will be correct, $49$\% of the time,
            typically,) then you will, also, give up the chance of the
            stock price increasing, (which will happen $51$\% of the
            time.) However, there is an alternative, and that would be
            to sell the stock, and invest in another stock, (which
            would also have a $51$\% chance of increasing in price, on
            the average---a kind of ``hedging'' strategy.)

            To implement such a strategy, one would never sell a stock
            for a stock with a smaller Shannon probability, without
            compelling reasons. In point of fact, it would probably
            be, at least heuristically, the best strategy to always be
            invested in the stocks with the most recent largest
            Shannon probability, the assumption being that during the
            periods when a stock's price is increasing, the short term
            ``instantaneous'' average Shannon probability will be
            larger than the long term average Shannon
            probability. (Not that this will always be true---only
            $51$\% of the time, for an average stock, will it succeed
            in the next time interval.) This will require specialized
            filtering, (to ``weight'' the most recent instantaneous
            Shannon probability more than the least recent,) and
            statistical estimation (to determine the accuracy of the
            measurement of the Shannon probability, upon which the
            decision will be made as to which stocks are in the
            portfolio at any instant in time.)

            This decision would be based on the normalized increments,

            \begin{equation}
                \frac{V_{t} - V_{t - 1}}{V_{t - 1}}
            \end{equation}

            of the time series, which, when averaged over a
            ``sufficiently large'' number of increments, is the mean
            of the normalized increments, $avg$. The term
            ``sufficiently large'' must be analyzed
            quantitatively. For example, the following table is the
            statistical estimate for a Shannon probability, $P$, of a
            time series, vs, the number of records required, based on
            a mean of the normalized increments $= 0.04$, as shown in
            table~\ref{TSSTOCKS1}.

            \begin{small}
                \begin{table}[ht]
                    \begin{center}
                        \caption[Shannon Probability vs. Number of Records]{Shannon Probability vs. Number of Records.}
                        \begin{tabular}{|l|l|l|l|l|} \hline
                            $P$    & $avg$    & $e$      & $c$      & $n$\\ \hline
                            $0.51$ & $0.0004$ & $0.0396$ & $0.7000$ & $27$\\
                            $0.52$ & $0.0016$ & $0.0384$ & $0.7333$ & $33$\\
                            $0.53$ & $0.0036$ & $0.0364$ & $0.7667$ & $42$\\
                            $0.54$ & $0.0064$ & $0.0336$ & $0.8000$ & $57$\\
                            $0.55$ & $0.0100$ & $0.0300$ & $0.8333$ & $84$\\
                            $0.56$ & $0.0144$ & $0.0256$ & $0.8667$ & $135$\\
                            $0.57$ & $0.0196$ & $0.0204$ & $0.9000$ & $255$\\
                            $0.58$ & $0.0256$ & $0.0144$ & $0.9333$ & $635$\\
                            $0.59$ & $0.0324$ & $0.0076$ & $0.9667$ & $3067$\\
                            $0.60$ & $0.0400$ & $0.0000$ & $1.0000$ & $\inf$\\ \hline
                        \end{tabular}
                        \label{TSSTOCKS1}
                    \end{center}
                \end{table}
            \end{small}

            where $avg$ is the average of the normalized increments,
            $e$ is the error estimate in $avg$, $c$ is the confidence
            level of the error estimate, and $n$ is the number of
            records required for that confidence level in that error
            estimate.  What this table means is that if a step
            function, from zero to $0.04$, (corresponding to a Shannon
            probability of $0.6$,) is applied to the system, then
            after $27$ records, we would be $70$\% confident that the
            error level was not greater than $0.0396$, or $avg$ was
            not lower than $0.0004$, which corresponds to an effective
            Shannon probability of $0.51$. Note that if many
            iterations of this example of $27$ records were performed,
            then $30$\% of the time, the average of the time series,
            $avg$, would be less than $0.0004$, and $70$\% greater
            than $0.0004$. This means that the the Shannon
            probability, $0.6$, would have to be reduced by a factor
            of $0.85$ to accommodate the error created by an
            insufficient data set size to get the effective Shannon
            probability of $0.51$. Since half the time the error would
            be greater than $0.0004$, and half less, the confidence
            level would be $1 - ((1 - 0.85) \cdot 2) = 0.7$, meaning
            that if we measured a Shannon probability of $0.6$ on only
            $27$ records, we would have to use an effective Shannon
            probability of $0.51$, corresponding to an $avg$ of
            $0.0004$. For $33$ records, we would use an $avg$ of
            $0.0016$, corresponding to a Shannon probability of
            $0.52$, and so on.

            The table above was made by iterating the {\it
            tsstatest}\/ program, and can be approximated by a single
            pole low pass recursive discreet time
            filter~\cite[pp. 11]{Conover}, with the pole frequency at
            $0.00045$ times the time series sampling frequency. The
            accuracy of the approximation is about $\pm 10$\% for the
            first $260$ samples, with the approximation accuracy
            prediction becoming optimistic thereafter, ie., about
            $+30$\%.

            A pole frequency of $0.033$ seems a good approximation for
            working with the root mean square of the normalized
            increments, with a reasonable approximation to about
            $5---10$ time units.

            The ``instantaneous,'' weighted, and statistically
            estimated Shannon probability, $P$, can be determined by
            dividing the filtered $rms$ by the filtered $avg$, adding
            unity, and dividing by two.

            (Note: there is some possibility of operating on the
            absolute value of the normalized increments, which is a
            close approximation to the root mean square of the
            normalized increments. Another possibility is to use
            trading volumes to calculate the instantaneous value for
            the average and root mean square of the increments as in
            the {\it tsshannonvolume}\/ program.  Also, another
            reasonable statistical estimate approximation is $P_{est}
            = 0.5 + (1 - 1 / sqrt(n)) \cdot ((2 \cdot P_{meas}) - 1)
            \cdot 0.5$, where $P_{meas}$ is the measured Shannon
            probability over $n$ many records, and $P_{est}$ is the
            Shannon probability that should be used do to the
            uncertainty created by an inadequate data set size.)

            The advantage of the discreet time recursive single pole
            filter approximation is that it requires only $3$ lines of
            code in the implementation---two for initialization, and
            one in the calculation construct.

            The single pole low pass filter is implemented from the
            following discrete time equation:

            \begin{equation}
                v_{n + 1} = I \cdot k2 + v_{n} \cdot k1
            \end{equation}

            \noindent where $I$ is the value of the current sample in
            the time series, $v$ are the value of the output time
            series, and $k1$ and $k2$ are constants determined from
            the following equations:

            \begin{equation}
                k1 = e^{-2 \cdot p \cdot \pi}
            \end{equation}

            \noindent and

            \begin{equation}
                k2 = 1 - k1
            \end{equation}

            where $p$ is a constant that determines the frequency of
            the pole---a value of unity places the pole at the sample
            frequency of the time series.

            The input file structure is a text file consisting of
            records, in temporal order, one record per time series
            sample.  Blank records are ignored, and comment records
            are signified by a `\#' character as the first non white
            space character in the record. Data records must contain
            at least one field, which is the data value of the sample,
            but may contain many fields---if the record contains many
            fields, then the first field is regarded as the sample's
            time, and the last field as the sample's value at that
            time.

        \subsection{tstrade}
            \subidx{programs}{tstrade}
            \subidx{tstrade}{program}

            Source tstrade.c is for simulating the optimal gains of
            multiple equity investments. The program decides which of
            all available equities to invest in at any single time, by
            calculating the instantaneous Shannon probability of all
            equities, and using an approximation to statistical
            estimation techniques to estimate the accuracy of the
            calculated Shannon probability.

            The input file structure is a text file consisting of
            records, in temporal order, one record per time series
            sample.  Blank records are ignored, and comment records
            are signified by a '\#' character as the first non white
            space character in the record. Each data record represents
            an equity transaction, consisting of a minium of six
            fields, separated by white space. The fields are ordered
            by time stamp, equity ticker identifier, maximum price in
            time unit, minimum price in time unit, closing price in
            time unit, and trade volume.  The existence of a record
            with more than 6 fields is used to suspend transactions on
            the equity, concluding with the record, for example:

            \begin{verbatim}

                930830      AA      38.125  37.875  37.938  333.6
                930830      AALR    3.250   2.875   3.250   7.2     Suspend
                930830      AHP     64.375  63.625  64.375  335.9

            \end{verbatim}

            Note: this program uses the following functions from
            other references:

            \begin{description}

                \item[ran1:] which returns a uniform random deviate
                between 0.0 and 1.0. See~\cite[pp. 210]{Press},
                referencing Knuth.

                \item[gasdev:] which returns a normally distributed
                deviate with zero mean and unit variance, using ran1 () as
                the source of uniform deviates. See~\cite[pp. 217]{Press}.

            \end{description}

            \subsubsection{Introduction}

                One of the prevailing concepts in financial
                quantitative analysis, (eg., ``financial
                engineering,'') is that equity prices exhibit ``random
                walk,'' (eg., Brownian motion, or fractal,)
                characteristics. The presentation by Brian
                Arthur~\cite{Arthur:CIEAFM} offers a compelling
                theoretical framework for the random walk model.
                William A. Brock and Pedro J. F. de Lima~\cite{Brock}
                among others, have published empirical evidence
                supporting Arthur's theoretical arguments.

                There is a large mathematical infrastructure available
                for applications of fractal analysis to equity
                markets. For example, the publications authored by
                Richard M. Crownover~\cite{Crownover}, Edgar
                E. Peters~\cite{Peters:CAOITCM}, and Manfred
                Schroeder~\cite{Schroeder} offer formal methodologies,
                while the books by John
                L. Casti~\cite{Casti:SFC},~\cite{Casti:C} offer a less
                formal approach for the popular press.

                There are interesting implications that can be
                exploited if equity prices exhibit fractal
                characteristics:

                \begin{enumerate}

                    \item \label{I1} It would be expected that equity
                    portfolio volatility would be equal to the root
                    mean square of the individual equity volatilities
                    in the portfolio.

                    \item \label{I2} It would be expected that equity
                    portfolio growth would be equal to the linear
                    addition of the growths of the individual equities
                    in the portfolio.

                    \item \label{I3} It would be expected that an
                    equity's price would fluctuate, over time, and the
                    range, of these fluctuations (ie., the maximum
                    price minus the minimum price,) would increase
                    with the square root of time.

                    \item \label{I4} It would be expected that the
                    number of equity price fluctuations in a time
                    interval, (ie., the number of times an equity's
                    price reaches a local maximum, then reverse
                    direction and decreases to a local minimum,) would
                    increase with the square root of time.

                    \item \label{I5} It would be expected that the
                    time between fluctuations in an equity's price,
                    (ie., the interval between an equity's price
                    reaching a local maximum and then a local
                    minimum,) would decrease with the reciprocal of
                    the square root of time.

                    \item \label{I6} It would be expected that an
                    equity's price, over time, would be mean
                    reverting, (ie., if an equity's price is below its
                    average, there would be a propensity for the
                    equity's price to increase, and vice versa.)

                \end{enumerate}

                Note that~\ref{I1} and~\ref{I2} above can be exploited
                to formulate an optimal hedging
                strategy;~\ref{I3},~\ref{I4}, and~\ref{I5} would tend
                to imply that ``market timing'' is not attainable;
                and~\ref{I6} can be exploited to formulate an optimal
                buy-sell strategy.

            \subsubsection{Derivation}

                As a tutorial, the derivation will start with a simple
                compound interest equation. This equation will be
                extended to a first order random walk model of equity
                prices. Finally, optimizations will derived based on
                the random walk model that are useful in optimizing
                equity portfolio performance.

                If we consider capital, $V$, invested in a savings
                account, and calculate the growth of the capital over
                time:

                \begin{equation}
                    V_{t} = V_{t - 1} \left(1 + a_{t}\right)
                    \label{nsummaryappb1}
                \end{equation}

                \noindent where $a_{t}$ is the interest rate at time
                $t$, (usually a constant\footnote{For example, if $a =
                0.06$, or $6$\%, then at the end of the first time
                interval the capital would have increased to $1.06$
                times its initial value. At the end of the second time
                interval it would be $\left(1.06\right)^{2}$, and so
                on. What Equation~\ref{nsummaryappb1} states is that
                the way to get the value, $V$ in the next time
                interval is to multiply the current value by
                $1.06$. Equation~\ref{nsummaryappb1} is nothing more
                than a ``prescription,'' or a process to make an
                exponential, or ``compound interest'' mechanism. In
                general, exponentials can always be constructed by
                multiplying the current value of the exponential by a
                constant, to get the next value, which in turn, would
                be multiplied by the same constant to get the next
                value, and so on. Equation~\ref{nsummaryappb1} is
                nothing more than a construction of $V\left(t\right) =
                e^{kt}$ where $k = \ln\left(1 + a\right)$. The
                advantage of representing exponentials by the
                ``prescription'' defined in
                Equation~\ref{nsummaryappb1} is analytical
                expediency. For example, if you have data that is an
                exponential, the parameters, or constants, in
                Equation~\ref{nsummaryappb1} can be determined by
                simply reversing the ``prescription,'' ie.,
                subtracting the previous value, (at time $t - 1$,)
                from the current value, and dividing by the previous
                value would give the exponentiating constant, $\left(1
                + a_{t}\right)$. This process of reversing the
                ``prescription'' is termed calculating the
                ``normalized increments.'' (Increments are simply the
                difference between two values in the exponential, and
                normalized increments are this difference divided by
                the value of the exponential.)  Naturally, since one
                usually has many data points over a time interval, the
                values can be averaged for better precision---there is
                a large mathematical infrastructure dedicated to
                precision enhancement, for example, least squares
                approximation to the normalized increments, and
                statistical estimation.}.) In equities, $a_{t}$ is not
                constant, and varies---perhaps being negative at
                certain times, (meaning that the value of the equity
                decreased.)  This fluctuation in an equity's value can
                be represented by modifying $a_{t}$ in
                Equation~\ref{nsummaryappb1}:

                \begin{equation}
                    a_{t} = f_{t} F_{t}
                    \label{nsummaryappb2}
                \end{equation}

                \noindent where the product $f_{t} \cdot F_{t}$ is the
                fluctuation in the equity's value at time $t$.

                An equity's value, over time, is similar to a simple
                tossed coin game~\cite[pp. 128]{Schroeder}, where
                $f_{t}$ is the fraction of a gambler's capital wagered
                on a toss of the coin, at time $t$, and $F_{t}$ is a
                random variable\footnote{``Random variable'' means
                that the process, $F_{t}$, is random in nature, ie.,
                there is no possibility of determining what the next
                value will be. However, $F_{t}$ can be analyzed using
                statistical
                methods~\cite[pp. 163]{Feder},~\cite[pp. 128]{Schroeder}. For
                example, $F_{t}$ typically has a Gaussian distribution
                for equity values~\cite[pp. 249]{Crownover}, in which
                case the it is termed a ``fractional Brownian
                motion,'' or simply a ``fractal'' process. In the case
                of a single tossed coin, it is termed ``fixed
                increment fractal,'' ``Brownian,'' or ``random walk''
                process. In any case, determination of the statistical
                characteristics of $F_{t}$ are the essence of
                analysis. Fortunately, there is a large mathematical
                infrastructure dedicated to the subject. For example,
                $F_{t}$ could be verified as having a Gaussian
                distribution using Chi---Square
                techniques. Frequently, it is convenient, from an
                analytical standpoint, to ``model'' $F_{t}$ using a
                mathematically simpler
                process~\cite[pp. 128]{Schroeder}. For example,
                multiple iterations of tossing a coin can be used to
                approximate a Gaussian distribution, since the
                distribution of many tosses of a coin is
                binomial---which if the number of tosses is sufficient
                will represent a Gaussian distribution to within any
                required
                precision~\cite[pp. 144]{Schroeder},~\cite[pp. 154]{Feder}.},
                signifying whether the game was a win, or a loss, ie.,
                whether the gambler's capital increased or decreased,
                and by how much. The amount the gambler's capital
                increased or decreased is $f_{t} \cdot F_{t}$.

                In general, $F_{t}$ is a function of a random
                variable, with an average, over time, of $avg_{f}$,
                and a root mean square value, $rms_{f}$, of
                unity. Note that for simple, time invariant, compound
                interest, $F_{t}$ has an average and root mean square,
                both being unity, and $f_{t}$ is simply the interest
                rate, which is assumed to be constant. For a simple,
                single coin game, $F_{t}$ is a fixed increment, (ie.,
                either $+1$ or $-1$,) random generator. From an
                analytical perspective, it would be advantageous to
                measure the the statistical characteristics of the
                generator. Substituting Equation~\ref{nsummaryappb2}
                into
                Equation~\ref{nsummaryappb1}\footnote{Equation~\ref{nsummaryappb3}
                is interesting in many other respects. For example,
                adding a single term, $m \cdot V_{t - 1}$, to the
                equation results in $V_{t} = V_{t - 1} \left(1 + f_{t}
                F_{t} + m \cdot V_{t - 1}\right)$ which is the
                ``logistic,'' or `S' curve equation, (formally termed
                the ``discreet time quadratic equation,'') and has
                been used successfully in many unrelated fields such
                as manufacturing operations, market and economic
                forecasting, and analyzing disease
                epidemics~\cite[pp. 131]{Modis}. There is continuing
                research into the application of an additional
                ``non-linear'' term in Equation~\ref{nsummaryappb3} to
                model equity value non-linearities. Although there
                have been modest successes, to date, the successes
                have not proved to be exploitable in a systematic
                fashion~\cite[pp. 133]{Peters:CAOITCM}. The reason for
                the interest is that the logistic equation can exhibit
                a wide variety of behaviors, among them, ``chaotic.''
                Interestingly, chaotic behavior is mechanistic, but
                not ``long term'' predictable into the future. A good
                example of such a system is the weather. It is an
                important concept that compound interest, the logistic
                function, and fractals are all closely related.}:

                \begin{equation}
                    V_{t} = V_{t - 1} \left(1 + f_{t} F_{t}\right)
                    \label{nsummaryappb3}
                \end{equation}

                \noindent and subtracting $V_{t - 1}$ from both sides:

                \begin{equation}
                    V_{t} - V_{t - 1} = V_{t - 1} \left(1 + f_{t} F_{t}\right) - V_{t - 1}
                    \label{nsummaryappb4}
                \end{equation}

                \noindent and dividing both sides by $V_{t - 1}$:

                \begin{equation}
                    \frac{V_{t} - V_{t - 1}}{V_{t - 1}} = \frac{V_{t - 1} \left(1 + f_{t} F_{t}\right) - V_{t - 1}}{V_{t - 1}}
                    \label{nsummaryappb5}
                \end{equation}

                \noindent and combining:

                \begin{equation}
                    \frac{V_{t} - V_{t - 1}}{V_{t - 1}} = \left(1 + f_{t} F_{t}\right) - 1 =  f_{t} F_{t}
                    \label{nsummaryappb6}
                \end{equation}

                We now have a ``prescription,'' or process, for
                calculating the characteristics of the random process
                that determines an equity's price, over time.  That
                process is, for each unit of time, subtract the value
                of the of the equity at the previous time from the
                value of the equity at the current time, and divide
                this by the value of the equity at the previous
                time. The root mean square\footnote{In this section,
                ``root mean square'' is used to mean the variance of
                the normalized increments. In Brownian motion
                fractals, this is computed by $\sigma_total^{2} =
                \sigma_1^{2} + \sigma_2^{2} + \cdots$ However, in many
                fractals, the variances are not calculated by adding
                the squares, (ie., a power of $2$,) of the
                values---the power may be ``fractional,'' ie., $3 / 2$
                instead of $2$, for
                example~\cite[pp. 130]{Schroeder},~\cite[pp. 178]{Feder}. However,
                as a first order approximation, the variances of the
                normalized increments of equity values can
                successfully be added root mean
                square~\cite[kpp. 250]{Crownover}. The so called
                ``Hurst'' coefficient, which can be measured,
                determines the process to be used. The Hurst
                coefficient is range of the equity values over a time
                interval, divided by the standard deviation of the
                values over the interval, and its determination is
                commonly called ``$R / S$'' analysis. As pointed out
                in~\cite[pp. 157]{Schroeder} the errors committed in
                such simplified assumptions can be
                significant---however, for analysis of equities,
                squaring the variances seems to be a reasonably
                accurate simplification.}  of these values are the
                root mean square of the random process. The average of
                these values are the average of the random process,
                $avg_{f}$. The root mean square of these values can be
                calculated by any convenient means, and will be
                represented by $rms$. The average of these values can
                be found by any convenient means, and will be
                represented by $avg$\footnote{For example, many
                calculators have averaging and root mean square
                functionality, as do many spreadsheet
                programs---additionally, there are computer source
                codes available for both. See the programs {\it
                tsrms}\/ and {\it tsavg}\/. The method used is not
                consequential.}. Therefore, if $f_{t} = f$, and
                assuming that it does not vary over time:

                \begin{equation}
                    rms = f
                    \label{nsummaryappb7}
                \end{equation}

                \noindent which, if there are sufficiently many
                samples, is a metric of the equity value's
                ``volatility,'' and:

                \begin{equation}
                    avg = f \cdot F_{t}
                    \label{nsummaryappb9}
                \end{equation}

                \noindent and if there are sufficiently many samples,
                the average of $F_{t}$ is simply $avg_{f}$, or:

                \begin{equation}
                    avg = f \cdot avg_{f}
                    \label{nsummaryappb10}
                \end{equation}

                \noindent which is a metric on the equity value's rate
                of ``growth.''  Note that this is the ``effective''
                compound interest rate from
                Equation~\ref{nsummaryappb1}.
                Equations~\ref{nsummaryappb7} and~\ref{nsummaryappb10}
                are important equations, since they can be used in
                portfolio management. For example,
                Equation~\ref{nsummaryappb7} states that the
                volatility of the capital invested in many equities,
                simultaneously, is calculated as the root mean square
                of the individual volatility of the
                equities. Equation~\ref{nsummaryappb10} states that
                the growths in the same equity values add together
                linearly\footnote{There are significant implications
                do to the fact that equity volatilities are calculated
                root mean square. For example, if capital is invested
                in $N$ many equities, concurrently, then the
                volatility of the capital will be $\frac{1}{\sqrt{N}}
                \cdot rms$ of an individual equity's volatility,
                $rms$, provided all the equites have similar
                statistical characteristics. But the growth in the
                capital will be unaffected, ie., it would be
                statistically similar to investing all the capital in
                only one equity. What this means is that capital, or
                portfolio, volatility can be minimized without
                effecting portfolio growth---ie., volatility risk can
                addressed.  There are further applications.  For
                example, Equation~\ref{nsummaryappb6} could be
                modified by dividing both the normalized increments,
                and the square of the normalized increments by the
                daily trading volume. The quotient of the normalized
                increments divided by the trading volume is the
                instantaneous growth, $avg_{f}$, of the equity, on a
                per-share basis. Likewise, the square root of the
                square of the normalized increments divided by the
                daily trading volume is the instantaneous root mean
                square, $rms_{f}$, of the equity on a per-share basis,
                ie., its instantaneous volatility of the equity. (Note
                that these instantaneous values are the statistical
                characteristics of the equity on a per-share bases,
                similar to a coin toss, and not on time.)
                Additionally, it can be shown that the range---the
                maximum minus the minimum---of an equity's value over
                a time interval will increase with the square root of
                of the size of the interval of
                time~\cite[pp. 178]{Feder}. Also, it can be shown that
                the number of expected equity value ``high and low''
                transitions scales with the square root of time,
                meaning that the probability of an equity value ``high
                or low'' exceeding a given time interval is
                proportional to the square root of the time
                interval~\cite[pp. 153]{Schroeder}.}.  Dividing
                Equation~\ref{nsummaryappb10} by
                Equation~\ref{nsummaryappb7} results in the two $f$'s
                canceling, or:

                \begin{equation}
                    \frac{avg}{rms} = avg_{f}
                    \label{nsummaryappb11}
                \end{equation}

                There may be analytical advantages to ``model'' $F_t$
                as a simple tossed coin game, (either played with a
                single coin, or multiple coins, ie., many coins played
                at one time, or a single coin played many
                times\footnote{Here the ``model'' is to consider two
                black boxes, one with a equity ``ticker'' in it, and
                the other with a casino game of a tossed coin in
                it. One could then either invest in the equity, or,
                alternatively, invest in the tossed coin game by
                buying many casino chips, which constitutes the
                starting capital for the tossed coin game. Later,
                either the equity is sold, or the chips ``cashed in.''
                If the statistics of the equity value over time is
                similar to the statistics of the coin game's capital,
                over time, then there is no way to determine which box
                has the equity, or the tossed coin game. The advantage
                of this model is that gambling games, such as the
                tossed coin, have a large analytical infrastructure,
                which, if the two black boxes are statistically the
                same, can be used in the analysis of equities. The
                concept is that if the value of the equity, over time,
                is statistically similar to the coin game's capital,
                over time, then the analysis of the coin game can be
                used on equity values. Note that in the case of the
                equity, the terms in $f_{t} \cdot F_{t}$ can not be
                separated. In this case, $f = rms$ is the fraction of
                the equity's value, at any time, that is ``at risk,''
                of being lost, ie., this is the portion of a equity's
                value that is to be ``risk managed.''  This is usually
                addressed through probabilistic methods, as outlined
                below in the discussion of Shannon probabilities,
                where an optimal wagering strategy is determined. In
                the case of the tossed coin game, the optimal wagering
                strategy is to bet a fraction of the capital that is
                equal to $f = rms = 2P - 1$~\cite[pp. 128,
                151]{Schroeder}, where $P$ is the Shannon
                probability. In the case of the equity, since $f =
                rms$ is not subject to manipulation, the strategy is
                to select equities that closely approximate this
                optimization, and the equity's value, over time, on
                the average, would increase in a similar fashion to
                the coin game. The growth of either investment would
                be equal to $avg = rms^{2}$, on average, for each
                iteration of the coin game, or time unit of equity
                investment. This is an interesting concept from risk
                management since it maximizes the gain in the capital,
                while, simultaneously, minimizing risk exposure to the
                capital.}.)  The number of wins minus the number of
                losses, in many iterations of a single coin tossing
                game would be:

                \begin{equation}
                    P - \left(1 - P\right) = 2P - 1
                \end{equation}

                \noindent where P is the probability of a win for the
                tossed coin. (This probability is traditionally
                termed, the ``Shannon probability'' of a win.)  Note
                that from the definition of $F_{t}$ above, that $P =
                avg_{f}$. For a fair coin, (ie., one that comes up
                with a win 50\% of the time,) $P = 0.5$, and there is
                no advantage, in the long run, to playing the
                game. However, if $P > 0.5$, then the optimal fraction
                of capital wagered on each iteration of the single
                coin tossing game, $f$, would be $2P - 1$. Note that
                if multiple coins were used for each iteration of the
                game, we would expect that the volatility of the
                gambler's capital to increase as the square root of
                the number of coins used, and the growth to increase
                linearly with the number of coins used, irregardless
                of whether many coins were tossed at once, or one coin
                was tossed many times, (ie., our random generator,
                $F_{t}$ would assume a binomial distribution---and if
                the number of coins was very large, then $F_{t}$ would
                assume, essentially, a Gaussian distribution.) Many
                equities have a Gaussian distribution for the random
                process, $F_{t}$. It may be advantageous to determine
                the Shannon probability to analyze equity investment
                strategies. From Equation~\ref{nsummaryappb11}:

                \begin{equation}
                    \frac{avg}{rms} = avg_{f} = 2P - 1
                    \label{nsummaryappb12}
                \end{equation}

                \noindent or:

                \begin{equation}
                    \frac{avg}{rms} + 1 = 2P
                    \label{nsummaryappb13}
                \end{equation}

                \noindent and:

                \begin{equation}
                    P = \frac{\frac{avg}{rms} + 1}{2}
                    \label{nsummaryappb14}
                \end{equation}

                \noindent where only the average and root mean square
                of the normalized increments need to be measured,
                using the ``prescription'' or process outlined above.

                Interestingly, what Equation~\ref{nsummaryappb12}
                states is that the ``best'' equity investment is not,
                necessarily, the equity that has the largest average
                growth, $avg_{f}$. The best equity investment is the
                equity that has the largest growth, while
                simultaneously having the smallest volatility. In
                point of fact, the optimal decision criteria is to
                choose the equity that has the largest {\it ratio}\/
                of growth to volatility, where the volatility is
                measured by computing the root mean square of the
                normalized increments, and the growth is computed by
                averaging the normalized increments.

            \subsubsection{Market}

                We now have a ``first order prescription'' that
                enables us to analyze fluctuations in equity values,
                although we have not explained why equity values
                fluctuate. For a formal presentation on the subject,
                see the bibliography in~\cite{Arthur:CIEAFM} which,
                also, offers non-mathematical insight into the
                subject.

                Consider a very simple equity market, with only two
                people holding equities.  Equity value ``arbitration''
                (ie., how equity values are determined,) is handled by
                one person posting (to a bulletin board,) a
                willingness to sell a given number of equities at a
                given price, to the other person. There is no other
                communication between the two people. If the other
                person buys the equity, then that is the value of the
                equity at that time. Obviously, the other person will
                not buy the equity if the price posted is too
                high---even if ownership of the equity is desired. For
                example, the other person could simply decide to wait
                in hopes that a favorable price will be offered in the
                future.  What this means is that the seller must
                consider not only the behavior of the other person,
                but what the other person thinks the seller's behavior
                will be, ie., the seller must base the pricing
                strategy on the seller's pricing strategy.  Such
                convoluted logical processes are termed ``self
                referential,'' and the implication is that the market
                can never operate in a consistent fashion that can be
                the subject of deductive
                analysis~\cite[pp. 101]{Penrose}\footnote{Penrose,
                referencing Russell's paradox, presents a very good
                example of logical contradiction in a self-referential
                system. Consider a library of books. The librarian
                notes that some books in the library contain their
                titles, and some do not, and wants to add two index
                books to the library, labeled ``A'' and ``B,''
                respectively; the ``A'' book will contain the list of
                all of the titles of books in the library that contain
                their titles; and the ``B'' book will contain the list
                of all of the titles of the books in the library that
                do not contain their titles. Now, clearly, all book
                titles will go into either the ``A'' book, or the
                ``B'' book, respectively, depending on whether it
                contains its title, or not.  Now, consider in which
                book, the ``A'' book or the ``B'' book, the title of
                the ``B'' book is going to be placed---no matter which
                book the title is placed, it will be contradictory
                with the rules. And, if you leave it out, the two
                books will be incomplete.)}. As pointed out
                by~\cite[Abstract]{Arthur:CIEAFM}, these types of
                indeterminacies pervade
                economics\footnote{\cite{Arthur:CIEAFM} cites the ``El
                Farol Bar'' problem as an example. Assume one hundred
                people must decide independently each week whether go
                to the bar. The rule is that if a person predicts that
                more than, say, 60 will attend, it will be too
                crowded, and the person will stay home; if less than
                60 is predicted, the person will go to the bar. As
                trivial as this seems, it destroys the possibility of
                long-run shared, rational expectations.  If all
                believe {\it few}\/ will go, then {\it all}\/ will go,
                thus invalidating the expectations. And, if all
                believe {\it many}\/ will go, then {\it none}\/ will
                go, likewise invalidating those expectations.
                Predictions of how many will attend depend on others'
                predictions, and others' predictions of others'
                predictions. Once again, there is no rational means to
                arrive at deduced {\it a-priori}\/ predictions. The
                important concept is that expectation formation is a
                self-referential process in systems involving many
                agents with incomplete information about the future
                behavior of the other agents. The problem of logically
                forming expectations then becomes ill-defined, and
                rational deduction, can not be consistent or
                complete. This indeterminacy of expectation-formation
                is by no means an anomaly within the real economy. On
                the contrary, it pervades all of economics and game
                theory~\cite{Arthur:CIEAFM}.}.  What the two players
                do, in absence of a deductively consistent and
                complete theory of the market, is to rely on inductive
                reasoning. They form subjective expectations or
                hypotheses about how the market operates. These
                expectations and hypothesis are constantly formulated
                and changed, in a world that forms from others'
                subjective expectations. What this means is that
                equity values will fluctuate as the expectations and
                hypothesis concerning the future of equity values
                change\footnote{Interestingly, the system described is
                a stable system, ie., if the players have a hypothesis
                that changing equity positions may be of benefit, then
                the equity values will fluctuate---a self fulfilling
                prophecy. Not all such systems are stable,
                however. Suppose that one or both players suddenly
                discover that equity values can be ``timed,'' ie.,
                there are certain times when equities can be
                purchased, and chances are that the equity values will
                increase in the very near future. This means that at
                certain times, the equites would have more value,
                which would soon be arbitrated away. Such a scenario
                would not be stable.}. The fluctuations created by
                these indeterminacies in the equity market are
                represented by the term $f_{t} F_{t}$ in
                Equation~\ref{nsummaryappb3}, and since there are many
                such indeterminacies, we would anticipate $F_t$ to
                have a Gaussian distribution.  This is a rather
                interesting conclusion, since analyzing the actions of
                aggregately many ``agents,'' each operating on
                subjective hypothesis in a market that is deductively
                indeterminate, can result in a system that can not
                only be analyzed, but optimized.

            \subsubsection{Optimization}

                The only remaining derivation is to show that the optimal
                wagering strategy is, as cited above:

                \begin{equation}
                    f = rms = 2P - 1
                \end{equation}

                \noindent where $f$ is the fraction of a gambler's capital
                wagered on each toss of a coin that has a Shannon
                probability, $P$, of winning.

                Following~\cite[pp. 450]{Reza}, consider that the gambler
                has a private wire into the future who places wagers on
                the outcomes of a game of chance. We assume that the side
                information which he receives has a probability, $P$, of
                being true, and of $1 - P$, of being false. Let the
                original capital of gambler be $V(0)$, and $V(n)$ his
                capital after the $n$'th wager. Since the gambler is not
                certain that the side information is entirely reliable, he
                places only a fraction, $f$, of his capital on each
                wager. Thus, subsequent to $n$ many wagers, assuming the
                independence of successive tips from the future, his
                capital is:

                \begin{equation}
                    V\left(n\right) = \left(1 + f\right)^{w} \left(1 - f\right)^{l} V\left(0\right)
                \end{equation}

                \noindent where $w$ is the number of times he won, and $l
                = n - w$, the number of times he lost. These numbers are,
                in general, values taken by two random variables, denoted
                by $W$ and $L$. According to the law of large numbers:

                \begin{equation}
                    \lim_{n\to\infty} \frac{1}{n} W = P
                \end{equation}

                \noindent and:

                \begin{equation}
                    \lim_{n\to\infty} \frac{1}{n} L = q = 1 - P
                \end{equation}

                The problem with which the gambler is faced is the
                determination of $f$ leading to the maximum of the average
                exponential rate of growth of his capital. That is, he
                wishes to maximize the value of:

                \begin{equation}
                    G = \lim_{n\to\infty} \frac{1}{n} \ln \frac{V\left(n\right)}{V\left(0\right)}
                \end{equation}

                \noindent with respect to $f$, assuming a fixed
                original capital and specified $P$:

                \begin{equation}
                    G = \lim_{n\to\infty} \frac{W}{n} \ln \left(1 + f\right) + \frac{L}{n} \ln \left(1 - f\right)
                \end{equation}

                \noindent or:

                \begin{equation}
                    G = P \ln \left(1 + f\right) + q \ln \left(1 - f\right)
                \end{equation}

                \noindent which, by taking the derivative with respect
                to $f$, and equating to zero, can be shown to have a
                maxima when:

                \begin{equation}
                    \frac{dG}{df} = P\left(1 + f\right)^{P - 1} \left(1 - f\right)^{1 - P} - \left(1 - P\right)\left(1 - f\right)^{1 - P - 1} \left(1 + f\right)^{P} = 0
                \end{equation}

                \noindent combining terms:

                \begin{equation}
                    P\left(1 + f\right)^{P - 1} \left(1 - f\right)^{1 - P} - \left(1 - P\right) \left(1 - f\right)^{P} \left(1 + f\right)^{P} = 0
                \end{equation}

                \noindent and splitting:

                \begin{equation}
                    P\left(1 + f\right)^{P - 1} \left(1 - f\right)^{1 - P} = \left(1 - P\right) \left(1 - f\right)^{P} \left(1 + f\right)^{P}
                \end{equation}

                \noindent then taking the logarithm of both sides:

                \begin{equation}
                    \ln \left(P\right) + \left(P - 1\right) \ln \left(1 + f\right) + \left(1 - P\right) \ln \left(1 - f\right) = \ln \left(1 - P\right) - P \ln \left(1 - f\right) + P \ln \left(1 + f\right)
                \end{equation}

                \noindent and combining terms:

                \begin{equation}
                    \left(P - 1\right) \ln \left(1 + f\right) - P \ln \left(1 + f\right) + \left(1 - P\right) \ln \left(1 - f\right) + P \ln \left(1 - f\right) = \ln \left(1 - P\right) - \ln \left(P\right)
                \end{equation}

                \noindent or:

                \begin{equation}
                    \ln \left(1 - f\right) - \ln \left(1 + f\right) = \ln \left(1 - P\right) - \ln \left(P\right)
                \end{equation}

                \noindent and performing the logarithmic operations:

                \begin{equation}
                    \ln \left(\frac{1 - f}{1 + f}\right) = \ln \left(\frac{1 - P}{P}\right)
                \end{equation}

                \noindent and exponentiating:

                \begin{equation}
                    \frac{1 - f}{1 + f} = \frac{1 - P}{P}
                \end{equation}

                \noindent which reduces to:

                \begin{equation}
                    P\left(1 - f\right) = \left(1 - P\right) \left(1 + f\right)
                \end{equation}

                \noindent and expanding:

                \begin{equation}
                    P - Pf = 1 - Pf - P + f
                \end{equation}

                \noindent or:

                \begin{equation}
                    P = 1 - P + f
                \end{equation}

                \noindent and, finally:

                \begin{equation}
                    f = 2P - 1
                \end{equation}

            \subsubsection{Fixed Increment Fractal}

                It was mentioned that it would be useful to model
                equity prices as a fixed increment fractal, ie., an
                unfair tossed coin game.


                As above, consider a gambler, wagering on the iterated
                outcomes of an unfair tossed coin game. A fraction,
                $f$, of the gambler's capital will be wagered on the
                outcome of each iteration of the unfair tossed coin,
                and if the coin comes up heads, with a probability,
                $P$, then the gambler wins the iteration, (and an
                amount equal to the wager is added to the gambler's
                capital,) and if the coin comes up tails, with a
                probability of $1 - P$, then the gambler looses the
                iteration, (and an amount of the wager is subtracted
                from the gambler's capital.)

                If we let the outcome of the first coin toss, (ie.,
                whether it came up as a win or a loss,) be $c(1)$ and
                the outcome of the second toss be $c(2)$, and so on,
                then the outcome of the $n$'th toss, $c(n)$, would be:

                \begin{equation}
                    C\left(n\right) = \left\{ \begin{array}{l l}
                                      win, & \mbox{with a probability of P}\\
                                      loose, & \mbox{with a probability of 1 - P}
                                      \end{array}
                    \right.
                \end{equation}

                \noindent for convenience, let a win to be represented
                by $+1$, and a loss by $-1$:

                \begin{equation}
                    C\left(n\right) = \left\{ \begin{array}{l l}
                                      +1, & \mbox{with a probability of P}\\
                                      -1, & \mbox{with a probability of 1 - P}
                                      \end{array}
                    \right.
                \end{equation}

                \noindent for the reason that when we multiply the
                wager, $f$, by $c(n)$, it will be a positive number,
                (ie., the wager will be added to the capital,) and for
                a loss, it will be a negative number, (ie., the wager
                will be subtracted from the capital.)  This is
                convenient, since the increment, by with the gambler's
                capital increased or decreased in the $n$'th iteration
                of the game is $f \cdot c(n)$.

                If we let $C(0)$ be the initial value of the gambler's
                capital, $C(1)$ be the value of the gambler's capital
                after the first iteration of the game, then:

                \begin{equation}
                    C\left(1\right) = C\left(0\right) \cdot \left(1 + c\left(1\right) \cdot f\left(1\right)\right)
                \end{equation}

                \noindent after the first iteration of the game, and:

                \begin{equation}
                    C\left(2\right) = C\left(0\right) \cdot \left(\left(1 + c\left(1\right) \cdot f\left(1\right)\right) \cdot \left(1 + c\left(2\right) \cdot f\left(2\right)\right)\right)
                \end{equation}

                \noindent after the second iteration of the game, and,
                in general, after the $n$'th iteration of the game:

                \begin{equation}
                    C\left(n\right) = C\left(0\right) \cdot \left(\left(1 + c\left(1\right) \cdot f\left(1\right)\right) \cdot \left(1 + c\left(2\right) \cdot f\left(2\right)\right) \cdot\hspace{0.1in} \cdots \hspace{0.1in} \cdot \left(1 + c\left(n\right) \cdot f\left(n\right)\right) \cdot \left(1 + c\left(n + 1\right) \cdot f\left(n + 1\right)\right)\right)
                \end{equation}

                For the normalized increments of the time series of
                the gambler's capital, it would be convenient to
                rearrange these formulas. For the first iteration of
                the game:

                \begin{equation}
                    C\left(1\right) - C\left(0\right) = C\left(0\right) \cdot \left(1 + c\left(1\right) \cdot f\left(1\right)\right) - C\left(0\right)
                \end{equation}

                \noindent or:

                \begin{equation}
                    \frac{C\left(1\right) - C\left(0\right)}{C\left(0\right)} = \frac{C\left(0\right) \cdot \left(1 + c\left(1\right) \cdot f\left(1\right)\right) - C\left(0\right)}{C\left(0\right)}
                \end{equation}

                \noindent and after reducing, the first normalized
                increment of the gambler's capital time series is:

                \begin{equation}
                    \frac{C\left(1\right) - C\left(0\right)}{C\left(0\right)} = \left(1 + c\left(1\right) \cdot f\left(1\right)\right) - 1 = c\left(1\right) \cdot f\left(1\right)
                \end{equation}

                \noindent and for the second iteration of the game:

                \begin{equation}
                    C\left(2\right) = C\left(0\right) \cdot \left(\left(1 + c\left(1\right) \cdot f\left(1\right)\right) \cdot \left(1 + c\left(2\right) \cdot f\left(2\right)\right)\right)
                \end{equation}

                \noindent but $C(0) \cdot ((1 + c(1) \cdot f(1))$ is
                simply $C(1)$:

                \begin{equation}
                    C\left(2\right) = C\left(1\right) \cdot \left(1 + c\left(2\right) \cdot f\left(2\right)\right)
                \end{equation}

                \noindent or:

                \begin{equation}
                    C\left(2\right) - C\left(1\right) = C\left(1\right) \cdot \left(1 + c\left(2\right) \cdot f\left(2\right)\right) - C\left(1\right)
                \end{equation}

                \noindent which is:

                \begin{equation}
                    \frac{C\left(2\right) - C\left(1\right)}{C\left(1\right)} = \frac{C\left(1\right) \cdot \left(1 + c\left(2\right) \cdot f\left(2\right)\right) - C\left(1\right)}{C\left(1\right)}
                \end{equation}

                \noindent and after reducing, the second normalized
                increment of the gambler's capital time series is:

                \begin{equation}
                    \frac{C\left(2\right) - C\left(1\right)}{C\left(1\right)} = 1 + c\left(2\right) \cdot f\left(2\right) - 1 = c\left(2\right) \cdot f\left(2\right)
                \end{equation}

                \noindent and it should be obvious that the process
                can be repeated indefinitely, so, the $n$'th
                normalized increment of the gambler's capital time
                series is:

                \begin{equation}
                    \frac{C\left(n\right) - C\left(n - 1\right)}{C\left(n\right)} = c\left(n\right) \cdot f\left(n\right)
                \end{equation}

                \noindent which is Equation~\ref{nsummaryappb6}.

            \subsubsection{Data Set Requirements}

                One of the implications of considering equity prices
                to have fractal characteristics, ie., random walk or
                Brownian motion, is that future prices can not be
                predicted from past equity price performance. The
                Shannon probability of a equity price time series is
                the likelihood that a equities price will increase in
                the next time interval. It is typically $0.51$, on a
                day to day bases, (although, occasionally, it will be
                as high as $0.6$) What this means, for a typical
                equity, is that $51$\% of the time, a equity's price
                will increase, and $49$\% of the time it will
                decrease---and there is no possibility of determining
                which will occur---only the probability.

                However, another implication of considering equity
                prices to have fractal characteristics is that there
                are statistical optimizations to maximize portfolio
                performance. The Shannon probability, $P$, is related
                to the volatility of a equity's price, (measured as
                the root mean square of the normalized increments of
                the equity's price time series,) $rms$, by $rms = 2P -
                1$. Also, the average of the normalized increments is
                the growth in the equity's price, and is equal to the
                square of the $rms$. Unfortunately, the measurements
                of $avg$ and $rms$ must be made over a long period of
                time, to construct a very large data set for
                analytical purposes do to the necessary accuracy
                requirements. Statistical estimation techniques are
                usually employed to quantitatively determine the size
                of the data set for a given analytical accuracy.

                The calculation of the Shannon probability, $P$, from
                the average and root mean square of the normalized
                increments, $avg$ and $rms$, respectively, will
                require require specialized filtering, (to "weight"
                the most recent instantaneous Shannon probability more
                than the least recent,) and statistical estimation (to
                determine the accuracy of the measurement of the
                Shannon probability.)

                This measurement would be based on the normalized
                increments, as derived in
                Equation~\ref{nsummaryappb6}:

                \begin{equation}
                    \frac{V_{t} - V_{t - 1}}{V_{t - 1}}
                \end{equation}

                which, when averaged over a ``sufficiently large''
                number of increments, is the mean of the normalized
                increments, $avg$. The term ``sufficiently large''
                must be analyzed quantitatively. For example, the
                following table is the statistical estimate for a
                Shannon probability, $P$, of a time series, vs, the
                number of records required, based on a mean of the
                normalized increments $= 0.04$, as shown in
                table~\ref{TSEQUITIES1}.

                \begin{small}
                    \begin{table}[ht]
                        \begin{center}
                            \caption[Shannon Probability vs. Number of Records]{Shannon Probability vs. Number of Records.}
                            \begin{tabular}{|l|l|l|l|l|} \hline
                                $P$    & $avg$    & $e$      & $c$      & $n$\\ \hline
                                $0.51$ & $0.0004$ & $0.0396$ & $0.7000$ & $27$\\
                                $0.52$ & $0.0016$ & $0.0384$ & $0.7333$ & $33$\\
                                $0.53$ & $0.0036$ & $0.0364$ & $0.7667$ & $42$\\
                                $0.54$ & $0.0064$ & $0.0336$ & $0.8000$ & $57$\\
                                $0.55$ & $0.0100$ & $0.0300$ & $0.8333$ & $84$\\
                                $0.56$ & $0.0144$ & $0.0256$ & $0.8667$ & $135$\\
                                $0.57$ & $0.0196$ & $0.0204$ & $0.9000$ & $255$\\
                                $0.58$ & $0.0256$ & $0.0144$ & $0.9333$ & $635$\\
                                $0.59$ & $0.0324$ & $0.0076$ & $0.9667$ & $3067$\\
                                $0.60$ & $0.0400$ & $0.0000$ & $1.0000$ & $\inf$\\ \hline
                            \end{tabular}
                            \label{TSEQUITIES1}
                        \end{center}
                    \end{table}
                \end{small}

                where $avg$ is the average of the normalized
                increments, $e$ is the error estimate in $avg$, $c$ is
                the confidence level of the error estimate, and $n$ is
                the number of records required for that confidence
                level in that error estimate.  What this table means
                is that if a step function, from zero to $0.04$,
                (corresponding to a Shannon probability of $0.6$,) is
                applied to the system, then after $27$ records, we
                would be $70$\% confident that the error level was not
                greater than $0.0396$, or $avg$ was not lower than
                $0.0004$, which corresponds to an effective Shannon
                probability of $0.51$. Note that if many iterations of
                this example of $27$ records were performed, then
                $30$\% of the time, the average of the time series,
                $avg$, would be less than $0.0004$, and $70$\% greater
                than $0.0004$. This means that the the Shannon
                probability, $0.6$, would have to be reduced by a
                factor of $0.85$ to accommodate the error created by
                an insufficient data set size to get the effective
                Shannon probability of $0.51$. Since half the time the
                error would be greater than $0.0004$, and half less,
                the confidence level would be $1 - ((1 - 0.85) \cdot
                2) = 0.7$, meaning that if we measured a Shannon
                probability of $0.6$ on only $27$ records, we would
                have to use an effective Shannon probability of
                $0.51$, corresponding to an $avg$ of $0.0004$. For
                $33$ records, we would use an $avg$ of $0.0016$,
                corresponding to a Shannon probability of $0.52$, and
                so on.

                The table above was made by iterating the {\it
                tsstatest}\/ program, and can be approximated by a
                single pole low pass recursive discreet time
                filter~\cite[pp. 11]{Conover}, with the pole frequency
                at $0.00045$ times the time series sampling
                frequency. The accuracy of the approximation is about
                $\pm 10$\% for the first $260$ samples, with the
                approximation accuracy prediction becoming optimistic
                thereafter, ie., about $+30$\%.

                A pole frequency of $0.033$ seems a good approximation
                for working with the root mean square of the
                normalized increments, with a reasonable approximation
                to about $5---10$ time units.

                The ``instantaneous,'' weighted, and statistically
                estimated Shannon probability, $P$, can be determined
                by dividing the filtered $rms$ by the filtered $avg$,
                adding unity, and dividing by two, as in
                Equation~\ref{nsummaryappb14}.

                The advantage of the discreet time recursive single
                pole filter approximation is that it requires only $3$
                lines of code in the implementation---two for
                initialization, and one in the calculation construct.

                The single pole low pass filter is implemented from
                the following discrete time equation:

                \begin{equation}
                    v_{n + 1} = I \cdot k2 + v_{n} \cdot k1
                \end{equation}

                \noindent where $I$ is the value of the current sample
                in the time series, $v$ are the value of the output
                time series, and $k1$ and $k2$ are constants
                determined from the following equations:

                \begin{equation}
                    k1 = e^{-2 \cdot p \cdot \pi}
                \end{equation}

                \noindent and

                \begin{equation}
                    k2 = 1 - k1
                \end{equation}

                where $p$ is a constant that determines the frequency
                of the pole---a value of unity places the pole at the
                sample frequency of the time series.

        \subsection{tstradesim}
            \subidx{programs}{tstradesim}
            \subidx{tstradesim}{program}

            Source tstradesim.c is for generating a time series for
            the tstrade program.  Generates a fractal time series, of
            many stocks, concurrently.

            The input file is organized, one stock per record, with
            each record having up to five fields, of which only the
            Shannon probability need be specified. The fields are
            sequential, in any order, with field the type specified by
            a single letter---P for Shannon probability, F for wager
            fraction, N for trading volume, and I for initial
            value. Any field that is not one of these letters is
            assumed to be the stock's name. For example:

            \begin{verbatim}

                ABC, P = 0.51, F = 0.01, N = 1000, I = 31
                DEF, P = 0.52, F = 0.02, N = 500, I = 4
                GHI, P = 0.53, F = 0.03, N = 300, I = 65

            \end{verbatim}

            Naturally, single letter stock names should be avoided,
            (since P, F, N, and I, are reserved tokens.) Any
            punctuation is for clarity, and ignored. Upper or lower
            case characters may be used. The fields are delimited by
            whitespace, or punctuation. Comment records are are
            signified by a '\#' character as the first non whitespace
            character in a record. Blank records are ignored.

            The output file structure is a text file consisting of
            records, in temporal order, one record per time series
            sample.  Blank records are ignored, and comment records
            are signified by a '\#' character as the first non white
            space character in the record. Each data record represents
            an equity transaction, consisting of a minium of six
            fields, separated by white space. The fields are ordered
            by time stamp, equity ticker identifier, maximum price in
            time unit, minimum price in time unit, closing price in
            time unit, and trade volume.  The existence of a record
            with more than 6 fields is used to suspend transactions on
            the equity, concluding with the record, for example:

            \begin{verbatim}

                1      ABC     38.125  37.875  37.938  333.6
                2      DEF     3.250   2.875   3.250   7.2
                3      GHI     64.375  63.625  64.375  335.9

            \end{verbatim}

            American markets, since 1950, can be emulated with 300
            stocks, each having p = 0.505, and f = 0.03; p = 0.52, f =
            0.03 for 300 stocks seems to emulate recent markets.

            Note: this program uses the following functions from other
            references:

            \begin{description}

                \item[ran1:] which returns a uniform random deviate
                between 0.0 and 1.0. See~\cite[pp. 210]{Press},
                referencing Knuth.

                \item[gasdev:] which returns a normally distributed
                deviate with zero mean and unit variance, using ran1 () as
                the source of uniform deviates. See~\cite[pp. 217]{Press}.

                \item[gammln:] which returns the log of the results of the
                gamma function.  See~\cite[pp. 168]{Press}.

            \end{description}

            The general outline of this program is:

            \begin{enumerate}

                \item given the Shannon probability, compute the
                abscissa value that divides the area under the normal
                curve, into two sections, such that the area to the
                left of the value, divided by the total area under the
                normal curve is the Shannon probability---a
                Newton-Raphson iterated approach using Romberg
                integration to find the area is used for this

                \item for each record:

                \begin{enumerate}

                    \item compute a gaussian distributed random
                    number

                    \item add the computed abscissa value to the
                    gaussian distributed number

                    \item multiply this number by the fraction of
                    cumulative sum to be wagered

                    \item multiply this number by the cumulative sum

                    \item add this number to the cumulative sum

                \end{enumerate}

            \end{enumerate}

            This program will require finding the value of the normal
            function, given the standard deviation. The method used is
            to use Romberg/trapezoid integration to numerically solve
            for the value.

            This program will require finding the functional inverse
            of the normal, ie., Gaussian, function. The method used is
            to use Romberg/trapezoid integration to numerically solve
            the equation:

            \begin{equation}
                F\left(x\right) = \int_{0}^{x} \frac{1}{2\pi} e^{\frac{-t^{2}}{2}} dt + 0.5
            \end{equation}

            \noindent which has the derivative:

            \begin{equation}
                f\left(x\right) = \frac{1}{2\pi} e^{\frac{-x^{2}}{2}}
            \end{equation}

            \noindent Since $F(x)$ is known, and it is desired to find
            $x$,

            \begin{equation}
                F\left(x\right) - \int_{0}^{x} \frac{1}{2\pi} e^{\frac{-t^{2}}{2}} dt + 0.5 = P\left(x\right) = 0
            \end{equation}

            \noindent and the Newton-Raphson method of finding roots
            would be:

            \begin{equation}
                P_{n + 1} = P_{n} - \frac{P\left(x\right)}{f\left(x\right)}
            \end{equation}

            As a reference on Newton---Raphson Method of root finding,
            see~\cite[pp. 270]{Press}.

            As a reference on Romberg Integration,
            see~\cite[pp. 124]{Press}.

            As a reference on trapezoid iteration,
            see~\cite[pp. 120]{Press}.

            As a reference on polynomial interpolation,
            see~\cite[pp. 90]{Press}.

        \subsection{tscauchy}
            \subidx{programs}{tscauchy}
            \subidx{tscauchy}{program}
            \subidx{time series}{Cauchy}
            \subidx{Cauchy}{time series}
            \subidx{Cauchy time series}{generation}
            \subidx{generation}{Cauchy time series}
            \subidx{Cauchy time series}{simulation}
            \subidx{simulation}{Cauchy time series}

            Source tscauchy.c, Cauchy distributed noise
            generator---generates a time series.  The idea is to
            produce a 1 / f power spectrum distribution.

            The particular method used is
            from~\cite[pp. 159]{Schroeder}.

            An example output from the {\it tscauchy}\/ program
            appears in Figure~\ref{cauchyexample}.

        \subsection{tslognormal}
            \subidx{programs}{tslognormal}
            \subidx{tslognormal}{program}
            \subidx{time series}{log-normal distribution}
            \subidx{log-normal distribution}{time series}
            \subidx{simulation}{log-normal distribution}
            \subidx{log-normal distribution}{simulation}

            Source tslognormal.c is for changing the distribution of a
            time series to a log-normal distribution. The value of a
            sample in the time series is subtracted from the previous
            sample in the time series, and divided by the value of the
            previous sample. This value is multiplied by its
            exponentiation, (i.e., e-to-the-power,) and the log-normal
            fractional time series is printed to stdout.

            The input file structure is a text file consisting of
            records, in temporal order, one record per time series
            sample.  Blank records are ignored, and comment records
            are signified by a `\#' character as the first non white
            space character in the record. Data records must contain
            at least one field, which is the data value of the sample,
            but may contain many fields---if the record contains many
            fields, then the first field is regarded as the sample's
            time, and the last field as the sample's value at that
            time.

            Example usage:

            \begin{center}
                tscoins -t -p 0.6 20000 | tslognormal | tsunfraction \\
            \end{center}

            An example output from the {\it tslognormal}\/ program
            appears in Figure~\ref{lognormalexample}.

            \begin{figure}[ht]
                \begin{center}
                    \begin{minipage}[t]{0.45\textwidth}
                        \epsfxsize=1.0\linewidth
                        \epsffile{../simulation/test/tscauchy.tsnormal.eps}
                        \caption[Example output of the {\it
                            tscauchy}\/ program]{Example output of
                            the {\it tscauchy}\/ program, using 1500
                            records. This is a plot of the frequency
                            histogram.}
                        \label{cauchyexample}
                    \end{minipage}
                    \begin{minipage}[t]{0.45\textwidth}
                        \epsfxsize=1.0\linewidth
                        \epsffile{../simulation/test/tscoins.tslognormal.tsnormal.eps}
                        \caption[Example output of the {\it
                            tslognormal}\/ program]{Example output of
                            the {\it tslognormal}\/ program, using
                            2500 records of the {\it tscoins}\/ and
                            {\it tsnormal} \/programs. The data is the
                            same as in Figure~\ref{coinsexample},
                            overlayed with the distribution in
                            Figure~\ref{coinsexampleni}.}
                        \label{lognormalexample}
                    \end{minipage}
                    \hfill
                \end{center}
            \end{figure}

        \subsection{tslaplacian}
            \subidx{programs}{tslaplacian}
            \subidx{tslaplacian}{program}
            \subidx{Laplacian distribution}{generation}
            \subidx{generation}{Laplacian distribution}
            \subidx{Laplacian distribution}{time series}
            \subidx{time series}{Laplacian distribution}
            \subidx{Laplacian distribution}{simulation}
            \subidx{simulation}{Laplacian distribution}

            Source tslaplacian.c, Laplacian noise
            generator---generates a time series.  The idea is to
            produce a a time series with a Laplacian distribution. The
            elements, $e$, of the time series are made from elements
            from the uniform distribution, $n$, and:

            \begin{equation}
                e = \left\{ \begin{array}{r@{\quad:\quad}l}
                \sqrt{0.5} \cdot \ln \left(2 \cdot n\right) & n<0.5 \\
                -\sqrt{0.5} \cdot \ln \left(2 \cdot \left(1 - n\right)\right) & n>=0.5
                \end{array} \right.
            \end{equation}

            \noindent where the $\sqrt{0.5}$ is the required scaling
            for a variance of unity.

            An example output from the {\it tslaplacian}\/ program appears
            in Figure~\ref{laplacianexample}.

            \begin{figure}[ht]
                \begin{center}
                    \begin{minipage}[t]{0.45\textwidth}
                        \epsfxsize=1.0\linewidth
                        \epsffile{../simulation/test/tslaplacian.tsnormal.eps}
                        \caption[Example output of the {\it
                            tslaplacian}\/ program]{Example output of
                            the {\it tslaplacian}\/ program, using the
                            {\it tsnormal}\/ program for analysis with
                            1500 records as input.}
                        \label{laplacianexample}
                    \end{minipage}
                \end{center}
            \end{figure}

% Local Variables:
% TeX-parse-self: t
% TeX-auto-save: t
% TeX-master: "fractal.tex"
% End:
