%
% -----------------------------------------------------------------------------
%
% A license is hereby granted to reproduce this software source code and
% to create executable versions from this source code for personal,
% non-commercial use.  The copyright notice included with the software
% must be maintained in all copies produced.
%
% THIS PROGRAM IS PROVIDED "AS IS". THE AUTHOR PROVIDES NO WARRANTIES
% WHATSOEVER, EXPRESSED OR IMPLIED, INCLUDING WARRANTIES OF
% MERCHANTABILITY, TITLE, OR FITNESS FOR ANY PARTICULAR PURPOSE.  THE
% AUTHOR DOES NOT WARRANT THAT USE OF THIS PROGRAM DOES NOT INFRINGE THE
% INTELLECTUAL PROPERTY RIGHTS OF ANY THIRD PARTY IN ANY COUNTRY.
%
% Copyright (c) 1994-2006, John Conover, All Rights Reserved.
%
% Comments and/or bug reports should be addressed to:
%
%     john@email.johncon.com (John Conover)
%
% -----------------------------------------------------------------------------
%
% Revision: \RCSRevision \\
% Revision Time: \RCSTime UMT \\
% Revision Date: \RCSDate \\
% Revision Id: \RCSId \\
% Revision File: \RCSLog \\
\RCS $Revision: 0.0 $
\RCS $Date: 2006/01/20 04:38:13 $
\RCS $Id: chap2.tex,v 0.0 2006/01/20 04:38:13 john Exp $
% $Log: chap2.tex,v $
% Revision 0.0  2006/01/20 04:38:13  john
% Initial version
%
%
\newcommand{\LABPRETWO}{C} % appendix C is \ref{markets}-certain sections are referenced from this chapter. The argument must agree with \renewcommand{\LABPRE}{C} in that appendix.
%
\chapter{General Derivation for Fractal Time Series}
    \label{general}

    \subidx{strategy}{betting}
    \subidx{betting}{strategy}
    \idx{speculative markets}
    This chapter presents a general derivation of the optimization of
    betting strategies in speculative markets. It is offered in
    academic perspective, and under no circumstances would it be
    appropriate to consider it financial advice. It can serve,
    however, as an introduction to the contemporary economic theory of
    speculative markets. Rigorous and sophisticated approaches that
    address the issues of investing in speculative markets are
    contained in the bibliography.

    \section{General Derivation}
        \label{derivation}

        \idx{speculative markets}
        \subidx{speculative markets}{as iterative processes}
        Consider that investing in a speculative market is an iterated
        process\footnote{It is an important concept that in many
        cases, the iterated speculative investment process is
        implicit. For example, in the stock market, an investment can
        obviously have losses or gains, over time, without one
        actually making any additional physical investments, ie.,
        ``letting the investment ride'' constitutes a speculative
        investment in itself.}, with the objective of maximizing the
        value of the investment's cumulative returns, $R$, and that
        the process operates according to the following principles for
        each and every iteration:

        \subidx{cumulative returns}{investment}
        \subidx{investment}{cumulative returns}
        \subidx{unfair games}{as an exploitable advantage}
        \idx{Brownian motion}
        \idx{random process}
        \idx{fractional Brownian motion}
        \subidx{distribution}{Gaussian}
        \idx{Gaussian distribution}
        \begin{enumerate}

            \item For any iteration, ``wagers'' are made that are a
            fraction, $f$, of the investment's cumulative returns,
            $R$. Of course, $0 \leq f \leq 1$, and the amount wagered
            is $f \cdot R$.

            \item The investment's returns, for the current iteration,
            will occur in the next iteration, and will be determined
            by a random process, $F$, which will determine whether the
            investment's return is a loss or a gain\footnote{If the
            iteration's random process, $F$, is either $+1$ or $-1$
            for gains and losses, respectively, then the random
            process is termed ``Brownian.'' However, if the random
            process, $F$, can assume other values, besides $+1$ and
            $-1$, and, furthermore, these values have a Gaussian
            distribution, then the random process is termed
            ``fractional Brownian,''~\cite[pp. 164,
            172]{Feder},~\cite[pp. 232]{Crownover}.  In either case,
            the random process may be unfairly biased. For example,
            there can be more iterations that have gains than have
            losses, on average, in the iterative process.  Such
            processes could, potentially, offer a knowledgeable
            investor an exploitable advantage---that is why they are
            termed ``unfairly biased.''  Apparently, many speculative
            markets exhibit such phenomena. For example, many capital
            markets are alleged to have unfair
            bias,~\cite[pp. 81]{Peters:CAOITCM},~\cite[pp. 127]{Schroeder},~\cite[pp. 255]{Casti:C},~\cite[pp. 450]{Reza},~\cite[pp. 270]{Pierce}.},
            and the amount wagered, $f \cdot R$, which will determine
            the amount of the loss or gain.

            \item The investment's returns, losses and gains, for each
            iteration are summed to the cumulative returns, $R$.

        \end{enumerate}

        \noindent then, for the $n^{th}$ iteration:

        \begin{eqnarray}
            \label{iteration}
            R_{n + 1} & = & R_n + \left(R_n \cdot f_n \cdot F_n\right)\\
                      & = & R_n \left(\left(f_n \cdot F_n\right) + 1\right)
        \end{eqnarray}

        \noindent and rearranging Equation~\ref{iteration}:

        \begin{eqnarray}
            \label{iteration1}
            \frac{R_{n + 1} - R_n}{R_n} & = & F_n \cdot f_n
        \end{eqnarray}

        \noindent therefore, the rules for deriving the values of $F_n
        \cdot f_n$, for an iteration are:

        \subidx{time series increments}{deriving}
        \begin{enumerate}

            \item Subtract the value of the last iteration's
            cumulative returns from the value of the current
            iteration's cumulative returns to calculate the current
            iteration's incremental difference in cumulative returns.

            \item Divide the current iteration's incremental
            difference in cumulative returns by the value of the last
            iteration's cumulative returns.

            \item The result is the random process, $F_n$, multiplied
            by the wager fraction, $f_n$, for the last iteration.

        \end{enumerate}

        \idx{Brownian motion}
        \idx{random process}
        \idx{fractional Brownian motion}
        \subidx{distribution}{Gaussian}
        \idx{Gaussian distribution}
        \subidx{time series increments}{deriving}

        Separating an iteration's wager fraction, $f_n$, from the
        iteration's random process, $F_n$, is difficult for
        incremental differences that are characterized by Gaussian
        distributions, ie., fractional Brownian random
        processes. However, for Brownian random processes, the sign of
        the iteration's incremental difference in cumulative returns,
        ie., whether the iteration's incremental difference was a loss
        or a gain, can be used to derive the value of the random
        process, $F_n$. Taking the absolute value\footnote{The
        absolute value of the normalized increments, when averaged, is
        related to the root mean square of the increments by a
        constant. If the normalized increments are a fixed increment,
        the constant is unity. If the normalized increments have a
        Gaussian distribution, the constant is $\approx 0.8$ depending
        on the accuracy of of ``fit'' to a Gaussian distribution.} of
        the iteration's incremental difference, in the specific case
        of Brownian random processes, is the amount that was wagered,
        $f_n$, (provided, of course, that any unfair bias in the
        random process was sufficiently small, which is usually the
        case.)

        \subsection{Fibonacci Sequence of a Time Series}
            \subidx{Fibonacci}{sequence}
            \subidx{sequence}{Fibonacci}

            Interestingly, the normalized increments, when constructed
            in this manner, is simply the Fibonacci sequence of
            $V_{t}$, minus unity. The recursive representation of
            constructing the normalized increments is:

            \begin{equation}
                V_{t} = V_{t - 1} \left(1 + f_{t} F_{t}\right)
            \end{equation}

            \noindent and subtracting $V_{t - 1}$ from both sides:

            \begin{equation}
                V_{t} - V_{t - 1} = V_{t - 1} \left(1 + f_{t} F_{t}\right) - V_{t - 1}
            \end{equation}

            \noindent and dividing both sides by $V_{t - 1}$:

            \begin{equation}
                \frac{V_{t} - V_{t - 1}}{V_{t - 1}} = \frac{V_{t - 1} \left(1 + f_{t} F_{t}\right) - V_{t - 1}}{V_{t - 1}}
            \end{equation}

            \noindent and combining:

            \begin{equation}
                \frac{V_{t} - V_{t - 1}}{V_{t - 1}} = \left(1 + f_{t} F_{t}\right) - 1 =  f_{t} F_{t}
                \label{Fib1}
            \end{equation}

            \noindent but the left side of Equation~\ref{Fib1} is:

            \begin{equation}
                \frac{V_{t} - V_{t - 1}}{V_{t - 1}} = \frac{V_{t}}{V_{t - 1}} - 1
                \label{Fib2}
            \end{equation}

            \noindent which is the Fibonacci sequence of $V_{t}$,
            minus unity. The {\it tsmath}\/ program can be used to add
            unity to the time series of the normalized increments of a
            time series to construct the Fibonacci sequence of the
            equity's value.  Additionally, the time series could be
            time sampled with the {\it tssample}\/ program prior to
            constructing the normalized increments to possibly
            investigate for any cyclic
            phenomena~\cite[pp. 49]{Schroeder}.

    \section{Construction of Random Process}
        \label{crp}

        \idx{Brownian motion}
        \subidx{Brownian motion}{fixed increments}
        \idx{random process}
        \idx{fractional Brownian motion}
        \subidx{distribution}{Gaussian}
        \idx{Gaussian distribution}
        \idx{Markov---Wiener process}
        \idx{random process}
        \idx{Brown noise}
        The strict definition of Brownian motion is the cumulative sum
        of a random variable with a Gaussian
        distribution,~\cite[pp. 164]{Feder},~\cite[pp. 481]{Peitgen},~\cite[pp. 232]{Crownover}. However,~\cite[pp. 145]{Schroeder}
        mentions a Brownian process with fixed increments, also called
        a Markov---Wiener process, and,
        further~\cite[pp. 125]{Schroeder}, mentions that Brown noise
        is generated by summing independent random numbers, with no
        mention of the distribution, other than
        on~\cite[pp. 128]{Schroeder} where Brown noise is generated by
        independent increments, and states that the fluctuating
        capital of the gambler is is also, Brown noise, deducing that
        the probability of winning a coin is $p$, and a probability of
        loosing the coin is $p - 1$, which at first consideration does
        not seem to present the process of a cumulative sum of a
        random variable with a {\it Gaussian}\/ distribution.

        \idx{Brown noise}
        \subidx{time series}{sampling}
        To reconcile the issue, consider the pseudo code for a simple
        Brownian noise generator time series, of n many samples, as
        proposed by~\cite[pp. 164]{Feder},~\cite[pp. 481]{Peitgen},
        and~\cite[pp. 232]{Crownover}. From~\cite[pp. 484]{Peitgen},
        using 6, 6 sided dice, for n many samples in the time series:

        \vspace{0.25in}
        \begin{bf}
            \begin{quotation}

            \vspace{0.1in}\noindent cumulative sum = 0

            \vspace{0.1in}\noindent for i = 1 to n

                \begin{quotation}

                \vspace{0.1in}\noindent throw 6 die

                \vspace{0.1in}\noindent cumulative sum = cumulative sum + total spots on all six die

                \vspace{0.1in}\noindent print cumulative sum

                \end{quotation}

            \end{quotation}
        \end{bf}
        \vspace{0.25in}

        \subidx{time series}{sampling}
        \idx{Central Limit Theorem}
        \idx{random process}
        \idx{cumulative returns}
        \idx{Gaussian distribution}
        \idx{Brown noise}
        where, citing the content of the Central Limit Theorem, a
        Gaussian distribution is approximated since the process was a
        cumulative sum of independent and similar random events, ie.,
        summing many independent and similar random events---the spots
        on 6 dice in this case---will produce a list of numbers with a
        Gaussian distribution. Summing these numbers will produce
        Brownian noise.

        Now consider the following pseudo code, which is much the
        same, but instead of using 6 dice, only one die is used, and
        the cumulative sum sampled every 6th roll of the die:

        \vspace{0.25in}
        \begin{bf}
            \begin{quotation}

                \vspace{0.1in}\noindent cumulative sum = 0

                \vspace{0.1in}\noindent for i = 1 to 6n

                \begin{quotation}

                    \vspace{0.1in}\noindent throw the die

                    \vspace{0.1in}\noindent cumulative sum = cumulative sum + total spots on the die

                    \vspace{0.1in}\noindent if modulus i, 6 is zero

                    \begin{quotation}

                        \vspace{0.1in}\noindent print cumulative sum

                    \end{quotation}

                \end{quotation}

            \end{quotation}
        \end{bf}
        \vspace{0.25in}

        \subidx{time series}{sampling}
        \idx{white noise}
        Note that the two algorithms give, approximately, identical
        results. Apparently, sampling a sum of independent and similar
        random numbers, ie., white noise, is identical to a sum of
        random numbers with a Gaussian distribution.

        \subidx{time series}{sampling}
        \idx{random process}
        \idx{Gaussian distribution}
        \subidx{simulation}{Monte Carlo}
        \subidx{Monte Carlo}{simulation}
        \idx{cumulative sum}
        \idx{coin game}
        Now, consider the original works of Hurst cited in~\cite[pp.
        154]{Feder}, using a ``Monte Carlo'' simulation for a random
        process of independent random variables obtained by tossing
        $m$ many coins $n$ many times and taking the random variable
        to be the number of heads minus the number of tails for each
        toss of the $m$ many coins. The probability of obtaining $k$
        heads by throwing $m$ coins is
        $(\frac{1}{2})^{m}(\frac{m!}{k!(m - k)!})$. If the coin set is
        tossed $n$ times, then $k$, and the the incremental
        differences in the cumulative sum, are given by the binomial
        distribution, which approaches a Gaussian distribution for
        large $n$ and $m$. The pseudo code for such a process, again
        using a single 6 sided die instead of a coin for compatability
        with the previous pseudo code, is:

        \vspace{0.25in}
        \begin{bf}
            \begin{quotation}

                \vspace{0.1in}\noindent cumulative sum = 0

                \vspace{0.1in}\noindent for i = 1 to n

                \begin{quotation}

                    \vspace{0.1in}\noindent die sum = 0

                    \vspace{0.1in}\noindent for j = 1 to m

                    \begin{quotation}

                        \vspace{0.1in}\noindent throw the die

                        \vspace{0.1in}\noindent if the number of spots on the die > 3

                        \begin{quotation}

                            \vspace{0.1in}\noindent increment the die sum by one

                        \end{quotation}

                        \vspace{0.1in}\noindent else

                        \begin{quotation}

                            \vspace{0.1in}\noindent decrement the die sum by one

                        \end{quotation}

                    \end{quotation}

                    \vspace{0.1in}\noindent cumulative sum = cumulative sum + die sum

                    \vspace{0.1in}\noindent print cumulative sum

                \end{quotation}

            \end{quotation}
        \end{bf}
        \vspace{0.25in}

        \subidx{time series}{sampling}
        which can further be simplified by sampling the cumulative sum
        every $m$th roll of the die:

        \vspace{0.25in}
        \begin{bf}
            \begin{quotation}

                \vspace{0.1in}\noindent cumulative sum = 0

                \vspace{0.1in}\noindent for i = 1 to nm

                \begin{quotation}

                    \vspace{0.1in}\noindent throw the die

                    \vspace{0.1in}\noindent if the number of spots on the die > 3

                    \begin{quotation}

                        \vspace{0.1in}\noindent increment the the cumulative sum by one

                    \end{quotation}

                    \vspace{0.1in}\noindent else

                    \begin{quotation}

                        \vspace{0.1in}\noindent decrement the cumulative sum by one

                    \end{quotation}

                    \vspace{0.1in}\noindent if modulus i, m is zero

                    \begin{quotation}

                        \vspace{0.1in}\noindent print cumulative sum

                    \end{quotation}

                \end{quotation}

            \end{quotation}
        \end{bf}
        \vspace{0.25in}

        \subidx{time series}{sampling}
        \idx{Brownian motion}
        As mentioned in~\cite[pp. 156]{Feder}, this process is
        asymptotic with a Brownian motion of random variables with a
        Gaussian distribution, provided $m$ and $n$ are sufficiently
        large.

        \subsection{Conclusion}

            \idx{Brownian motion}
            \subidx{Gaussian}{distribution}
            \subidx{distribution}{Gaussian}
            \subidx{time series}{sampling}
            \idx{Hurst coefficient}
            \idx{random process}
            The three methods of generating a time series for Brownian
            motion with an incremental difference distribution that is
            Gaussian are approximately identical, provided:

            \begin{itemize}

                \item The sampling rate is sufficiently low to allow
                many iterations of the uniform distributed random
                process to be added to the cumulative sum between
                samples.

                \item The fractional Brownian time series is
                sufficiently close to classical, (ie., a random walk
                with a Gaussian distribution step length,) Brownian
                motion, ie., the time series' Hurst coefficient is
                sufficiently near 0.5.

            \end{itemize}

    \section{A Simple Analysis}
        \label{simple}

        \subidx{time series}{example analysis}
        \subidx{Shannon}{probability}
        \subidx{probability}{Shannon}
        Consider the following fragment of a time series:

            \vspace{0.1in}
            \noindent\hspace*{1.0in}0.2\\
            \noindent\hspace*{1.0in}-0.2\\
            \noindent\hspace*{1.0in}0.2\\
            \noindent\hspace*{1.0in}-0.2\\
            \noindent\hspace*{1.0in}0.2\\
            \vspace{0.1in}

        and if the fragment is replicated many times to produce a time
        series data file containing many records that ``oscillates,''
        on a period of $5$, with a Shannon probability of $3 / 5 =
        0.6$, since $f = 2P - 1$, where $P = 0.6$, and $f = 0.2$.

        The rationale is as follows:

        \begin{figure}[ht]
            \parbox[b]{\textwidth}
            {%
                \begin{center}
                    \setlength{\unitlength}{1.0in}
                    \begin{picture}(6.05,1.00)
                       \thicklines
                       \put(0.00,-0.04){-0.2}
                       \put(0.00,0.46){+0.2}
                       \put(0.40,-0.2){1}
                       \put(0.65,-0.2){2}
                       \put(0.90,-0.2){3}
                       \put(1.15,-0.2){4}
                       \put(1.40,-0.2){5}
                       \put(1.85,-0.2){.}
                       \put(1.90,-0.2){.}
                       \put(1.95,-0.2){.}
                       \put(2.30,-0.2){n-1}
                       \put(2.60,-0.2){n}
                       \put(2.78,-0.2){n+1}
                       \put(3.80,-0.2){.}
                       \put(3.85,-0.2){.}
                       \put(3.90,-0.2){.}
                       %first
                           \put(0.30,0.50){\line(1,0){0.25}}
                           \put(0.55,0.50){\line(0,-1){0.50}}
                           \put(0.55,0.00){\line(1,0){0.25}}

                           \put(0.80,0.00){\line(0,1){0.50}}
                           \put(0.80,0.50){\line(1,0){0.25}}
                           \put(1.05,0.50){\line(0,-1){0.50}}

                           \put(1.05,0.00){\line(1,0){0.25}}
                           \put(1.30,0.00){\line(0,1){0.50}}
                           \put(1.30,0.50){\line(1,0){0.25}}
                           % dashes
                           \put(1.65,0.50){\line(1,0){0.10}}
                           \put(1.85,0.50){\line(1,0){0.10}}
                           \put(2.05,0.50){\line(1,0){0.10}}
                       %second
                           \put(2.25,0.50){\line(1,0){0.25}}
                           \put(2.50,0.50){\line(0,-1){0.50}}
                           \put(2.50,0.00){\line(1,0){0.25}}

                           \put(2.75,0.00){\line(0,1){0.50}}
                           \put(2.75,0.50){\line(1,0){0.25}}
                           \put(3.00,0.50){\line(0,-1){0.50}}

                           \put(3.00,0.00){\line(1,0){0.25}}
                           \put(3.25,0.00){\line(0,1){0.50}}
                           \put(3.25,0.50){\line(1,0){0.25}}
                           %dashes
                           \put(3.60,0.50){\line(1,0){0.10}}
                           \put(3.80,0.50){\line(1,0){0.10}}
                           \put(4.00,0.50){\line(1,0){0.10}}
                        %third
                           \put(4.20,0.50){\line(1,0){0.25}}
                           \put(4.45,0.50){\line(0,-1){0.50}}
                           \put(4.45,0.00){\line(1,0){0.25}}

                           \put(4.70,0.00){\line(0,1){0.50}}
                           \put(4.70,0.50){\line(1,0){0.25}}
                           \put(4.95,0.50){\line(0,-1){0.50}}

                           \put(4.95,0.00){\line(1,0){0.25}}
                           \put(5.20,0.00){\line(0,1){0.50}}
                           \put(5.20,0.50){\line(1,0){0.25}}
                           %dashes
                           \put(5.55,0.50){\line(1,0){0.10}}
                           \put(5.75,0.50){\line(1,0){0.10}}
                           \put(5.95,0.50){\line(1,0){0.10}}
                    \end{picture}
                    \vspace*{0.1in}
                    \caption[Win or Loss Fraction of Cumulative Sum.]{Win or
                        Loss Fraction of Cumulative Sum.}
                    \label{WSFOCS}
                \end{center}
            }
        \end{figure}

        \subsection{``Average'' Exponential Returns}
            \label{areturns}

            \subidx{returns}{exponential}
            \subidx{exponential}{returns}
            \subidx{Shannon}{probability}
            \subidx{probability}{Shannon}
            \idx{cumulative returns}
            \subidx{time series increments}{mean}
            In Figure~\ref{WSFOCS}, there are $3$ $+0.2$'s for every
            $2$ $-.2$'s, for an average of $+0.2$ per $5$ time units,
            for an average of $+0.2 / 5 = +0.04$. The reason for the
            numbers, $+0.2$ and $-0.2$, is that it is the optimum for
            a Shannon probability of $0.6$, since $0.2 = 2P - 1$,
            (which also equals $P - (1 - P)$,) where $2 \cdot 0.6 - 1 =
            0.2$, which is the optimal amount of the cumulative
            returns to wager with an unfair coin that has a
            probability of $0.6$ of a win, ie., $3$ out of $5$. If the
            $n-1$'th value in the time series is subtracted from the
            $n$'th value, and the value of this subtraction divided by
            the $n-1$'th value, then the quotient should be $+0.2$ or
            $-0.2$ depending on the whether the wager was won or lost.
            It is an important insight that the ``average'' returns is
            not useful, and creates substantial errors. See
            Section~\ref{ereturns}, below.

            Under this scenario, $P = 0.6$, and the returns are:

            \begin{equation}
                2^{0.029049406} = e^{0.020135514}
                \label{expequation}
            \end{equation}

            \subidx{programs}{tsshannon}
            \subidx{tsshannon}{program}
            which can be verified with the program {\it tsshannon}\/,
            which is briefly described in appendix~\ref{programs}, and
            is consistent with~\cite[pp. 128]{Schroeder}.

            But, using the ``average'' value, of $0.04$:

            \begin{equation}
                2^{0.056583528} = e^{0.039220713}
            \end{equation}

            \noindent which is in substantial error.

        \subsection{Exponential Returns}
            \label{ereturns}

            \subidx{programs}{tsunfairbrownian}
            \subidx{tsunfairbrownian}{program}
            \idx{exponential returns}
            Using {\it tsunfairbrownian}\/, which is also described in
            appendix~\ref{programs}, with arguments of -f 0.2 will
            construct an exponential data time series that is known to
            be optimum, ie., a Shannon probability of $0.6$, with an
            optimal wager fraction of $0.2$, with an ``approximate''
            Brownian motion noise content-albeit not random. It is
            useful for evaluating analytical methodologies. The
            derivation of the exponential is as follows:

            \noindent assume an exponential function:

            \begin{equation}
               f\left(t\right) = e^{kt}
               \label{eequation}
            \end{equation}

            \noindent then:

            \begin{equation}
                \frac{f\left(t\right) - f\left(t - 1\right)}{f\left(t\right)} = A = \frac{e^{kt} - e^{k\left(t - 1\right)}}{e^{k\left(t - 1\right)}}
            \end{equation}

            \noindent or:

            \begin{equation}
                A = \frac{e^{kt} - e^{kt - k}}{e^{kt - k}} = \frac{e^{kt} - e^{kt}e{-k}}{e^{kt}e^{-k}} = \frac{1 - e^{-k}}{e^{-k}} = e^{k} - 1
                \label{avalue}
            \end{equation}

            \noindent where:

            \begin{equation}
                k = \ln \left(A + 1\right)
                \label{kvalue}
            \end{equation}

            \subidx{logarithmic}{returns}
            \subidx{returns}{logarithmic}
            As a useful equation in data reduction involving Shannon
            probability, which requires an argument of logarithmic
            returns in bits:

            \begin{equation}
                2^{bits \cdot t} = \left(A + 1\right)^{t}
            \end{equation}

            \noindent or:

            \begin{equation}
                \left(bits \cdot t\right) \ln \left(2\right) = t \cdot \ln \left(A + 1\right)
            \end{equation}

            \noindent and dividing both sides of the equation by $t$
            and solving for the number of bits:

            \begin{equation}
                bits = \frac{\ln \left(A + 1\right)}{\ln \left(2\right)}
                \label{abits}
            \end{equation}

            Now, consider the iterated cycles in Figure~\ref{WSFOCS},
            beginning with cumulative returns of $1$:

            \begin{itemize}

               \item after the first, the cumulative returns are
               $1.2$, since it it was a ``win.''

               \item after the second, the cumulative returns are $1.2
               \cdot 0.8 = 0.96$, since it was a ``loss.''

               \item after the third, the cumulative returns are $1.2
               \cdot 0.8 \cdot 1.2 = 1.152$, since it was a ``win.''

               \item after the fourth, the cumulative returns are $1.2
               \cdot 0.8 \cdot 1.2 \cdot 0.8 = 0.9216$, since it was a
               ``loss.''

               \item after the fifth, the cumulative returns are $1.2
               \cdot 0.8 \cdot 1.2 \cdot 0.8 \cdot 1.2 = 1.10592$,
               since it was a ``win.''

            \end{itemize}

            \noindent Therefore:

            \begin{equation}
                R_{n+5} = R_{n} \cdot \left(1.2 \cdot 0.8 \cdot 1.2 \cdot 0.8 \cdot 1.2\right) = 1.10592R_{n}
            \end{equation}

            \noindent and finding the average gains per one iteration
            period of the ``game:''

            \begin{equation}
                R_{n+1} = R{n} \cdot 1.10592^{\frac{1}{5}} = R_{n} \cdot 1.020339601
                \label{aequation}
            \end{equation}

            \subidx{time series increments}{revenue gain}
            \noindent or the incremental revenue gain, from one time period
            to the next is:

            \begin{equation}
                \frac{R_{n+1}}{R_{n}} = 1.020339601
            \end{equation}

            \noindent or:

            \begin{equation}
                \frac{R_{n+1}}{R_{n}} - 1 = 1.020339601 - 1 0.020339601 = A
            \end{equation}

            \noindent where $A$ is from Equation~\ref{avalue}. From
            Equation~\ref{kvalue}:

            \begin{equation}
                k = \ln \left(A + 1\right) = \ln \left(1.020339601\right) = 0.020135514
            \end{equation}

            \idx{cumulative returns}
            \noindent Therefore, the formula for the exponential
            cumulative returns function is, from
            Equation~\ref{eequation}:

            \begin{equation}
                f\left(t\right) = e^{kt} = e^{0.020135514t}
            \end{equation}

            \noindent which is Equation~\ref{expequation}. The time
            series analyzed in this section is simulated in
            appendix~\ref{markets}, Section~\ref{\LABPRETWO:TSTE}.  It
            is an important insight that the ``average'' returns
            derived in Section~\ref{areturns} is not useful, and
            creates substantial errors.

       \subsection{General Analysis of Exponential Returns}
            \label{GA}

            \subidx{Shannon}{probability}
            \subidx{probability}{Shannon}
            \idx{random process}
            Consider a time series, similar to Figure~\ref{WSFOCS},
            but with a true random distribution. As in
            Section~\ref{areturns}, the Shannon probability, $P$, is
            $P \leq 0 \leq 1$.  The fraction of the cumulative
            returns, $R$, that is wagered with each iteration of the
            game is $f$, where $0 \leq f \leq 1$, and $F$ is an
            independent random process:

            \begin{equation}
                 F = \left\{ \begin{array}{l l}
                                    +1, & \mbox{with a probability of P}\\
                                    -1, & \mbox{with a probability of 1 - P}
                             \end{array}
                     \right.
                 \label{Fequation}
            \end{equation}

            Then in general, the returns after $N$ many iterations
            would be:

            \begin{eqnarray}
                R_{N} & = & R_{0} \cdot \left(\left(1 + F_{1} \cdot f_{1}\right) \cdot \left(1 + F_{2} \cdot f_{2}\right) \cdot \left(1 + F_{3} \cdot f_{3}\right) \cdot \hspace*{0.1in} \cdots \hspace*{0.1in}\right.\nonumber\\
                      &   & \cdot \left(1 + F_{n-1} \cdot f_{n-1}\right) \cdot \left(1 + F_{n} \cdot f_{n}\right) \cdot \left(1 + F_{n+1} \cdot f_{n+1}\right) \cdot \hspace*{0.1in} \cdots \hspace*{0.1in}\nonumber\\
                      &   & \cdot \left.\left(1 + F_{N-1} \cdot f_{N-1}\right) \cdot \left(1 + F_{N} \cdot f_{N}\right)\right)
                \label{requation}
            \end{eqnarray}

            \idx{cumulative returns}
            \noindent where $R_{0}$ is the cumulative returns at time
            $0$, $f_{n}$ is the fraction of the cumulative returns
            wagered in the $n$'th iteration of the game, and $F_{n}$
            determines whether the $n$'th wager was one or lost, ie.,
            added or subtracted from the cumulative returns.

            Out of $N$ many iterations, there will be $P \cdot N$ many
            wins, and $(1 - P) \cdot N$ many losses. Therefore, if $f$
            is constant,~\cite[pp. 38]{Shannon},~\cite[pp. 114,
            pp. 450]{Reza},~\cite[pp. 270]{Pierce},
            \cite[pp. 155]{Klir},~\cite[pp. 9]{Ash},
            Equation~\ref{requation} reduces to:

            \begin{equation}
                R_{N} = R_{0} \left(1 + f\right)^{PN} \left(1 - f\right)^{\left(1 - P\right)N}
            \end{equation}

            \noindent or:

            \begin{equation}
                \frac{R_{N}}{R_{0}} = \left(1 + f\right)^{PN} \left(1 - f\right)^{\left(1 - P\right)N}
                \label{returnequation}
            \end{equation}

            \idx{cumulative returns}
            \noindent if N is sufficiently large, then the average
            increase in cumulative returns for one iteration of the
            game, $\frac{R_{n+1}}{R_{n}}$, is:

            \begin{eqnarray}
                \left(\frac{R_{n+1}}{R_{n}}\right)^{N} & = & \frac{R_{N}}{R_{0}}\\
                                                       & = & \left(1 + f\right)^{PN} \left(1 - f\right)^{\left(1 - P\right)N}\\
                                                       & = & \left[\left(1 + f\right)^{P} \left(1 - f\right)^{\left(1 - P\right)}\right]^{N}
            \end{eqnarray}

            \noindent and:

            \begin{equation}
                \frac{R_{n+1}}{R_{n}} = \left(1 + f\right)^{P} \left(1 - f\right)^{\left(1 - P\right)}
                \label{onereturnequation}
            \end{equation}

            \noindent and from Equation~\ref{kvalue}:

            \begin{equation}
                k = \ln \left(\left(1 + f\right)^{P} \left(1 - f\right)^{\left(1 - P\right)} - 1\right)
            \end{equation}

            \idx{cumulative returns}
            \noindent where the equation for the cumulative returns, as a
            function of time, $R(t)$, from Equation~\ref{eequation}, is:

            \begin{eqnarray}
               R\left(t\right) & = & e^{\left(\ln \left(\left(1 + f\right)^{P} \left(1 - f\right)^{\left(1 - P\right)} - 1\right)\right)t}\\
                               & = & \left[\left(1 + f\right)^{P} \left(1 - f\right)^{1 - P}\right]^{t}
               \label{requation1}
            \end{eqnarray}

            \idx{cumulative returns}
            Using, for example, $P = 0.6$ and $f = 0.2$, the
            cumulative returns, as a function of time, would be:

            \begin{eqnarray}
               R\left(t\right) & = & \left[\left(1 + 0.2\right)^{0.6} \left(1 - 0.2\right)^{\left(1 - 0.6\right)}\right]^{t}\\
                               & = & \left[1.2^{0.6} \cdot 0.8^{0.4}\right]^{t}\\
                               & = & \left[1.115600622 \cdot 0.914610104\right]^{t}\\
                               & = & \left[1.020339601\right]^{t}
            \end{eqnarray}

            \noindent which is in agreement with
            Equation~\ref{aequation}

            \subsubsection{Optimization of Returns}

            \idx{cumulative returns}
            From Equation~\ref{returnequation}, the average increase
            in the cumulative returns for one iteration of the game
            would be:

            \begin{eqnarray}
                              R_{N} & = & R_{0} \left(1 + f\right)^{PN} \left(1 - f\right)^{\left(1 - P\right)N}\\
                \frac{R_{N}}{R_{0}} & = & \left(1 + f\right)^{PN} \left(1 - f\right)^{\left(1 - P\right)N}\\
                \frac{R_{1}}{R_{0}} & = & \left(1 + f\right)^{P} \left(1 - f\right)^{\left(1 - P\right)}
            \end{eqnarray}

            \subidx{cumulative returns}{maximizing}
            \noindent which can be maximized to maximize the growth in
            the cumulative returns, $G$, for each iteration of the
            game. Considering $G$ as a function of $f$, the fraction
            of the the cumulative returns wagered on each iteration of
            the game:

            \begin{equation}
                G\left(f\right) = \frac{R_{1}}{R_{0}} = \left(1 + f\right)^{P} \left(1 - f\right)^{\left(1 - P\right)}
            \end{equation}

            \noindent and taking the derivative, and equating to $0$
            to find the maxima:

            \begin{equation}
                \frac{dG\left(f\right)}{df} = P\left(1 + f\right)^{P - 1} \left(1 - f\right)^{1 - P} - \left(1 - P\right) \left(1 - f\right)^{1 - P - 1} \left(1 + f\right)^{P} = 0
            \end{equation}

            \noindent and combining terms:

            \begin{equation}
                P\left(1 + f\right)^{P - 1} \left(1 - f\right)^{1 - P} - \left(1 - P\right) \left(1 - f\right)^{-P} \left(1 + f\right)^{P} = 0
            \end{equation}

            \noindent and splitting:

            \begin{equation}
                P\left(1 + f\right)^{P - 1} \left(1 - f\right)^{1 - P} = \left(1 - P\right) \left(1 - f\right)^{-P} \left(1 + f\right)^{P}
            \end{equation}

            \noindent and taking the natural logarithm of both sides
            of the equation:

            \begin{equation}
                \ln \left(P\right) + \left(P - 1\right) \ln \left(1 + f\right) + \left(1 - P\right) \ln \left(1 - f\right) = \ln \left(1 - P\right) - P \ln \left(1 - f\right) + P \ln \left(1 + f\right)
            \end{equation}

            \noindent and combining terms:

            \begin{eqnarray}
                (\left(P - 1\right) \ln \left(1 + f\right) - P \ln \left(1 + f\right) + \left(1 - P\right) \ln \left(1 - f\right) + P \ln \left(1 - f\right) & = & \ln \left(1 - P\right) - \ln \left(P\right)\\
                                                                                                              \ln \left(1 - f\right) - ln \left(1 + f\right) & = & \ln \left(1 - P\right) - \ln \left(P\right)\\
            \end{eqnarray}

            \noindent and performing the logarithmic operations:

            \begin{equation}
                \ln \left(\frac{1 - f}{1 + f}\right) = \ln \left(\frac{1 - P}{P}\right)
            \end{equation}

            \noindent and exponentiating:

            \begin{eqnarray}
                \frac{1 - f}{1 + f} & = & \frac{1 - P}{P}\\
                P\left(1 - f\right) & = & \left(1 - P\right)\left(1 + f\right)\\
                             P - Pf & = & 1 - Pf - P + f\\
                                 2P & = & 1 + f
            \end{eqnarray}

            \noindent and finally:

            \begin{equation}
                f = 2P - 1
                \label{optimumequation}
            \end{equation}

            \subidx{cumulative returns}{maximizing} \noindent which is
            identical to Equations~\ref{bet_optimum1}
            and~\ref{bet_optimum5}, and agrees with~\cite[pp. 128,
            151]{Schroeder}. A more elegant approach to maximization
            of the cumulative returns, using information---theoretic
            techniques, is presented
            in~\cite[pp. 38]{Shannon},~\cite[pp. 114,
            pp. 450]{Reza},~\cite[pp. 270]{Pierce},~\cite[pp. 155]{Klir},~\cite[pp. 9]{Ash}.


            \subidx{time series increments}{root mean square}
            Note that, referring to Figure~\ref{WSFOCS} and
            Equations~\ref{Fequation} and~\ref{requation}, that the
            root mean square, $rms$, of Equation~\ref{Fequation} is
            simply $f$---assuming $f$ is constant and $N$ is
            sufficiently large, or:

            \begin{equation}
                rms = f
                \label{rms}
            \end{equation}

            \subidx{time series increments}{mean}
            \noindent and the average $avg$ will be $P - (1 - P)$ time
            this value, since there will be $P$ many $f$'s and $P - 1$
            many $-f$'s, or:

            \begin{equation}
                avg = rms \left[P - \left(1 - P\right)\right] = rms \left(2P - 1\right)
                \label{avg}
            \end{equation}

            Note that if the game is not being run at the optimum,
            where $2P - 1 = f$, then $f$ must be increased by a factor
            which can be calculated from measuring $avg$ and $rms$.
            Letting $K$ be the amount that $f$ must be multiplied by
            so that the $f$ will be optimum, $f_{opt}$, from
            Equation~\ref{avg}:

            \begin{equation}
                \frac{avg}{rms} = \left(2P - 1\right)
                \label{fopt3}
            \end{equation}

            \noindent which will be true whether $f$ is optimum or
            not. But $2P - 1 = f_{opt}$ when optimum, therefore:

            \begin{equation}
                \frac{avg}{rms} = f_{opt}
                \label{fopt1}
            \end{equation}

            \noindent but from Equation~\ref{rms}, $rms = f$, and
            after dividing both sides by $rms = f$:

            \begin{equation}
                \frac{avg}{rms^2} = \frac{f_{opt}}{rms} = K
                \label{fopt2}
            \end{equation}

        \subsection{Ancillary derivation of the Shannon Probability, $P$}

            \subidx{Shannon}{probability}
            \subidx{probability}{Shannon}
            As an interesting manipulation to
            Equation~\ref{onereturnequation},

            \begin{equation}
                \frac{R_{n+1}}{R_{n}} = \left(1 + f\right)^{P} \left(1 - f\right)^{\left(1 - P\right)}
            \end{equation}

            \noindent or:

            \begin{equation}
                \frac{R_{n+1}}{R_{n}} = \frac{\left(1 + f\right)^{P} \left(1 - f\right)}{\left(1 - f\right)^{P}}
            \end{equation}

            \noindent and:

            \begin{eqnarray}
                \frac{\frac{R_{n+1}}{R_{n}}}{\left(1 - f\right)} & = & \frac{\left(1 + f\right)^{P}}{\left(1 - f\right)^{P}}\\
                                                                 & = & \left(\frac{\left(1 + f\right)}{\left(1 - f\right)}\right)^{P}
            \end{eqnarray}

            \noindent and taking the natural logarithm of both sides:

            \begin{equation}
                \ln \left(\frac{\frac{R_{n+1}}{R_{n}}}{\left(1 - f\right)}\right) = P \ln \left(\frac{\left(1 + f\right)}{\left(1 - f\right)}\right)
            \end{equation}

            \noindent and solving for $P$:

            \begin{equation}
                P = \frac{\ln
                \left(\frac{\frac{R_{n+1}}{R_{n}}}{\left(1 - f\right)}\right)}{\ln \left(\frac{\left(1 + f\right)}{\left(1 - f\right)}\right)}
                \label{pequation}
            \end{equation}

            Equation~\ref{pequation} presents an important
            relationship, since metric methodologies can be used to
            quantize $P$, $\frac{R_{n+1}}{R_{n}}$, and $f$.

    \section{The Case of the Time Sampled Time Series}
        \label{samplets}

        \subidx{time series}{time sampled}
        In many cases, the time series data under consideration is
        time sampled. For example, the data may be by month, which is
        the aggregate of many time series, in a fashion similar to the
        situation outlined in this chapter, Section~\ref{crp}.

        Referring to Figure~\ref{WSFOCS} and Equations~\ref{Fequation}
        and~\ref{requation}, the root mean square, $rms$, of
        Equation~\ref{Fequation} is simply $f$---assuming $f$ is
        constant and $N$ is sufficiently large.  From
        Equation~\ref{avg}:

        \begin{equation}
            avg = rms \left[P - \left(1 - P\right)\right] = rms \left(2P - 1\right)
        \end{equation}

        \noindent and letting $rms$ = $f$:

        \begin{equation}
            avg = f \left(2P - 1\right)
            \label{avgts}
        \end{equation}

        \noindent But from Equation~\ref{bet_optimum5}:

        \begin{equation}
           F = 2P - 1
        \end{equation}

        \noindent where $f$ is $F$ with the signs removed, following
        reasoning similar to those in Equations~\ref{Fequation}
        and~\ref{requation}, or:

        \begin{equation}
           f = 2P - 1
        \end{equation}

        \noindent Inserting into Equation~\ref{avgts}:

        \begin{equation}
            avg = f^2 = rms^2
            \label{avgrmss}
        \end{equation}

        It would be desirable to consider $avg$, $f$, and $rms$ as
        functions of the number of iterations in a time sample, $N$:

        \begin{equation}
            avg\left(N\right) = f\left(N\right)^2 = rms\left(N\right)^2
            \label{avgts1}
        \end{equation}

        \subidx{time series increments}{root mean square}
        \subidx{time series}{time sampled}
        Since the root mean square value, $rms$ will combine the
        iterations in a sample time interval in a root mean square
        fashion, it would be expected that $rms$ would be proportional
        to the square root of the number of iterations in a time
        sample\footnote{Technically, the results of the iterations are
        multiplied together, as described in
        Equation~\ref{requation}. However, it can be reasoned that
        these multiplications can be broken down into additions in a
        fashion similar to multiplying $3$ by $2$. The operation can
        be performed by adding $2$, three times, $2 + 2 + 2 = 6$. The
        reasoning is that root mean square operations can be used for
        multiplying variables with a normal distribution together, in
        a manner of implementing Equation~\ref{requation} with the
        methodologies outlined in this chapter,
        Section~\ref{crp}. The reasoning is that the random process,
        summed over $N$ many intervals will have the characteristics
        of a Central Limit process.}, $N$, or $rms$ as a function of
        $N$:

        \begin{equation}
            rms\left(N\right) = rms_0 \sqrt{N}
        \end{equation}

        \subidx{time series}{time sampled}
        where $rms_0$ is a constant of proportionality and is the
        value of $rms$ when the number of intervals in a time sample
        is one. From this, it can be concluded that the value of $(2P
        - 1)$ in Equation~\ref{avgts} must be proportional to the
        square root of $N$, also. Solving for the function $P(N)$:

        \begin{equation}
            2P\left(N\right) - 1 = rms_0 \sqrt{N}
        \end{equation}

        \noindent and solving for $P(N)$:

        \begin{equation}
            P\left(N\right) = \frac{rms_0 \sqrt{N} + 1}{2}
        \end{equation}

        From Equation~\ref{avgts1}:

        \begin{equation}
            avg\left(N\right) = \left(rms_0 \sqrt{N}\right)^2 = rms_0^2 N
            \label{avgrms}
        \end{equation}

        \noindent or $avg$ will vary linearly with $N$.

        \subidx{time series}{time sampled}
        \subidx{programs}{tscoin}
        \subidx{tscoin}{program}
        As a simulated example, the variables $rms$, $mean$, which is
        $avg$ above, and $P$ can be plotted. In the
        directory~graphics/probability is a collection of files,
        generated by the script file ``probability'', which uses $P$
        with an initial value of $0.51$ and constructs files that are
        sampled on intervals $N$ = $1$ through $100$. The graphs of
        the functions are superimposed with the computed theoretical
        values, in Figure~\ref{figprobability}.

        As another simulated example, the output of the programs {\it
        tsrms}, {\it tslsq}\/, {\it tsshannon}\/, {\it
        tslogreturns}\/, and {\it tsnormal}\/ are plotted as a
        function of the sample interval in the
        directory~/graphics/brownian as generated by the script file
        ``brownian,'' which uses $P$ with an initial value of $0.501$
        and constructs files that are sampled on intervals $N$ = $1$
        through $100$. The graphs of the functions are superimposed
        with the computed theoretical values, in
        Figure~\ref{figbrownian}.

        \begin{figure}[ht]
            \begin{center}
                \begin{minipage}[t]{0.45\textwidth}
                    \epsfxsize=1.0\linewidth
                    \epsffile{graphics/probability/probability.eps}
                    \caption[Example probability as a function of sample
                        intervals]{Example probability, as a function of
                        sample intervals, root mean square, mean, and
                        standard deviation, of Equation~\ref{requation}
                        with $P = 0.51$.}
                    \label{figprobability}
                \end{minipage}
                \hfill
                \begin{minipage}[t]{0.45\textwidth}
                    \epsfxsize=1.0\linewidth
                    \epsffile{graphics/brownian/brownian.eps}
                    \caption[Example program outputs as a function of
                        sample intervals]{Example program outputs as a
                        function of sample intervals.}
                    \label{figbrownian}
                \end{minipage}
            \end{center}
        \end{figure}

        \subidx{time series}{time sampled}
        \subidx{time series increments}{mean}
        \subidx{programs}{tscoin}
        \subidx{tscoin}{program}
        As interesting aside, the file ``mean,'' produced by the
        ``probability.vs.N'' script, should be linear as a function of
        $N$, at least in principle. The general formula for these
        files, as a function of the sampling period, $N$, is $mean =
        rms (2P - 1)$. If the input file is time sampled, then the
        formula is $mean N = rms \sqrt(N) (2P - 1) \sqrt (N)$, and the
        ``mean'' file should be linear, with a slope of the mean as a
        function of $N$.  To exercise this, see the file
        ``probability.vs.N'' in the
        directory~../markets/tscoin.tssample.

        \subsection{Simulation of Simultaneous Games}

            Consider playing $N$ many coin tossing games,
            simultaneously. This can be simulated as a binomial
            distribution.  Let $f =$ fraction of capital wagered in
            each unit of time (this is the normalized increment of the
            total capital, where the total capital is the
            instantaneous value of the sum of the capital for all
            games.) Then, letting $avg =$ the average of $f$, $rms =$
            the root mean square of $f$, and $P_f =$ the fundamental
            Shannon probability of each coin, in each game, (assumed
            to be identical,) then:

            \begin{equation}
                avg = \left(2P_f + 1\right)Nf
            \end{equation}

            \noindent and:

            \begin{equation}
                rms = f\sqrt{N}
            \end{equation}

            \noindent where $f$, $rms$, and $avg$ are measured, and
            represent the fraction of capital wagered, the root mean
            square, and the average of the normalized increments of
            the time series of the total capital, ie., playing N many
            games, simultaneously:

            \begin{equation}
                rms = \frac{f}{\sqrt{N}}
            \end{equation}

            \noindent and:

            \begin{equation}
                avg = \left(2P_f - 1\right)f
            \end{equation}

            What this means is that the average, $avg$, adds linearly,
            and the $rms$ root mean square, where the wager on each of
            the simultaneous games is $f / N$.

            It makes no difference whether many coins are thrown at
            once, or one coin thrown many times.

            In other words, playing many games simultaneously, each
            with coins of equal Shannon probability, will not effect
            the growth of the total capital. The volatility of the
            total capital will go down by the square root of the
            number of games played.

            It is rather computationally inefficient, but this can be
            verified with the {\it tsbinomial}\/ program, using the -r
            option.

    \section{Analysis of Brownian Motion with Fixed Increments}
        \label{abmfi}
        \subidx{cumulative returns}{increments}
        \subidx{increments}{cumulative returns}
        \subidx{Brownian motion}{fixed increments}
        \idx{random process}

        \subidx{cumulative returns}{increments}
        \subidx{increments}{cumulative returns}
        \subidx{Brownian motion}{fixed increments}
        \idx{random process}
        \subidx{Shannon}{probability}
        \subidx{probability}{Shannon}
        Consider a time series, similar to Figure~\ref{WSFOCS}, but
        with a true random distribution. As in Section~\ref{areturns},
        the Shannon probability, $P$, is $P \leq 0 \leq 1$.  The
        fraction of the cumulative returns, $R$, that is wagered with
        each iteration of the game is $f$, where $0 \leq f \leq 1$,
        and $F$ is an independent random process, from
        Equation~\ref{Fequation}:

        \begin{equation}
             F = \left\{ \begin{array}{l l}
                                +1, & \mbox{with a probability of P}\\
                                -1, & \mbox{with a probability of 1 - P}
                         \end{array}
                 \right.
        \end{equation}

        Then in general, the returns after $N$ many iterations
        would be, from Equation~\ref{requation}:

        \begin{eqnarray}
            R_{N} & = & R_{0} \cdot \left(\left(1 + F_{1} \cdot f_{1}\right) \cdot \left(1 + F_{2} \cdot f_{2}\right) \cdot \left(1 + F_{3} \cdot f_{3}\right) \cdot \hspace*{0.1in} \cdots \hspace*{0.1in}\right.\nonumber\\
                  &   & \cdot \left(1 + F_{n-1} \cdot f_{n-1}\right) \cdot \left(1 + F_{n} \cdot f_{n}\right) \cdot \left(1 + F_{n+1} \cdot f_{n+1}\right) \cdot \hspace*{0.1in} \cdots \hspace*{0.1in}\nonumber\\
                  &   & \cdot \left.\left(1 + F_{N-1} \cdot f_{N-1}\right) \cdot \left(1 + F_{N} \cdot f_{N}\right)\right)
        \end{eqnarray}

        \idx{time series}
        \idx{cumulative sum}
        \idx{random process}
        \subidx{incremental returns}{calculation}
        \noindent which, after performing a process similar to the
        operations described in Chapter~\ref{methodology},
        Section~\ref{generalconcepts}, would result in
        Equation~\ref{iteration1}:

        \begin{eqnarray}
            \frac{R_{n + 1} - R_n}{R_n} & = & F_n \cdot f_n
            \label{bmformula1}
        \end{eqnarray}

        \noindent which is the increments of the cumulative returns.

        \subsubsection{Standard Deviation of Increments of the Cumulative Returns}
            \subidx{standard deviation}{increments of cumulative returns}
            \subidx{increments of cumulative returns}{standard deviation}

            The standard deviation, $\sigma$, of the increments of
            the cumulative returns is:

            \begin{equation}
                \sigma^2 = \frac{1}{N} \sum_{i=1}^{N} \left(F_n f_n - avg\right)^2
                \label{stddev1}
            \end{equation}

            \noindent where $avg$ is the average of the increments
            of the cumulative returns, from Equation~\ref{avg}:

            \begin{equation}
                avg = rms \left[P - \left(1 - P\right)\right] = rms \left(2P - 1\right)
            \end{equation}

            \noindent and from Equation~\ref{rms}:

            \begin{equation}
                rms = f
            \end{equation}

            \noindent or:

            \begin{equation}
                avg = rms \left(2P - 1\right) = f \left(2P - 1\right)
            \end{equation}

            \noindent and Equation~\ref{stddev1} becomes, if $f_n$
            is constant:

            \begin{equation}
                \sigma^2 = \frac{1}{N} \sum_{i=1}^{N} \left(F_n f - f \left(2P - 1\right)\right)^2
                \label{stddev2}
            \end{equation}

            \noindent and factoring $f$:

            \begin{equation}
                \sigma^2 = \frac{f^2}{N} \sum_{i=1}^{N} \left(F_n - \left(2P - 1\right)\right)^2
                \label{stddev3}
            \end{equation}

            \noindent and additionally, the summation series will
            contain $PN$ many cases where $F_n = 1$, and $(1 - P)N$
            many cases where $F_n = -1$, or:

            \begin{equation}
                \sigma^2 = \frac{f^2}{N} \left(PN\left(1 - \left(2P - 1\right)\right)^2 + \left(1 - P\right)N\left(-1 - \left(2P - 1\right)\right)^2\right)
                \label{stddev4}
            \end{equation}

            \noindent and factoring $N$:

            \begin{eqnarray}
                \sigma^2 & = & \frac{Nf^2}{N} \left(P\left(1 - \left(2P - 1\right)\right)^2 + \left(1 - P\right)\left(-1 - \left(2P - 1\right)\right)^2\right)\\
                         & = & f^2 \left(P\left(1 - \left(2P - 1\right)\right)^2 + \left(1 - P\right)\left(-1 - \left(2P - 1\right)\right)^2\right)\\
                         & = & f^2 \left(P\left(1 - 2P + 1\right)^2 + \left(1 - P\right)\left(-1 - 2P + 1\right)^2\right)\\
                         & = & f^2 \left(P\left(2 - 2P\right)^2 + \left(1 - P\right)\left(2P\right)^2\right)\\
                         & = & f^2 \left(P\left(2 - 2P\right)^2 + \left(1 - P\right)\left(2P\right)^2\right)\\
                         & = & 4f^2 \left(P\left(1 - P\right)^2 + \left(1 - P\right)\left(P\right)^2\right)\\
                         & = & 4Pf^2 \left(\left(1 - P\right)^2 + \left(1 - P\right)\left(P\right)\right)\\
                         & = & 4Pf^2 \left(\left(1 - P\right)^2 + P\left(1 - P\right)\right)\\
                         & = & 4P(1 - P)f^2 \left(\left(1 - P\right) + P\right)\\
                         & = & 4P\left(1 - P\right)f^2
                \label{stddev5}
            \end{eqnarray}

            \noindent or:

            \begin{equation}
                \sigma^2 = 4 f^2 P\left(1 - P\right)
                \label{stddev6}
            \end{equation}

            \noindent and solving Equation~\ref{stddev6} for $f$:

            \begin{equation}
                \frac{\sigma^2}{4P\left(1 - P\right)} =  f^2
                \label{stddev7}
            \end{equation}

            \noindent or:

            \begin{equation}
                f = \frac{\sigma}{\sqrt{4P\left(1 - P\right)}} = \frac{\sigma}{2\sqrt{P\left(1 - P\right)}}
                \label{stddev8}
            \end{equation}

            \noindent and substituting into Equation~\ref{avg}

            \begin{equation}
                avg = rms \left(2P - 1\right) = \frac{\sigma \left(2P - 1\right)}{2\sqrt{P\left(1 - P\right)}}
                \label{stddev9}
            \end{equation}

            \subidx{standard deviation, mean, root mean square}{relationship}
            \subidx{mean, standard deviation, root mean square}{relationship}
            \subidx{root mean square, mean, standard deviation}{relationship}
            \noindent which states the relationship between the
            average, or mean, the standard deviation, and the root
            mean square of the increments of the returns for a time
            series representing Brownian motion with fixed
            increments. Furthermore, metrics for these values can be
            determined using the methodology outlined in
            Chapter~\ref{methodology}, Section~\ref{generalconcepts}.

    \section{Analysis of a Fractal Time Series with Many Contributing Agents}
        \label{aftsma}

        \subidx{time series}{markets}
        \subidx{markets}{time series}
        \subidx{markets}{many contributing agents}
        \subidx{market}{volatility}
        \subidx{market}{variance}
        \idx{Brownian motion}
        \idx{fractional Brownian motion}
        \subidx{market}{model}
        \subidx{model}{market}
        In many market and financial instrument historical time
        series, there are many contributing agents. For example, an
        industrial market time series may be the aggregate of the
        production of many participating companies. From
        Equation~\ref{fopt2}:

            \begin{equation}
                \frac{avg}{rms^2} = \frac{f_{opt}}{rms} = K
            \end{equation}

        \noindent where $avg$ is the average of the normalized
        increments of the time series and is related to the long term
        exponential growth of the market, $(1 + avg)^n$, where n is
        the number of time units, and $rms$ is the root mean square of
        the normalized increments of the time series, and is related
        to the ``volatility,'' or variance of the market. If the
        following ``model'' of a company participating in the market
        is assumed:

        \begin{enumerate}

            \item Each company acts independently, and will receive
            cash flow from the market.

            \item Some of this cash flow will be diverted into new
            product manufacturing, development, etc., which in turn
            will go back into the market, which in turn will create
            cash flow, and so on---but there is a random element in
            this process.

            \item Analysis of various markets, (see
            Appendix~\ref{markets},) yields that they are probably a
            fractal, (fractional Brownian variety,) with a fairly
            accurate distribution of the normalized increments that
            appears to be Gaussian in nature, a range that appears to
            increase with the square root of time, and an exponential
            curvature. These are indicative of system that can be
            modeled by as a gambler's capital in an unfair coin toss
            game, or Brownian fractal.

        \end{enumerate}

        Under these assumptions, it would seem reasonable that:

        \begin{enumerate}

            \item If a market that is supplied by a single
            company. The time series for the market could be
            represented, at least statistically, as an unfair coin
            tossing game, (see the {\it tscoins}\/ program,) with each
            time unit of manufacturing going into the marketplace, the
            marketplace returning cash to the company's P \& L, which
            is distributed to the company's operations to manufacture
            more product, and so on. But there is an element of
            randomness in this process that represents the aggregate
            of customer desires and market forces---this is assumed be
            a central limit phenomena, ie., it can be represented as a
            random variable with a normal, (Gaussian,)
            distribution. Note, that like the gambler, the company's
            operations managers are continually wagering on the
            future---and each wager may, or may not prove to be a
            successful.  It is further assumed that the company will
            commit capital to enhancing its market position, (ie.,
            increase manufacturing capacity, develop new products,
            etc.,) and, as above, the decision to do so will contain
            an element of risk, and will sometimes work out, and
            sometimes not.

            \item Now consider that another company decides to
            participate in the marketplace---under the same scenario
            of as above. If everything else is equal, we would expect
            the market, eventually, to be divided equally between the
            two companies, or each company would have half the market.
            When the second company was added to the market, the first
            company's contribution to the marketplace was cut in
            half---and its root mean square value of its normalized
            increments contribution to the marketplace was also cut in
            half. The second company's contribution to the marketplace
            is the remaining one half, and its contribution to the
            root mean square value of its normalized increments is the
            same as the first company's. (The point is that the
            contributions to the marketplace add linearly, but the
            contribution of to the normalized increments of the
            marketplace add root mean square---so we would expect the
            root mean square value of the normalized increments to
            decrease when the number of participants in the
            marketplace changes from one to two---since the value of
            the normalized increments for each company is proportional
            to the contribution to its the market.)  Conceptually,
            think of it as a Gaussian noise generator. If we cut the
            root mean square value (amplitude,) of the noise generator
            in one half, and add an identical noise generator, the
            resulting noise output of both generators will be the
            square root of two, divided by two.

            \item Or in general, the root mean square value of the
            normalized increments of a marketplace time series will be
            proportional to one over the square root of the number of
            companies in the market.

        \end{enumerate}

        Figure~\ref{figmarket1} is a schematic representation of a
        company's relation to the market of the above scenario. $G(t)$
        represents the company's random element in the system, and is
        a function of a random variable with a Gaussian
        distribution. $V(n)$ is the market, and $v(n)$ is the
        companies contribution to the market in the $n$'th time
        interval. Note that the various companies in the system have
        P\&L's that are dependent on their own random element, and the
        market as a whole, which, in addition, has randomness that is
        dependent on the random element and fiscal strategy of all of
        the companies. It should be mentioned that ``models'' proposed
        in Figures~\ref{figmarket1} and~\ref{figmarket2} are not the
        only possible implementations. For example,
        Appendix~\ref{markets}, Section~\ref{\LABPRETWO:SEMIX},
        suggests another of many alternatives.

        \begin{figure}[ht]
            \begin{center}
                \begin{minipage}[t]{0.45\textwidth}
                    \epsfxsize=1.0\linewidth
                    \epsffile{graphics/xfig/market1.eps}
                    \caption[Schematic representation of a company in a
                        market]{Schematic representation of a company
                        in a market.}
                    \label{figmarket1}
                \end{minipage}
                \hfill
                \begin{minipage}[t]{0.45\textwidth}
                    \epsfxsize=1.0\linewidth
                    \epsffile{graphics/xfig/market2.eps}
                    \caption[Alternative schematic representation of a
                        company in a market]{Alternative schematic
                        representation of a company in a market.}
                    \label{figmarket2}
                \end{minipage}
            \end{center}
        \end{figure}

        Or, generalizing, letting $rms_{ind}$ and $avg_{ind}$ be the
        root mean square and average value of the normalized
        increments of the market time series for the aggregate
        industry, and $rms_{comp}$ be the root mean square of the
        normalized increments for each individual company, (which in
        this simple analysis are assumed to be equal for all companies
        participating in the marketplace,) and if it is further
        assumed that all of the companies are operating optimally,
        then, from Equation~\ref{avgrmss}:

            \begin{equation}
                avg_{comp} = rms_{comp}^2
            \end{equation}

        \noindent and summing the variances for $N$ many companies to
        obtain the average and variance of the aggregate:

            \begin{equation}
                rms_{ind}^2 = \overbrace{\frac{rms_{comp}}{N}^2 + \cdots + \frac{rms_{comp}}{N}^2}^{N many}
            \end{equation}

        \noindent or:

            \begin{equation}
                rms_{ind}^2 = \frac{n}{N^2} rms_{comp} = \frac{1}{N} rms_{comp}
            \end{equation}

        \noindent or the number of companies participating in the
        market place, $N$, is:

            \begin{equation}
                N = \frac{avg_{ind}}{rms_{ind}^2}
                \label{ncompanies}
            \end{equation}

        \subidx{programs}{tsmarket}
        \subidx{tsmarket}{program}
        \noindent under the above approximations and assumptions, and
        where $avg_{ind} = avg_{comp} + \cdots + avg_{comp}$. It
        should be noted that this derivation assumes a characteristic
        Brownian motion time series, which is only an approximation,
        and assumes a Hurst exponent of 0.5. As pointed out
        in~\cite[pp. 157]{Schroeder}, this assumption must be used
        with care, and can lead to incorrect conclusions---but with
        the simplified assumptions used, it will probably suffice for
        ``general'' analysis. A simulation program, {\it tsmarket}\/,
        which is briefly described in appendix~\ref{programs} can be
        used to investigate the validity of the assumptions.

        Note that it would be anticipated, under the above
        assumptions, that an individual company's variance would be
        larger than the variance of the industry as a whole by an
        amount $\sqrt{N}$. This means that the Shannon probability,
        $P$ for an ``average'' company in the industry, from
        Equation~\ref{avg}, would be:

            \begin{equation}
                P = \frac{\frac{avg_{ind}}{\sqrt{N} rms_{ind}} + 1}{2}
                \label{pncompanies}
            \end{equation}

        \noindent which would be smaller than the Shannon probability
        for the aggregate industry.

        \subsection{Optimization to maximize the P\&L}
            \label{ompl}

            \subidx{optimization}{P\&L}
            \subidx{P\&L}{optimization}
            Figure~\ref{figmarket1} presents another optimization
            alternative. It is generally assumed that optimizing cash
            flow in a corporation's P\&L to maximize market growth is
            the objective---and that this in turn will maximize the
            P\&L. However, that may not necessarily be the
            case. Consider the following, where, in a manner similar
            to that described schematically in Figure~\ref{figmarket1},
            the P\&L is the capital on hand at time $t$:

            \begin{itemize}

                \item Let $I(t)$ be the amount of capital at time $t$,
                ie., the value of the P\&L.

                \item Let $W(t)$ be the amount of the capital wagered
                at time $t$, ie., the amount of the the capital
                distributed through the company for manufacturing
                operations, new product development, etc., or in other
                words, what is ``wagered'' on the future.

                \item Let $V(t)$ be the value of the industrial market
                at time $t$.

            \end{itemize}

            \noindent then, letting $f$ be the fraction of the
            capital, or P\&L wagered in a fashion similar to
            Section~\ref{abmfi}:

            \begin{equation}
                W\left(t\right) = f I\left(t - 1\right)
            \end{equation}

            \noindent where $f$ is presumed not to be a function of
            time. Then:

            \begin{equation}
                I\left(t\right) = I\left(t - 1\right) + W\left(t\right) \frac{V\left(t\right) - V\left(t - 1\right)}{V\left(t - 1\right)}
            \end{equation}

            \noindent and substituting:

            \begin{equation}
                I\left(t\right) = I\left(t - 1\right) + f  I\left(t - 1\right) \frac{V\left(t\right) - V\left(t - 1\right)}{V\left(t - 1\right)}
            \end{equation}

            \noindent or:

            \begin{equation}
                \frac{I\left(t\right)}{I\left(t - 1\right)} = 1 + f \frac{V\left(t\right) - V\left(t - 1\right)}{V\left(t - 1\right)}
            \end{equation}

            If it is assumed that the stock's price time series can be
            represented as a Brownian noise fractal, then the optimum
            value of f would be, from Equation~\ref{optimumequation}:

            \begin{equation}
                f = 2P - 1
            \end{equation}

            \noindent where P is the Shannon probability of the market
            time series, found by:

            \begin{equation}
                P = \frac{\frac{avg}{rms} + 1}{2}
                \label{piopt}
            \end{equation}

            where, as in Equation~\ref{fopt3}, avg is the average, and
            rms is the root mean square, of the normalized increments
            of the market's time series, which can be calculated by:

            \begin{equation}
                \frac{V\left(t\right) - V\left(t - 1\right)}{V\left(t - 1\right)}
            \end{equation}

            \noindent for each data point in the market's time series.

            Since the market's time series already has a value rms as
            the root mean square of the normalized increments, for the
            optimal wagering strategy, the fraction should be divided
            by rms to provide a multiplier:

            \begin{equation}
                multiplier = fraction / rms
            \end{equation}

            \noindent so that:

            \begin{equation}
                \frac{I\left(t\right)}{I\left(t - 1\right)} = 1 + multiplier \frac{V\left(t\right) - V\left(t - 1\right)}{V\left(t - 1\right)}
            \end{equation}

            This is similar to the ``wagering'' strategies used in
            entropic financial instrument trading. What this means is
            that if you have capital, (ie, a portfolio,) $I(t)$, the
            fraction of $I(t)$ that should be wagered with each
            iteration of the ``game'', (ie., time unit,) would be
            twice the Shannon probability minus unity, where the
            capital, (or portfolio,) is the sum total of cash on hand,
            $C(t)$, and the current value investment in the industrial
            market, ie., the inventory. Or, to maximize the P\&L
            growth:

            \begin{equation}
                f = 2P - 1
                \label{piopt2}
            \end{equation}

            \noindent where the Shannon probability, $P$ is calculated
            from Equation~\ref{piopt}.

            This is interesting, at least according to the simplified
            model used in this section, because maximizing market
            segment growth and maximizing the P\&L will be mutually
            exclusive except when the market consists of exactly one
            company.

            Figure~\ref{figmarket2} is a schematic representation of a
            company's relation to the market in this scenario.

    \section{Statistical Estimation of Required Data Set Size}
        \label{serdss}

        Consider the following formula for determination of the
        Shannon Probability, $P$, of an equity market time series,
        using the average and root mean square of the normalized
        increments, $avg$, and, $rms$, respectively, by rearranging
        Equation~\ref{fopt3}:

            \begin{equation}
                P = \frac{\frac{avg}{rms} + 1}{2}
            \end{equation}

        \noindent which is useful in the determination of the optimal
        fraction of capital, $f$, to invest in a stock, from
        Equation~\ref{optimumequation}:

            \begin{equation}
               f = 2P - 1
            \end{equation}

        The objective is to estimate how large the data set has to be
        for determining $P$ to a given accuracy, possibly using
        statistical estimates of how many data points are required for
        a given confidence level that the error is less than a
        specific value.

        Suppose we have a confidence level, $0 < c < 1$, that a value
        is within, plus or minus, an error level, $e$. What this
        means, for example if $c = 0.9$, and $e = 0.1$, is that for
        $90\%$ of the cases, the value will be within the limits of
        $\pm e$, or, $5\%$ of the time, on the average, it will be
        less than $-e$, and $5\%$ of the time more than $+e$.

        The error level for $avg$, $e_{avg}$,for a given confidence
        level, will be:

            \begin{equation}
                e_{avg} = k \frac{rms}{\sqrt{n}}
            \end{equation}

        \noindent where $n$ is the number of records in the data set,
        and $k$ is a function involving a normal distribution. The
        error level for $rms$, for the same given confidence level,
        will be:

            \begin{equation}
                e_{rms} = k \frac{rms}{\sqrt{2n}}
            \end{equation}

        \noindent where $k$ is identical in both cases. Also, the
        number of records required for a given error level would be:

            \begin{equation}
                n_{avg} = \left(\frac{(rms \cdot k)}{e_{rms}}\right)^2
            \end{equation}

        \noindent and

            \begin{equation}
                n_{rms} = \frac{1}{2} \left(\frac{(rms \cdot k)}{e_{rms}}\right)^2
            \end{equation}

        \noindent where $k$ is the same as above.

        For equity market indices, a typical value for $rms$ would be
        $0.01$, and $0.0003$ for $avg$. This is probably typical for
        many stocks, however, high gain stocks, in a ``bull'' market
        can have an $rms$ of $0.04$, and an $avg$ of $0.005$.

        The value of $k$ can be determined from standard statistical
        tables, as shown in table~\ref{CLVSSL}, where $k = $ sigma
        level, for a confidence level, $c$.

        \begin{small}
            \begin{table}[ht]
                \begin{center}
                    \caption[Confidence Level vs. $\sigma$ Level]{Confidence Level vs. $\sigma$ Level.}
                    \begin{tabular}{|l|l|} \hline
                        Confidence Level, $c$ & $\sigma$ level \\ \hline
                        (\%) & \hspace{0.01in} \\ \hline
                        50      &    0.67 \\
                        68.27   &    1.00 \\
                        80      &    1.28 \\
                        90      &    1.64 \\
                        95      &    1.96 \\
                        95.45   &    2.00 \\
                        99      &    2.58 \\
                        99.73   &    3.00 \\ \hline
                    \end{tabular}
                    \label{CLVSSL}
                \end{center}
            \end{table}
        \end{small}

        Note that for a given confidence level:

        \begin{eqnarray}
        \frac{avg}{rms} & = & \frac{avg \pm k \frac{rms}{\sqrt{n}}}{rms \pm k \frac{rms}{\sqrt{2n}}} \\
                        & = & \frac{\frac{avg}{rms} \pm k \frac{1}{\sqrt{n}}}{1 \pm k \frac{1}{4\sqrt{n}}}
        \end{eqnarray}

        Now, consider the specific example of $avg$ and $rms$ for an
        exponential function. In this specific case, $avg = rms$, and
        $\frac{avg}{rms} = 1$. Since $k$ is assumed to be a function
        of a normally distributed random variable, the error in the
        ratio $\frac{avg}{rms}$ as a function of the data set size,
        {n}, can be found by superposition, and adding the
        contributing error values as a function of $n$ for both $rms$
        and $avg$ root mean square, or:

            \begin{equation}
                \sqrt{1^2 + \left(\frac{1}{4}\right)^2} = 1.030776406
            \end{equation}

        \noindent or:

            \begin{equation}
                \frac{avg}{rms} \sim \frac{avg}{rms} \pm 1.03 \frac{1}{\sqrt{n}} k \sim \frac{avg}{rms} \pm \frac{1}{\sqrt{n}} k
            \end{equation}

        \noindent where $k$ is determined from the table, above. In
        this specific case, where $avg = rms$:

            \begin{equation}
                \frac{avg}{rms} \sim \frac{avg}{rms} \left(1 \pm \frac{1}{\sqrt{n}} k\right)
            \end{equation}

        An interpretation of what this means is that, given a data set
        size, $n$, and a confidence level of, say $90\%$, then $90\%$
        of the time, our measurements of $\frac{avg}{rms}$, would fall
        within an error level of $\pm 1.64 \frac{1}{\sqrt{n}}$, ie.,
        $5\%$ of the time it would be greater than the error value,
        and $5\%$ of the time, it would be lower than the error
        value. In general, the concern is with the lower error value
        since from the equation:

            \begin{equation}
                P = \frac{\frac{avg}{rms} + 1}{2}
            \end{equation}

        \noindent (at least in this specific case where $avg = rms$,)
        that a $90\%$ confidence level would imply that there is a
        $5\%$ chance of the real value $\frac{avg}{rms}$ being zero is
        where:

            \begin{equation}
                \frac{k}{\sqrt{n}} = 1
            \end{equation}

        \noindent or:

            \begin{equation}
                \frac{1.64}{\sqrt{n}} = 1
            \end{equation}

        \noindent or $n = 2.6896 \sim 3$.

        What this means is that, if we repeat the experiment of
        finding $3$ records in a row that have $rms = avg$, with
        neither equal to zero, many times, that we would loose money
        in $5\%$ of the cases, making the measured Shannon
        probability, $P$, unity, and the estimated Shannon
        probability, $0.95$, eg., we should consider the Shannon
        probability as $0.95$ in this specific case---ie., it would be
        ill advised to invest all of the capital in such a scenario,
        since, sooner or later, all of the capital would be lost, (on
        average, by the 20'th game.)

        This implies a simple methodology. Measure $avg$ and $rms$,
        and compute the Shannon probability. Decease that probability
        by a factor---ie., one minus the confidence level, divided by
        two---that the wager could be a loosing proposition, based on
        the estimates that $avg$ could be zero, (which is a function
        of the confidence level, and the number of records in the data
        set.) This, conceivably, could provide a quantitative estimate
        on the number of records required in a data set.

        Note that if $\frac{avg}{rms}$ is measured at $0.9$, then:

            \begin{equation}
                \frac{1.64}{\sqrt{n}} = 0.9
            \end{equation}

        \noindent for the same confidence level of $0.9$, or

            \begin{equation}
                n = 3.32
            \end{equation}

        \noindent and:

        \begin{small}
            \begin{table}[ht]
                \begin{center}
                    \caption[Shannon Probability vs. Data Set Size]{Shannon Probability vs. Data Set Size.}
                    \begin{tabular}{|r|r|r|r|} \hline
                        $\frac{avg}{rms}$ & $n$ & $P_{measured}$ & $P$\\ \hline
                        1.0  &   2.7  & 1.00  &  0.95 \\
                        0.9  &   3.3  & 0.95  &  0.90 \\
                        0.8  &   4.2  & 0.90  &  0.86 \\
                        0.7  &   5.5  & 0.85  &  0.81 \\
                        0.6  &   7.5  & 0.80  &  0.76 \\
                        0.5  &  10.8  & 0.75  &  0.71 \\
                        0.4  &  16.8  & 0.70  &  0.67 \\
                        0.3  &  29.9  & 0.65  &  0.62 \\
                        0.2  &  67.2  & 0.60  &  0.57 \\
                        0.1  & 268.9  & 0.55  &  0.52 \\
                        0.05 & 1075.8 & 0.53  &  0.50 \\ \hline
                    \end{tabular}
                    \label{SPVSDSS}
                \end{center}
            \end{table}
        \end{small}

        for the same confidence level $0.9$. What the table means is
        that if you have a stock price time series of $67$ records,
        then the minimum measured Shannon probability must be at least
        $0.6$---and the wagering strategy should use the Shannon
        probability of $0.57$---and the minimum number of records used
        to measure $avg$ and $rms$ is $67$. Additionally, a stock time
        series with a Shannon probability of $0.53$ should be measured
        using not less than $1076$ records, and no wager should be
        made, unless the measurements involve substantially more than
        $1076$ records. In general, the Shannon probability of almost
        all stock time series fall, inclusively, in this range. $67$
        business days is, approximately, $13.4$ weeks, or little more
        than a calendar quarter.  $1076$ business days is slightly
        longer than four calendar years.

        Note that~\cite[pp. 83]{Peters:CAOITCM}
        referencing~\cite[pp. 179]{Feder}, the claim is made that 2500
        records is the minimum size of the data set for using fractal
        analytical methodologies. Note that a data set of this size
        would have, with an $\frac{avg}{rms}$ of $0.5$---which is
        ``typical'' for a stock time series, a Shannon probability
        error level that is approximately $1\%$, since it lies between
        $2$ and $3$ sigma, and $c$ would be approximately $0.99$. This
        would seem to be consistent with the empirical arguments of
        both Peters and Feder, although Peters implies that less could
        be used if the system being analyzed is ``chaotic'' in nature,
        and one ``cycle'' of the system's, apparently, ``strange
        attractor'' is less than $2500$ time units. This analysis
        would seem to be consistent with the observations of these
        authors, provided that it is a requirement that the measured
        Shannon probability be used to calculate the optimum wager
        fraction.

        What this analysis would tend to suggest is that, although
        Feder's and Peter's arguments seem to be confirmed, that there
        may, also, be other viable solutions for data sets, (or
        fragments thereof,) that are very much smaller, provided that
        the measured Shannon probability of the data set, or segment,
        is sufficiently large---for example, a stock that has a time
        series fragment that has $5$ out of $6$ upward movements may
        prove to be a viable investment opportunity at a measured
        Shannon probability that is greater than $0.85$, ($\frac{5}{6}
        =$ a Shannon probability of $0.833 \sim 0.85$,) if played at a
        Shannon probability as high as $0.8$, but no higher.

        For example, using a Shannon probability, $P$, of $0.51$ for
        the {\it tscoins}\/ and {\it tsfraction}\/ programs, to
        provide an input fractal time series for the {\it tsstatest}\/
        program, and iterating, indicates that for a standard
        deviation of $0.020000$, with a confidence level of $0.960784$
        that the error did not exceed $0.020000$, $3$ samples would be
        required.

        Since the Shannon probability is calculated directly from the
        standard deviation, (ie., $rms$ = root mean square of the
        normalized increments,) the maximum error can be calculated:

        \begin{equation}
            \frac{0.5}{0.51} = 0.980392157
        \end{equation}

        which means that a confidence level of $0.960784314$ that the
        error level in the standard deviation is less than $0.02$
        because standard deviation $= rms = 0.02 - 0.02 = 0$, which
        would correspond to a Shannon probability, $P$, of $0.5$, and
        since half the errors outside the range of $0.02$ would be
        negative, (and the other half positive,) the confidence level
        required would be $1 - ((1 - 0.980392157) \cdot 2)$.

        What this means is that $((1 - 0.960784314) / 2) \cdot 100$
        percent of the time, the actual $rms$ value will be
        sufficiently small to make $P$ equal to, or less than
        $0.5$. This means that $P$ must be decreased by $1.960784300$
        percent. The reasoning is that after many iterations, the
        measured $P$ would be too small by $1.90784300$\% of the time,
        on average, making the measured $P$, over all of the
        iterations, $0.5$.

        This suggests a dynamic rule: do not wager unless the Shannon
        probability, $P$, is strictly greater than $0.51$, as measured
        on strictly more than $3$ time units. Interestingly, the Hurst
        Coefficient, as measured by the {\it tshurst}\/ program, graph
        of a random walk, Brownian motion, or fractional Brownian
        motion fractals indicates that there is significant near term
        correlations for $4$ or less time units. This suggests a
        dynamic trading methodology for equities.

        Similar reasoning would indicate that using a value of $P =
        0.6$ for the {\it tscoins}\/ and {\it tsfraction}\/ programs
        to provide input to the {\it tsstatest}\/ program with a
        confidence level of $0.8$, and an error of $0.12$, (ie., 10\%
        of the time the value of $P$ would be less than $0.9 \cdot 0.6
        = 0.54$, where $0.2 - 0.12 = 0.08$, and $0.54 = \frac{0.08 +
        1}{2}$,) would require a minimum of $3$ records. The fraction
        of capital wagered should be $2 \cdot 0.54 - 1 = 0.08$.

    \section{Non-linear extensions}
        \label{nlextend}
        \subidx{market}{non-linearity}
        \subidx{non-linearity}{market}
        \subidx{logistic}{function}
        \subidx{cumulative returns}{increments}
        \subidx{increments}{cumulative returns}
        \subidx{Brownian motion}{fixed increments}
        \idx{random process}

        \subidx{cumulative returns}{increments}
        \subidx{increments}{cumulative returns}
        \subidx{Brownian motion}{fixed increments}
        \idx{random process}
        \subidx{Shannon}{probability}
        \subidx{probability}{Shannon}
        \subidx{programs}{tscoin}
        \subidx{tscoin}{program}
        \subidx{programs}{tscoins}
        \subidx{tscoins}{program}
        \subidx{programs}{tsdlogistic}
        \subidx{tsdlogistic}{program}
        \subidx{programs}{tslsq}
        \subidx{tslsq}{program}
        As mentioned in Section~\ref{abmfi}, consider a time series,
        similar to Figure~\ref{WSFOCS}, but with a true random
        distribution. As in Section~\ref{areturns}, the Shannon
        probability, $P$, is $P \leq 0 \leq 1$.  The fraction of the
        cumulative returns, $R$, that is wagered with each iteration
        of the game is $f$, where $0 \leq f \leq 1$, and $F$ is an
        independent random process, from Equation~\ref{Fequation}:

        \begin{equation}
             F = \left\{ \begin{array}{l l}
                                +1, & \mbox{with a probability of P}\\
                                -1, & \mbox{with a probability of 1 - P}
                         \end{array}
                 \right.
        \end{equation}

        Then in general, the returns after $N$ many iterations would
        be, from Equation~\ref{requation}:

        \begin{eqnarray}
            R_{N} & = & R_{0} \cdot \left(\left(1 + F_{1} \cdot f_{1}\right) \cdot \left(1 + F_{2} \cdot f_{2}\right) \cdot \left(1 + F_{3} \cdot f_{3}\right) \cdot \hspace*{0.1in} \cdots \hspace*{0.1in}\right.\nonumber\\
                  &   & \cdot \left(1 + F_{n-1} \cdot f_{n-1}\right) \cdot \left(1 + F_{n} \cdot f_{n}\right) \cdot \left(1 + F_{n+1} \cdot f_{n+1}\right) \cdot \hspace*{0.1in} \cdots \hspace*{0.1in}\nonumber\\
                  &   & \cdot \left.\left(1 + F_{N-1} \cdot f_{N-1}\right) \cdot \left(1 + F_{N} \cdot f_{N}\right)\right)
        \end{eqnarray}

        \idx{time series}
        \idx{cumulative sum}
        \idx{random process}
        \subidx{incremental returns}{calculation}
        \noindent which, after performing a process similar to the
        operations described in Chapter~\ref{methodology},
        Section~\ref{generalconcepts}, would result in
        Equation~\ref{iteration1}:

        \begin{equation}
            \frac{R_{n + 1} - R_n}{R_n} = F_n \cdot f_n
        \end{equation}

        \noindent which is the general formula for deriving the
        cumulative returns of the next iteration from the current
        iteration. Rearranging:

        \begin{equation}
            R_{n + 1} = R_n + R_n \cdot F_n \cdot f_n
            \label{bmformula2}
        \end{equation}

        \noindent and adding a constant, $n$, which represents the
        magnitude of the second order part of the time series:

        \begin{equation}
            R_{n + 1} = R_n + R_n \cdot F_n \cdot f_n + n \cdot R_n^2
        \end{equation}

        \noindent or:

        \begin{equation}
            R_{n + 1} = R_n \left(1 + F_n \cdot f_n + n \cdot R_n\right)
        \end{equation}

        \noindent which is the logistic equation, with Brownian noise,
        ie., Equations~\ref{bmformula1} and~\ref{bmformula2} are first
        order approximations to logistic equation.

        Letting $1 + \overline{F_n} \cdot \overline{f_n}$ be $a$ where
        $\overline{F_n}$ and $\overline{f_n}$ are the average of the
        independent random process and wager fraction, respectively:

        \begin{equation}
            R_{n + 1} = R_n \left(a + b \cdot R_n\right)
            \label{logistic1}
        \end{equation}

        \noindent where $n = b$ for compatability with the {\it
        tsdlogistic}\/ program. Equation~\ref{logistic1} is identical
        to the algorithm used in the program {\it tsdlogistic}\/ which
        implements the discreet time logistic function. The programs
        {\it tscoin}\/ and {\it tscoins}\/ also have options to
        implement the logistic function. A brief description of the
        programs appears in Appendix~\ref{programs}.

        Note that the logistic function has been used to model market
        dynamics in the literature,~\cite[pp. 55, 124, 188,
        239-274]{Modis}. Interestingly, the sign of the non-linear
        term is insignificant, provided the Shannon probability, $P$
        is sufficiently close to $1 / 2$, and $n = b$ is sufficiently
        small. The reason for this is that the independent random
        process, $F_n$, has values of $\pm 1$, and dominates the
        equation.

        Continuing from Equation~\ref{logistic1}, by subtracting $R_n$
        from both sides:

        \begin{equation}
            R_{n + 1} - R_{n} = R_n \left(a + b \cdot R_n\right) - R_{n}
        \end{equation}

        \noindent and dividing both sides by $R_{n}$:

        \begin{equation}
            \frac{R_{n + 1} - R_{n}}{R_{n}} = a + b \cdot R_n - 1
        \end{equation}

        \noindent which is the formula for the normalized increments
        of the discreet time logistic function. Note that the right
        side of the formula is linear, ie., it is possible to derive
        the constants of the discreet time logistic function by using
        the program {\it tsfraction}\/ to make a time series of the
        normalized increments of a time series, and, possibly, using
        the program {\it tslsq}\/ with the -p option to provide the
        formula for the least squares linear fit. The least squares
        fit is of the form:

        \begin{equation}
            f(t) = P - Qt
        \end{equation}

        \noindent where:

        \begin{equation}
            a = P + 1
        \end{equation}

        \noindent and:

        \begin{equation}
            b = Q
        \end{equation}

        \noindent could be used as the arguments to the {\it
        tsdlogistic}\/ program.

        The presumption used in Equations~\ref{bmformula1}
        and~\ref{bmformula2} is that the non-linear term of the
        logistic equation is insignificant when studying new growth
        industry markets---ie., it can be ignored when concern is the
        left side of the logistic equation. This is inappropriate in
        the study of long term market phenomena, such as saturation,
        or limitation of resources.

        Equity/asset market pricing may exhibit logistical
        non-linearities~\cite[pp. 156]{Modis}.

        More recently,~\cite{Arthur:CTIRALIBHE} has proposed that
        industrial markets exhibit economic non-linear increasing
        returns phenomena. Although specifically addressing market
        share of competing
        technologies,~\cite[pp. 6]{Arthur:CTIRALIBHE} offers the
        argument that such scenarios will have a time series that is a
        random walk, ie., fixed increment Brownian motion fractal, in
        the case of diminishing and constant returns. Further, it is
        shown,~\cite[pp. 16]{Arthur:CTIRALIBHE} that the case of
        increasing returns also exhibits random walk
        characteristics---at least where the agent's expectations are
        that that will be the case. See~\cite{Arthur:IRABR} for
        arguments concerning the nature of expectations in a
        multi-agent economic environment. These environments are not
        ergodic, (ie., not mean reverting,) and exhibit
        unpredictability, and are not necessarily path
        efficient~\cite[pp. 7]{Arthur:CTIRALIBHE}. However, if it is
        assumed that an industrial market is a sum of the many random
        walk time series of the many agent's market share, and it is
        assumed that the sum of many random walk time series is a
        random walk, then the market time series could, possibly, be
        analyzed by the methodology proposed in this chapter. However,
        in the case of increasing returns, it is doubtful that the
        constants in the time series could be derived with adequate
        precision for analysis of market share, but, perhaps, the
        market itself may be analyzed. As a case in point,
        in~\cite[pp. 6]{Arthur:CIEAFM} argues that individual agents
        operating in the equity markets exhibit behavior that is
        derived from {\it inductive}\/ reasoning, but the market
        itself exhibits random walk characteristics. Quoting
        from~\cite[Abstract]{Arthur:CIEAFM}:

        \begin{quote}

            Actions taken by economic decision makers are typically
            predicated upon hypotheses or predictions about future
            states of a world that is itself in part the consequence
            of these hypotheses or predictions. When we attempt to
            model how such predictions might be generated we become
            stymied: the predictions some economic agents might form
            depend on the predictions they believe others might form;
            and the predictions these might form depend upon the
            predictions {\it they}\/ believe the original group might
            form. Predictions or expectations can then become
            self-referential and deductively indeterminate. This
            indeterminacy pervades economics and game theory.

        \end{quote}

        The three papers,~\cite{Arthur:CIEAFM},~\cite{Arthur:IRABR},
        and~\cite{Arthur:CTIRALIBHE} present compelling arguments for
        a new economic paradigm. In practical multi-agent economic
        scenarios, some models of the economy will seem to be correct,
        for a while, only to be discarded
        later. In~\cite{Arthur:CIEAFM} the argument is presented that
        some models will be stable, ie., if it is believed that a
        model works, the economy will operate according to that model.
        In~\cite{Arthur:CTIRALIBHE} it is shown that the random walk
        hypothesis is one such model---at least in the case of
        competing technologies. It is not clear whether all
        multi-agent systems exhibit random walk characteristics, but
        it would seem reasonable to assume so,
        and~\cite[[pp. 9]{Arthur:CIEAFM} presents simulation results
        that tend to confirm the assumption.  If that is the case,
        then the random walk analytical methodology presented in this
        chapter could be applicable.

    \section{Generalization}
        \label{GEN}
        \idx{random walk}
        \idx{Brownian motion}
        \idx{fractal}

        To reiterate the general concepts presented so far, a fractal
        is a cumulative sum of a random process. In the literature, it
        is sometimes called a Brownian motion, or ``random walk,''
        process since, at any time, the next element in the process
        time series is a random increment added to the current element
        in the time series. From~\cite[pp. 164]{Feder}:

        \begin{quote}

            We emphasize that in Brownian motion it is not the
            position of the particle at one time that is independent
            of the position of the particle at another; it is the
            displacement of that particle in one time interval that is
            independent of the displacement of the particle during
            another time interval.

        \end{quote}

        This is a subtile concept. Note that the term ``cumulative
        sum'' really means that in any time interval, the position of
        the particle is dependent only on the position of the particle
        in the previous time interval, and a random displacement. But
        the position in the previous time interval was dependent only
        on the position in the time interval prior to that, and
        another displacement, and so on, ie., to make a fractal
        process, we need only know where the particle is at the
        current time, and add a displacement to it, for each interval
        in time. The subtilty is that we need only know where the
        particle is, and not where it has been to calculate where it
        will be.

        This section will use this concept, and expand the concept of
        the random process to include game-theoretic issues by
        introducing iterated two player mixed strategy games, then a
        simple self-referencing game where no formal strategy can
        exist, and finally multi-player games, where the random
        process is generated by the inconsistency of the
        self-referential, inductive reasoning among the players. In
        all cases, the iterated time series of such games will be
        argued to be fractal, in nature.

        \subsection{The Game of Mora}
            \label{MORA}
            \subidx{zero-sum}{game}
            \subidx{game}{zero-sum}
            \subidx{game}{of strategy}
            \subidx{game}{Mora}
            \subidx{Mora}{game}
            \idx{game theory}

            A simple coin tossing game was analyzed in
            Section~\ref{crp}. In this section, those concepts will be
            expanded to include games of strategy. The game of Mora,
            following~\cite[pp. 434]{Bronowski}, is very old, (being
            mentioned in Sanskrit,) and is played between two players
            and, in its simplest version, goes as follows. The two
            players move simultaneously. Each shows either one or two
            fingers, and at the same time guesses whether the other
            player is showing one or two fingers. If both players
            guess right, or both guess wrong, no money changes
            hands. However, if only one player guesses right, the
            player wins from the other as many coins as the two
            players together showed fingers. The possible outcomes of
            any game are as follows if your call is right, and your
            opponent's wrong:

            \begin{enumerate}

                \item Guessing your opponent will show $1$ finger and
                showing $1$ finger you will win $2$ coins.

                \item Guessing your opponent will show $2$ fingers and
                showing $1$ finger you will win $3$ coins.

                \item Guessing your opponent will show $1$ finger and
                showing $2$ fingers you will win $3$ coins.

                \item Guessing your opponent will show $2$ fingers and
                showing $2$ fingers you will win $4$ coins.

            \end{enumerate}

            The game is fair, but a player who knows the right
            strategy will, with average luck, win against one who does
            not. The right strategy is to ignore courses $1$) and
            $4$), and to play courses $2$) and $3$) in the ratio of
            $7$ to $5$, ie., the right strategy is, in any 12
            iterations of the game, to play course $2$) on the average
            $7$ times, and course $3$) on the average 5
            times. Obviously, your opponent must not know which course
            you are going to play, so the two courses must be
            intermixed randomly.

            The game is zero-sum, meaning that what one player wins,
            the other looses. The mathematical method by which the
            best strategy was found is called game theory. However, it
            is not hard to verify that the strategy is effective by
            calculating what happens when your opponent counters by
            using course $1$), $2$), $3$), or $4$), above. Namely, if
            your opponent chooses course:

            \begin{enumerate}

                \item Course $1$), will, on the average, win $7$ times
                out of $12$, and will win only $2$ coins for each win;
                whereas losses will occur $5$ times out of $12$, and
                those losses will be $3$ coins for each loss---making
                an average loss of $1$ coin in $12$ iterations of the
                game.

                \item Course $2$), will have no coins change hands,
                since either both players are right, or both are
                wrong.

                \item Course $3$), will have no coins change hands,
                since either both players are right, or both are
                wrong.

                \item Course $4$), will, on the average, win $5$ times
                out of $12$, and will win $4$ coins for each win;
                whereas losses will be occur $7$ times out of $12$,
                and those losses will be $3$ coins for each
                loss---making an average loss of $1$ coin in $12$
                iterations of the game.

            \end{enumerate}

            As in Section~\ref{crp}, the objective of each player is
            to maximize the number of coins won over many iterations
            of the game, ie., to maximize the cumulative returns of
            the game. Note that each player's capital, will fluctuate,
            depending on the outcome of a particular iteration-and
            that fluctuation will be random, and either $0$, $2$, $3$,
            or $4$ coins. We would expect that the time series
            representing the fluctuations in a player's capital to be
            a random walk, which could be represented by a formula
            similar to Equation~\ref{iteration}.

            It is often convenient to represent the game as a table,
            which lists all the possibilities of the courses for both
            players, and how much the each player would win or loose
            for each course, ie., a {\it payoff matrix}, where one
            player's alternatives are represented by the columns in
            Table~ref{MORA:POM}, and the other player's alternatives
            are represented by the rows. The payoff to a particular
            game solution is the intersection of the row and column of
            the course played by the two players.

            \begin{small}
                \begin{table}[ht]
                    \begin{center}
                        \caption[The Game of Mora, Payoff Matrix]
                            {The Game of Mora, Payoff Matrix.}
                        \begin{tabular}{|c||c|c|c|c|} \hline
                            Finger, Guess & 1,1 & 1,2 & 2,1 & 2,2\\ \hline\hline
                            1,1           &   0 &   2 &  -3 &   0\\ \hline
                            1,2           &  -2 &   0 &   0 &   3\\ \hline
                            2,1           &   3 &   0 &   0 &  -4\\ \hline
                            2,2           &   0 &  -3 &   4 &   0\\ \hline
                        \end{tabular}
                        \label{MORA:POM}
                    \end{center}
                \end{table}
            \end{small}

            \idx{linear algebra}
            \idx{operations research}
            \subidx{simplex}{algorithm}
            \subidx{algorithm}{simplex}
            The optimal strategy for a game as simple as Mora can be
            derived by game-theoretic methodology\footnote{These
            methodologies are often called {\it operations
            research}. The algorithm of choice used to derive the
            optimal game play seems to be the {\it simplex
            algorithm}---at least for games with a small payoff
            matrix. The simplex algorithm is one of a class of
            algorithms that are implemented using {\it linear
            algebra}.}~\cite[pp. 56]{Luce},~\cite[pp. 441]{Hillier},~\cite[pp. 419]{Dorfman},~\cite[pp. 209]{Saaty},~\cite[pp. 127]{Singh},~\cite[pp. 435]{Strang},~\cite[pp. 258]{Nering},~\cite[pp. 67]{Karloff},~\cite[pp. 105]{Kaplan},
            but in many games of interest, the rules are too
            complicated, and may even change over time\footnote{In the
            game of Mora, the optimal strategy does not depend on the
            strategy of the opposing player. In more sophisticated
            games, this is not true.}.  In these scenarios, the
            strategy can be derived empirically, over time, using {\it
            adaptive control}\/ computational methodologies. For
            example, if the strategy of Mora was not known, the
            optimal ratio of courses could be determined by varying
            the ratio, and observing the effect on the cumulative
            reserves over many iterations of the game. Note that such
            a methodology can be problematical since your opponent may
            be doing the same thing. An example of such a scenario is
            presented in the next section.

        \subsection{Prisoner's Dilemma}
            \label{PD}
            \subidx{nonzero-sum}{game}
            \subidx{game}{nonzero-sum}
            \subidx{game}{of strategy}
            \subidx{game}{Prisoner's Dilemma}
            \subidx{Prisoner's Dilemma}{game}
            \idx{game theory}
            \idx{prisoner's dilemma}

            A simple mixed strategy zero-sum game was analyzed in the
            previous section. In the game of Mora, the optimal
            strategy does not depend on how your opponent plays the
            game over time. The prisoner's dilemma game is
            qualitatively different. It is also one of the most
            commonly studied scenarios in game theory\footnote{The
            prisoner's dilemma has generated much interest since it is
            a game that is simple to understand, and has all of the
            intrigue and strategy of many human social dilemmas---for
            example, John Von Neumann, the inventor of game theory,
            once said that the reason we do not find intelligent
            beings in the universe is that they probably existed, but
            did not solve the prisoner's dilemma problem and destroyed
            their self. The prisoner's dilemma has been used to model
            such scenarios as the nuclear arms race, battle of the
            sexes,
            etc.}~\cite[pp. 94]{Luce},~\cite{Poundstone:PD},~\cite[pp. 262]{Waldrop},~\cite[pp. 262]{Casti:C},~\cite[pp. 295]{Casti:AR},~\cite[pp. 199]{Casti:PL}~\cite[pp. 297]{Casti:SFC},~\cite[pp. 439]{Strang},
            and~\cite[pp. 155]{Kaplan}~\cite[pp. 170]{Davis}.

            The rules of the game are simple. There are two players,
            and each player has only two choices for each iteration of
            the ``game,'' and those choices are to chose either ``A''
            or ``B.'' If both players pick ``A,'' then each wins 3
            coins. If one picks ``A,'' and the other ``B,'' then the
            player picking ``B'' wins 6 coins, and the other player
            gets nothing. However, if both players pick ``B,'' then
            both win 1 coin.

            The payoff matrix for the prisoner's dilemma game is shown
            in Table~\ref{PD:POM}, where, as before, one player's
            alternatives are represented by the columns, the other
            player's alternatives are represented by the rows. The
            payoff to a particular game solution is the intersection
            of the row and column of the course played by the two
            players.

            \begin{small}
                \begin{table}[ht]
                    \begin{center}
                        \caption[The Prisoner's Dilemma Game, Payoff Matrix]
                            {The Prisoner's Dilemma Game, Payoff Matrix.}
                        \begin{tabular}{|c||c|c|} \hline
                            Choice &   A &   B\\ \hline\hline
                            A      & 3,3 & 6,0\\ \hline
                            B      & 6,0 & 1,1\\ \hline
                        \end{tabular}
                        \label{PD:POM}
                    \end{center}
                \end{table}
            \end{small}

            The prisoner's dilemma is not a zero-sum game---neither
            player can ever loose any money. So there is an incentive
            to always play. The choice ``A'' is known as a
            ``cooperation strategy,'' and the choice ``B'' is known as
            the ``defection strategy'' for each player. It is a very
            subtile and devious game. Here is why, and the logic you
            would go through. Just before you played an iteration of
            the game, you would think:

            \begin{enumerate}

                \item If you choose ``A,'' there are two possible
                scenarios:

                \begin{enumerate}

                    \item If your opponent chooses ``A,'' you would
                    get 3 coins, and your opponent would get 3 coins.

                    \item If your opponent chooses ``B,'' you would
                    get 1 coin, and your opponent would get 6 coins.

                \end{enumerate}

            \item If you choose ``B,'' there are also two possible
            scenarios:

                \begin{enumerate}

                    \item If your opponent chooses ``A,'' you would get
                    6 coins, and your opponent would get nothing.

                    \item If your opponent chooses ``B,'' you would get
                    one coin, and your opponent would get one coin.

                \end{enumerate}

            \end{enumerate}

            Note that by choosing ``A,'' the best you could do is to
            win 3 coins, and the worst is to win nothing. But, by
            choosing ``B,'' the best you could make is 6 coins, and
            the worst is one coin. It would appear, at least
            initially, that ``B,'' is the best choice, irregardless of
            what you opponent does.

            But now the logic of the game gets subtile. Your opponent
            will determine the same strategy, and will never play
            ``A.''  So you both make one coin with every iteration of
            the the game.  But you could make 3 coins---if you
            cooperated, by both playing ``A.''  But if you do that,
            there is an incentive for either player to play ``B,'' if
            he knows the other player is going to play ``A,'' and thus
            make 6 coins. And we are right back where we
            started. Indeed, a very diabolical game.

            It is an important concept that you will be basing your
            decision whether to cooperate, ie., choose ``A,'' or
            defect, ie., choose ``B,'' based on how you think your
            opponent is going to play. But your opponent's decision
            will be based on consideration of how you are going to
            play. Which, in turn, will be based on how you think your
            opponent will play, ad infinitum. It is circular logic, or
            more correctly, the game strategy is {\it
            self-referential}~\cite[pp. 17,
            pp. 465]{Hofstadter}~\cite[pp. 361,
            pp. 379]{Casti:SFC},~\cite[pp. 335]{Casti:PL},~\cite[pp. 356]{Casti:AR},~\cite[pp. 84,
            pp. 103,
            pp. 215]{Hodges},~\cite[pp. 101]{Penrose}\footnote{The
            Penrose citation, referencing Russell's paradox, is a very
            good example of logical contradiction in a
            self-referential system. Consider a library of books. The
            librarian notes that some books in the library contain
            their titles, and some do not, and wants to add two index
            books to the library, labeled ``A'' and ``B,''
            respectively; the ``A'' book will contain the list of all
            of the titles of books in the library that contain their
            titles; and the ``B'' book will contain the list of all of
            the titles of the books in the library that do not contain
            their titles. Now, clearly, all book titles will go into
            either the ``A'' book, or the ``B'' book, respectively,
            depending on whether it contains its title, or not.  Now,
            consider in which book, the ``A'' book or the ``B'' book,
            the title of the ``B'' book is going to be placed---no
            matter in which book the title is placed, it will be
            contradictory with the rules. And, if you leave it out,
            the two books will be incomplete.)}.

            This presents a problem in defining an optimal strategy
            for playing the game of the iterated prisoner's dilemma
            since no ``theory of operation'' of a self-referential
            system can ever be proposed that will be both consistent
            and complete, ie., whatever theory is proposed, it will
            not cover all circumstances, or provide inconsistent
            results in other circumstances~\cite[pp. 465,
            pp. 471]{Hofstadter},~\cite{Arthur:CIEAFM}. The best way
            to play the game is deductively indeterminate. This
            indeterminacy pervades economics and game
            theory~\cite[Abstract]{Arthur:CIEAFM}.

            However, just because such problems do not have
            axiomatized, provably robust solutions does not mean that
            good strategies do not exist. For example, the {\it
            tit-for-tat}\/ strategy~\cite[pp. 239]{Poundstone:PD} has
            been shown to be a very effective. The objective is to
            avoid letting the game degenerate into both players
            playing defection strategies. It is very simple, and
            consists of cooperating, ie., playing ``A,'' on the first
            iteration of the game, and then do whatever the other
            player did on the previous iteration\footnote{The
            tit-for-tat strategy sounds like a human social strategy
            between two people---as well it should. It is known to
            work well with human
            subjects~\cite[pp. 239]{Poundstone:PD}. It is also strict
            military dogma, and has formed the strategy of arbitration
            of the complexity of power in many marriages.}. Note that
            it is a ``nice'' strategy, (in the jargon of game theory,
            a ``nice'' strategy is one that never defects first.) It
            is also a ``provocable'' strategy---it defects in response
            to a defection by the opponent. It is also a ``forgiving''
            strategy---the opponent can implicitly ``learn'' that
            there is an incentive for cooperating after a
            defection\footnote{Tit-for-tat is kind of a ``do unto
            others as you would have them do unto you---or else,''
            strategy. The tit-for-tat strategy in human relationships
            is very old. Another ancient proverb illustrating
            tit-for-tat is ``an eye for an eye, a tooth for a
            tooth.''}. An important concept of the tit-for-tat
            strategy is that, unlike the game of Mora, the strategy
            does not have to be kept secret. When one is faced by an
            opponent that is playing tit-for-tat, one can do no better
            than to cooperate. This makes tit-for-tat a stable
            strategy.

            Unfortunately, tit-for-tat does not do so well when the
            opponent occasionally defects, and then returns to a
            generally cooperative strategy. Neither does it do well
            when the other player is playing a random strategy. As in
            the case of the game of Mora, the strategy can be derived
            empirically, over time, using adaptive control
            computational methodologies. The subject of {\it inductive
            reasoning}\/ as an adaptive control methodology is
            considered in Section~\ref{MG}.

            As in Section~\ref{crp}, the objective of each player is
            to maximize the number of coins won over many iterations
            of the game, ie., to maximize the cumulative returns of
            the game. Note that each player's capital, will fluctuate,
            depending on the outcome of a particular iteration-and
            that fluctuation will be random, and either $0$, $1$, $3$,
            or $6$ coins. We would expect that the time series
            representing the fluctuations in a player's capital to be
            a random walk, which could be represented by a formula
            similar to Equation~\ref{iteration}\footnote{Assuming that
            one player, or the other, will, at least occasionally,
            alter strategy in an attempt to gain an advantage---in
            this case, for example, two players, each playing
            tit-for-tat will ``lock'' in to either a defection
            strategy, or cooperation strategy. This is considered a
            degenerate case of Equation~\ref{iteration}.}. Computer
            simulations of the co-evolving strategies of iterated
            multi-player prisoner dilemma scenarios where the
            individual players ``learn'' how to cooperate further
            support the hypothesis~\cite[pp. 170]{Davis}.

        \subsection{Multi-Player Games}
            \label{MG}

            A simple coin tossing game was analyzed in
            Section~\ref{crp}. In Section~\ref{MORA}, those concepts
            were expanded to include zero-sum games of mixed strategy,
            using the game of Mora as an example. It was shown in
            these types of games, the optimal strategy does not depend
            on how your opponent plays the game over time. In
            Section~\ref{PD}, a nonzero-sum game, the prisoner's
            dilemma, was analyzed and it was shown that the strategy
            for the game is deductively indeterminate since the game's
            logic is self-referential. The reason for this was that
            one player's strategy depended on how the other player
            plays the game over time. In both cases, the cumulative
            sum of winnings of a player was shown to have
            characteristics of a random walk, Brownian motion
            fractal. In this section, these concepts will be expanded
            to include multi-player games, where the players use
            inductive reasoning to determine a set of perceptions,
            expectations, and beliefs concerning the best way to play
            the game. These types of scenarios are typical of
            industrial manufacturing and equity markets.

            \subsubsection{Inductive Reasoning}

                Paraphrasing\footnote{Actually, plagiarize would be a
                more appropriate choice of wording. This entire
                section is a condensed version of the text
                from~\cite{Arthur:CIEAFM}
                and~\cite{Arthur:IRABR}.}~\cite{Arthur:CIEAFM},
                actions taken by economic decision makers are
                typically a predicated on hypotheses or predictions
                about future states of the world that is itself, in
                part, the consequence of these hypotheses or
                predictions. Predictions or expectations can then
                become self-referential and deductively
                indeterminate. In such situations, agents predict not
                deductively, but inductively. They form subjective
                expectations or hypotheses about what determines the
                world they face. These expectations are formulated,
                used, tested, modified in a world that forms from
                others' subjective expectations. This results in
                individual expectations trying to prove themselves
                against others' expectations. The result is an ecology
                of co-evolving expectations that can often only be
                analyzed by computational means. This co-evolution of
                expectations explains phenomena seen in real equity
                markets that appear as anomalies to standard finance
                theory~\cite{Arthur:CIEAFM},~\cite{Arthur:IRABR}.

                This concept views such ``games'' in psychological
                terms: as a collection of beliefs, anticipations,
                expectations, cognitions, and interpretations; with
                decision-making and strategizing and action-taking
                predicated upon beliefs and expectations. Of course
                this view and the standard economic views are
                related---activities follow from beliefs and
                expectations, which are mediated by the physical
                economy~\cite{Arthur:CIEAFM}.

                This is a very useful concept because it essentially
                states that economic agents make their choices based
                upon their current beliefs or hypothesis about future
                prices, interest rates, or a competitors' future move
                in a market. These choices, when aggregated, in turn
                shape the prices, interest rates, market strategies,
                etc., that the agents face. These beliefs or
                hypotheses of the agents are largely individual,
                subjective, and private. They are constantly tested
                and modified in a world that forms from their's and
                others' actions~\cite{Arthur:CIEAFM}.

                In the aggregate, the economy will consist of a vast
                collection of these beliefs or hypotheses, constantly
                being formulated, acted upon, changed and discarded;
                all interacting and competing and evolving and
                co-evolving. Beyond the simplest problems in
                economics, this ecological view of the economy becomes
                inevitable~\cite{Arthur:CIEAFM}.

                The ``standard way'' to handle predictive beliefs in
                economics is to assume identical agents who possess
                perfect rationality and arrive at shared, logical
                conclusions about the economic environment. When these
                these expectations are validated as predictions, then
                they are in equilibrium, and are called {\it rational
                expectations}. Rational expectations often are not
                robust since many agents can arrive at different
                conclusions from the same data, causing some to
                deviate in their expectations, causing others to
                predict something different and then deviate
                too~\cite{Arthur:CIEAFM}.

                \cite{Arthur:CIEAFM} cites the ``El Farol Bar''
                problem as an example. Assume one hundred people must
                decide independently each week whether go to the
                bar. The rule is that if a person predicts that more
                than, say, 60 will attend, it will be too crowded, and
                the person will stay home; if less than 60 is
                predicted, the person will go to the bar. As trivial
                as this seems, it destroys the possibility of long-run
                shared, rational expectations.  If all believe {\it
                few}\/ will go, then {\it all}\/ will go, thus
                invalidating the expectations. And, if all believe
                {\it many}\/ will go, then {\it none}\/ will go,
                likewise invalidating those expectations. Like the
                iterated prisoner's dilemma, predictions of how many
                will attend depend on others' predictions, and others'
                predictions of others' predictions.  Once again, there
                is no rational means to arrive at deduced {\it
                a-priori}\/ predictions. The important concept is that
                expectation formation is a self-referential process in
                systems involving many agents with incomplete
                information about the future behavior of the other
                agents. The problem of logically forming expectations
                then becomes ill-defined, and rational deduction, can
                not be consistent or complete. This indeterminacy of
                expectation-formation is by no means an anomaly within
                the real economy. On the contrary, it pervades all of
                economics and game theory~\cite{Arthur:CIEAFM}.

                It is an important concept that this view of
                industrial and financial markets address such notions
                as market ``psychology,'' ``moods,'' and ``jitters.''
                Markets do turn out to be reasonably efficient, as
                predicted by standard financial theory, but the
                statistics show that trading volume and price
                volatility in real markets are a great deal higher
                than the standard theories predict. Statistical tests
                also show that technical trading can produce
                consistent, if modest, long-run profits. And the crash
                of 1987 showed dramatically that sudden price changes
                do not always reflect rational adjustments to news in
                the market~\cite{Arthur:CIEAFM}.

                In this market model, inductive reasoning prevails as
                the ``engine'' of the market since no deductive
                hypothesis is possible because of the G{\"o}delian
                issues of self-referential arbitrage.

                It should be pointed out that inductive reasoning in
                such scenarios is not an exact process, and usually
                relies, to some extent, on correlation between events
                in the economy. In self-referential processes, single
                simplex statistical evaluations are not possible, and
                this can lead to misinterpretation of the significance
                of the statistics of the
                events~\cite[pp. 50]{Casti:SFC}\footnote{Additionally,
                there are issues concerning causality. Cause and
                effect may not be discernable from each other.}.

            \subsubsection{A multi-player, self-referential model of an equities market}

                Suppose that throughout a trading day, agents line up
                to buy or sell a stock. When a particular agents' turn
                comes, the agent has the option to try to increase or
                decrease the price of the stock from the transaction
                price of the previous agent, (by lowering the price to
                sell stock the agent owns, or raising the price to buy
                stock from another agent.) The agent will have to make
                this decision based on beliefs concerning the beliefs
                of the agents in the rest of the market. This decision
                process will vary as different agents post their
                transaction through the day, based on their personal
                set of beliefs, cognitions, and hypothesis concerning
                the market.  We would expect that the time series
                representing the fluctuations in a stock's price to be
                a random walk, which could be represented by a formula
                similar to
                Equation~\ref{iteration}~\cite[pp. 8]{Arthur:CIEAFM}.
                Empirical analysis of many stocks tend to support the
                hypothesis that stock prices can be ``modeled'' as a
                random walk, or fractional Brownian motion
                fractal. Additionally, computer models of stock market
                asset pricing under inductive reasoning with many
                agents has been initiated and further support the
                hypothesis~\cite[pp. 8]{Arthur:CIEAFM}.

            \subsubsection{Stability Issues}

                In section~\ref{nlextend} and~\ref{PD} the issues of
                process stability were mentioned. Note that not all
                processes are stable. For example, consider a stock
                market scenario that historically had cyclic or
                periodic increases and decreases in price. The value
                at the bottom of the cycle would increase, (because
                the agents in the market could exploit a ``buy low,
                sell high'' strategy that would be predictable,) and
                the price advantage would be arbitrated away, and the
                cyclic phenomena would disappear. Cyclic phenomena
                would then be considered as an unstable
                process---similar to the El Farol Bar problem
                mentioned above. However, note that if the agents in
                the market believed that their financial position
                could be improved by altering their investment
                strategy, by buying or selling of stocks, then, as
                outlined in the previous section, the stock price
                would fluctuate similar to a random walk, and this
                would be stable since it is a self reinforcing
                situation.

            \subsubsection{Extensibility Speculations}

                Interestingly, the arguments presented in this section
                are possibly extensible into other areas. For example,
                the Stanford economist Kenneth Arrow has shown that
                the ranking of priorities in a group is
                intransitive\cite[pp. 1]{Lenstra}~\cite[pp. 327]{Luce}~\cite[pp. 213]{Hoffman}. What
                this means is that there exists no way to use
                deductive rationality to rank priorities in a society.
                If it is assumed that it is necessary to do so, then
                inductive reasoning would have to be used. If it is
                further assumed that such a situation is
                self-referential, which seems reasonable by arguments
                similar to those presented in this section, then the
                same issues outlined in this section could be
                applicable to social welfare issues, etc. This would
                tend to imply that political issues were fractal in
                nature, and the political process justified---which is
                contrary to the thinking of many. The arguments
                presented in~\cite{Arthur:CIEAFM},
                and~\cite{Arthur:IRABR} may well be extensible into
                other fields of interest. Other speculations could
                involve theoretical interests in the dynamics of
                democratic process, legal process\footnote{Could the
                legal system be optimized? Or is that an oxymoron?},
                and organizational process\footnote{For
                example,~\cite[pp. 81]{Senge} has a diagram of the
                sales department process in an organization. It has
                the same schema as Figures~\ref{figmarket1}
                and~\ref{figmarket2}. If it could be shown that
                organizational complexity is an NP
                problem~\cite[pp. 313]{Sommerhalder},~\cite[pp. 13]{Garey},
                then there there could be some reasonable
                formalization of the observations presented
                in~\cite{Brooks} and~\cite{Ulam}.}.  There are
                probably other applications\footnote{Others feel a bit
                more epistemological about the
                issue---see~\cite[pp. 178]{Rucker}, the chapter
                entitled ``Life is a Fractal in Hilbert Space.''}.

                As another interesting aside, the arguments presented
                in this section side-stepped the issue of utility
                theory.

            \subsubsection{Conclusion}

                In this section, it was shown that markets would be
                expected to exhibit self-referential processes, which
                can not be analyzed by deductive rationality. However,
                when players rely on inductive reasoning to formulate
                strategies to execute their market agenda, the result
                is that the market will exhibit fractal
                dynamics. Previously, in this chapter, it was shown
                that the fractal dynamics can be exploited and
                optimized. Interestingly, in some sense, there appears
                to be a convergence of game-theoretic,
                information-theoretic, non-linear dynamical systems
                theory, and fractal/chaos-theoretic concepts.
                Further, there also appears to be a convergence of
                these concepts with the cognitive sciences.

    \section{Conclusion}

        In this chapter, a very simple, first order, model of an
        industrial market has been proposed. The reader should be
        aware that a paradigm is involved. This chapter proposes that
        industrial markets can be adequately modeled as a fixed
        increment Brownian, perhaps time sampled, fractal. In general,
        markets exhibit fractional Brownian fractal characteristics. A
        method of optimization of market operations is proposed that
        uses fixed increment Brownian fractal methodologies to analyze
        industrial markets.  The analysis of various markets in
        Appendix~\ref{markets} would tend to offer supporting evidence
        that the paradigm is practical. However, the application of
        Equation~\ref{optimumequation}, $f = 2P - 1$ to fractional
        Brownian fractal time series remains an open formal issue. The
        model is extended with non-linear terms, and shown to be a
        logistic function, which is compatible with other literature.

        The first order model is then extended into iterated
        game-theoretic scenarios, as a mixed strategy zero-sum game,
        and then as a nonzero-sum game, that is self-referential. In
        both cases, the capital of a player in the game is argued to
        show random walk, or Brownian motion fluctuations.

        Finally, the model is extended into a multi-player market
        scenario, where the agents use inductive reasoning to cope
        with the self-referential characteristics of the marketplace.
        An argument is presented that the time series of the market
        will have fluctuations that are similar to a random walk, or
        Brownian motion fractal.

    \section{Summary}

        If we consider capital, $V$, invested in a savings account,
        and calculate the growth of the capital over time:

        \begin{equation}
            V_{t} = V_{t - 1} \left(1 + a_{t}\right)
            \label{summary1}
        \end{equation}

        \noindent where $a_{t}$ is the interest rate at time $t$,
        (usually a constant\footnote{For example, if $a = 0.06$, or
        $6$\%, then at the end of the first time interval the capital
        would have increased to $1.06$ times its initial value. At the
        end of the second time interval it would be
        $\left(1.06\right)^{2}$, and so on. What
        Equation~\ref{summary1} states is that the way to get the
        value, $V$ in the next time interval is to multiply the
        current value by $1.06$. Equation~\ref{summary1} is nothing
        more than a ``prescription,'' or a process to make an
        exponential, or ``compound interest'' mechanism. In general,
        exponentials can always be constructed by multiplying the
        current value of the exponential by a constant, to get the
        next value, which in turn, would be multiplied by the same
        constant to get the next value, and so
        on. Equation~\ref{summary1} is nothing more than a
        construction of $V\left(t\right) = e^{kt}$ where $k =
        \ln\left(1 + a\right)$. The advantage of representing
        exponentials by the ``prescription'' defined in
        Equation~\ref{summary1} is analytical expediency. For example,
        if you have data that is an exponential, the parameters, or
        constants, in Equation~\ref{summary1} can be determined by
        simply reversing the ``prescription,'' ie., subtracting the
        previous value, (at time $t - 1$,) from the current value, and
        dividing by the previous value would give the exponentiating
        constant, $\left(1 + a_{t}\right)$. This process of reversing
        the ``prescription'' is termed calculating the ``normalized
        increments.'' (Increments are simply the difference between
        two values in the exponential, and normalized increments are
        this difference divided by the value of the exponential.)
        Naturally, since one usually has many data points over a time
        interval, the values can be averaged for better
        precision---there is a large mathematical infrastructure
        dedicated to precision enhancement, for example, least squares
        approximation to the normalized increments, and statistical
        estimation.}.) In equities, $a_{t}$ is not constant, and
        varies---perhaps being negative at certain times, (meaning
        that the value of the equity decreased.)  This fluctuation in
        an equity's value can be represented by modifying $a_{t}$ in
        Equation~\ref{summary1}:

        \begin{equation}
            a_{t} = f_{t} F_{t}
            \label{summary2}
        \end{equation}

        \noindent where the product $f_{t} \cdot F_{t}$ is the
        fluctuation in the equity's value at time $t$.

        An equity's value, over time, is similar to a simple tossed
        coin game~\cite[pp. 128]{Schroeder}, where $f_{t}$ is the
        fraction of a gambler's capital wagered on a toss of the coin,
        at time $t$, and $F_{t}$ is a random
        variable\footnote{``Random variable'' means that the process,
        $F_{t}$, is random in nature, ie., there is no possibility of
        determining what the next value will be. However, $F_{t}$ can
        be analyzed using statistical
        methods~\cite[pp. 163]{Feder},~\cite[pp. 128]{Schroeder}. For
        example, $F_{t}$ typically has a Gaussian distribution for
        equity values~\cite[pp. 249]{Crownover}, in which case the it
        is termed a ``fractional Brownian motion,'' or simply a
        ``fractal'' process. In the case of a single tossed coin, it
        is termed ``fixed increment fractal,'' ``Brownian,'' or
        ``random walk'' process. In any case, determination of the
        statistical characteristics of $F_{t}$ are the essence of
        analysis. Fortunately, there is a large mathematical
        infrastructure dedicated to the subject. For example, $F_{t}$
        could be verified as having a Gaussian distribution using
        Chi---Square techniques. Frequently, it is convenient, from an
        analytical standpoint, to ``model'' $F_{t}$ using a
        mathematically simpler process~\cite[pp. 128]{Schroeder}. For
        example, multiple iterations of tossing a coin can be used to
        approximate a Gaussian distribution, since the distribution of
        many tosses of a coin is binomial---which if the number of
        tosses is sufficient will represent a Gaussian distribution to
        within any required
        precision~\cite[pp. 144]{Schroeder},~\cite[pp. 154]{Feder}.},
        signifying whether the game was a win, or a loss, ie., whether
        the gambler's capital increased or decreased, and by how
        much. The amount the gambler's capital increased or decreased
        is $f_{t} \cdot F_{t}$. In general, $F_{t}$ is a function of a
        random variable, with an average, over time, of $avg_{f}$, and
        a root mean square value, $rms_{f}$, of unity. Note that for
        simple, time invariant, compound interest, $F_{t}$ has an
        average and root mean square, both being unity, and $f_{t}$ is
        simply the interest rate, which is assumed to be constant. For
        a simple, single coin game, $F_{t}$ is a fixed increment,
        (ie., either $+1$ or $-1$,) random generator. From an
        analytical perspective, it would be advantageous to measure
        the the statistical characteristics of the
        generator. Substituting Equation~\ref{summary2} into
        Equation~\ref{summary1}\footnote{Equation~\ref{summary3} is
        interesting in many other respects. For example, adding a
        single term, $m \cdot V_{t - 1}$, to the equation results in
        $V_{t} = V_{t - 1} \left(1 + f_{t} F_{t} + m \cdot V_{t -
        1}\right)$ which is the ``logistic,'' or `S' curve equation,
        (formally termed the ``discreet time quadratic equation,'')
        and has been used successfully in many unrelated fields such
        as manufacturing operations, market and economic forecasting,
        and analyzing disease epidemics~\cite[pp. 131]{Modis}. There
        is continuing research into the application of an additional
        ``non-linear'' term in Equation~\ref{summary3} to model equity
        value non-linearities. Although there have been modest
        successes, to date, the successes have not proved to be
        exploitable in a systematic
        fashion~\cite[pp. 133]{Peters:CAOITCM}. The reason for the
        interest is that the logistic equation can exhibit a wide
        variety of behaviors, among them, ``chaotic.'' Interestingly,
        chaotic behavior is mechanistic, but not ``long term''
        predictable into the future. A good example of such a system
        is the weather. It is an important concept that compound
        interest, the logistic function, and fractals are all closely
        related.}:

        \begin{equation}
            V_{t} = V_{t - 1} \left(1 + f_{t} F_{t}\right)
            \label{summary3}
        \end{equation}

        \noindent and subtracting $V_{t - 1}$ from both sides:

        \begin{equation}
            V_{t} - V_{t - 1} = V_{t - 1} \left(1 + f_{t} F_{t}\right) - V_{t - 1}
            \label{summary4}
        \end{equation}

        \noindent and dividing both sides by $V_{t - 1}$:

        \begin{equation}
            \frac{V_{t} - V_{t - 1}}{V_{t - 1}} = \frac{V_{t - 1} \left(1 + f_{t} F_{t}\right) - V_{t - 1}}{V_{t - 1}}
            \label{summary5}
        \end{equation}

        \noindent and combining:

        \begin{equation}
            \frac{V_{t} - V_{t - 1}}{V_{t - 1}} = \left(1 + f_{t} F_{t}\right) - 1 =  f_{t} F_{t}
            \label{summary6}
        \end{equation}

        We now have a ``prescription,'' or process, for calculating
        the characteristics of the random process that determines an
        equity's value.  That process is, for each unit of time,
        subtract the value of the of the equity at the previous time
        from the value of the equity at the current time, and divide
        this by the value of the equity at the previous time. The root
        mean square\footnote{In this section, ``root mean square'' is
        used to mean the variance of the normalized increments. In
        Brownian motion fractals, this is computed by
        $\sigma_total^{2} = \sigma_1^{2} + \sigma_2^{2} + \cdots$
        However, in many fractals, the variances are not calculated by
        adding the squares, (ie., a power of $2$,) of the values---the
        power may be ``fractional,'' ie., $3 / 2$ instead of $2$, for
        example~\cite[pp. 130]{Schroeder},~\cite[pp. 178]{Feder}. However,
        as a first order approximation, the variances of the
        normalized increments of equity values can successfully be
        added root mean square~\cite[kpp. 250]{Crownover}. The so
        called ``Hurst'' coefficient, which can be measured,
        determines the process to be used. The Hurst coefficient is
        range of the equity values over a time interval, divided by
        the standard deviation of the values over the interval, and
        its determination is commonly called ``$R / S$'' analysis. As
        pointed out in~\cite[pp. 157]{Schroeder} the errors committed
        in such simplified assumptions can be significant---however,
        for analysis of equities, squaring the variances seems to be a
        reasonable simplification.}  of these values are the root mean
        square of the random process. The average of these values are
        the average of the random process, $avg_{f}$. The root mean
        square of these values can be calculated by any convenient
        means, and will be represented by $rms$. The average of these
        values can be found by any convenient means, and will be
        represented by $avg$\footnote{For example, many calculators
        have averaging and root mean square functionality, as do many
        spreadsheet programs---additionally, there are computer source
        codes available for both. See the programs {\it tsrms}\/ and
        {\it tsavg}\/. The method used is not
        consequential.}. Therefore, if $f_{t} = f$, and does not vary
        over time:

        \begin{equation}
            rms = f
            \label{summary7}
        \end{equation}

        \noindent which, if there are sufficiently many samples, is a
        metric of the equity value's ``volatility,'' and:

        \begin{equation}
            avg = f \cdot F_{t}
            \label{summary9}
        \end{equation}

        \noindent and if there are sufficiently many samples, the
        average of $F_{t}$ is simply $avg_{f}$, or:

        \begin{equation}
            avg = f \cdot avg_{f}
            \label{summary10}
        \end{equation}

        \noindent which is a metric on the equity value's rate of
        ``growth.''  Note that this is the ``effective'' compound
        interest rate from Equation~\ref{summary1}.

        Equations~\ref{summaryappb7} and~\ref{summaryappb10} are
        important equations, since they can be used in portfolio
        management. For example, Equation~\ref{summaryappb7} states
        that the volatility of the capital invested in many equities,
        simultaneously, is calculated as the root mean square of the
        individual volatility of the
        equities. Equation~\ref{summaryappb10} states that the growths
        in the same equity values add together linearly\footnote{There
        are significant implications do to the fact that equity
        volatilities are calculated root mean square. For example, if
        capital is invested in $N$ many equities, concurrently, then
        the volatility of the capital will be $\frac{1}{\sqrt{N}}
        \cdot rms$ of an individual equity's volatility, $rms$,
        provided all the equites have similar statistical
        characteristics. But the growth in the capital will be
        unaffected, ie., it would be statistically similar to
        investing all the capital in only one equity. What this means
        is that capital, or portfolio, volatility can be minimized
        without effecting portfolio growth---ie., volatility risk can
        addressed.  Further, it does not make any difference, as far
        as portfolio value growth is concerned, whether the individual
        equities are invested in concurrently, or serially, ie., if
        one invested in $10$ different equities for $100$ days,
        concurrently, or one could invest in only one equity, for $10$
        days, and then the next equity for the next $10$ days, and so
        on. The capital growth would have the same characteristics for
        both agendas. (Note that the concurrent agenda is superior
        since the volatility of the capital will be the root mean
        square of the individual equity volatilities divided by the
        square root of the number of equities.  In the serial agenda,
        the volatility of the capital will be simply the root mean
        square of the individual equity volatilities.) Almost all
        equity wagering strategies will consist of optimizing
        variations on combinations of serial and concurrent
        agendas. There are further applications.  For example,
        Equation~\ref{summaryappb6} could be modified by dividing both
        the normalized increments, and the square of the normalized
        increments by the daily trading volume. The quotient of the
        normalized increments divided by the trading volume is the
        instantaneous growth, $avg_{f}$, of the equity, on a per-share
        basis. Likewise, the square root of the square of the
        normalized increments divided by the daily trading volume is
        the instantaneous root mean square, $rms_{f}$, of the equity
        on a per-share basis, ie., its instantaneous volatility of the
        equity. (Note that these instantaneous values are the
        statistical characteristics of the equity on a per-share
        bases, similar to a coin toss, and not on time.) Additionally,
        it can be shown that the range---the maximum minus the
        minimum---of an equity's value over a time interval will
        increase with the square root of of the size of the interval
        of time~\cite[pp. 178]{Feder}. Also, it can be shown that the
        number of expected stock value ``high and low'' transitions
        scales with the square root of time, meaning that the
        probability of an equity value ``high or low'' exceeding a
        given time interval is proportional to the square root of the
        time interval~\cite[pp. 153]{Schroeder}.}.

        Dividing Equation~\ref{summary10} by Equation~\ref{summary7}
        results in the two $f$'s canceling, or:

        \begin{equation}
            \frac{avg}{rms} = avg_{f}
            \label{summary11}
        \end{equation}

        There may be analytical advantages to ``model'' $avg_{f}$ as a
        simple tossed coin game, (either played with a single coin, or
        multiple coins, ie., many coins played at one time, or a
        single coin played many times\footnote{Here the ``model'' is
        to consider two black boxes, one with a stock ``ticker'' in
        it, and the other with a casino game of a tossed coin in
        it. One could then either invest in the equity, or,
        alternatively, invest in the tossed coin game by buying many
        casino chips, which constitutes the starting capital for the
        tossed coin game. Later, either the equity is sold, or the
        chips ``cashed in.'' If the statistics of the equity value
        over time is similar to the statistics of the coin game's
        capital, over time, then there is no way to determine which
        box has the equity, or the tossed coin game. The advantage of
        this model is that gambling games, such as the tossed coin,
        have a large analytical infrastructure, which, if the two
        black boxes are statistically the same, can be used in the
        analysis of equities. The concept is that if the value of the
        equity, over time, is statistically similar to the coin game's
        capital, over time, then the analysis of the coin game can be
        used on equity values. Note that in the case of the equity,
        the terms in $f_{t} \cdot F_{t}$ can not be separated. In this
        case, $f = rms$ is the fraction of the equity's value, at any
        time, that is ``at risk,'' of being lost, ie., this is the
        portion of a equity's value that is to be ``risk managed.''
        This is usually addressed through probabilistic methods, as
        outlined below in the discussion of Shannon probabilities,
        where an optimal wagering strategy is determined. In the case
        of the tossed coin game, the optimal wagering strategy is to
        bet a fraction of the capital that is equal to $f = rms = 2P -
        1$~\cite[pp. 128, 151]{Schroeder}, where $P$ is the Shannon
        probability. In the case of the equity, since $f = rms$ is not
        subject to manipulation, the strategy is to select equities
        that closely approximate this optimization, and the equity's
        value, over time, on the average, would increase in a similar
        fashion to the coin game. The growth of either investment
        would be equal to $avg = rms^{2}$, on average, for each
        iteration of the coin game, or time unit of equity
        investment. This is an interesting concept from risk
        management since it maximizes the gain in the capital, while,
        simultaneously, minimizing risk exposure to the capital.}.)
        The number of wins minus the number of losses, in many
        iterations of a single coin tossing game would be:

        \begin{equation}
            P - \left(1 - P\right) = 2P - 1
        \end{equation}

        \noindent where P is the probability of a win for the tossed
        coin. (This probability is traditionally termed, the ``Shannon
        probability'' of a win.)  Note that from the definition of
        $F_{t}$ above, that $P = avg_{f}$. For a fair coin, (ie., one
        that comes up with a win 50\% of the time,) $P = 0.5$, and
        there is no advantage, in the long run, to playing the
        game. However, if $P > 0.5$, then the optimal fraction of
        capital wagered on each iteration of the single coin tossing
        game, $f$, would be $2P - 1$. Note that if multiple coins were
        used for each iteration of the game, we would expect that the
        volatility of the gambler's capital to increase as the square
        root of the number of coins used, and the growth to increase
        linearly with the number of coins used, irregardless of
        whether many coins were tossed at once, or one coin was tossed
        many times, (ie., our random generator, $F_{t}$ would assume a
        binomial distribution---and if the number of coins was very
        large, then $F_{t}$ would assume, essentially, a Gaussian
        distribution.) Many equities have a Gaussian distribution for
        the random process, $F_{t}$. It may be advantageous to
        determine the Shannon probability to analyze equity investment
        strategies. From Equation~\ref{summary11}:

        \begin{equation}
            \frac{avg}{rms} = avg_{f} = 2P - 1
            \label{summary12}
        \end{equation}

        \noindent or:

        \begin{equation}
            \frac{avg}{rms} + 1 = 2P
            \label{summary13}
        \end{equation}

        \noindent and:

        \begin{equation}
            P = \frac{\frac{avg}{rms} + 1}{2}
            \label{summary14}
        \end{equation}

        \noindent where only the average and root mean square of the
        normalized increments need to be measured, using the
        ``prescription'' or process outlined above.

        Interestingly, what Equation~\ref{summary12} states is that
        the ``best'' equity investment is not, necessarily, the equity
        that has the largest average growth, $avg_{f}$. The best
        equity investment is the equity that has the largest growth,
        while simultaneously having the smallest volatility. In point
        of fact, the optimal decision criteria is to choose the equity
        that has the largest {\it ratio}\/ of growth to volatility,
        where the volatility is measured by computing the root mean
        square of the normalized increments, and the growth is
        computed by averaging the normalized increments.

        We now have a ``first order prescription'' that enables us to
        analyze fluctuations in equity values, although we have not
        explained why equity values fluctuate. For a formal
        presentation on the subject, see the bibliography
        in~\cite{Arthur:CIEAFM} which, also, offers non-mathematical
        insight into the explanation.

        Consider a very simple equity market, with only two people
        holding equities.  Equity value ``arbitration'' (ie., how
        equity values are determined,) is handled by one person
        posting (to a bulletin board,) a willingness to sell a given
        number of stocks at a given price, to the other person. There
        is no other communication between the two people. If the other
        person buys the stock, then that is the value of the stock at
        that time. Obviously, the other person will not buy the stock
        if the price posted is too high---even if ownership of the
        stock is desired. For example, the other person could simply
        decide to wait in hopes that a favorable price will be offered
        in the future. So the stock seller must not post a price that
        the other person would consider too high, and the other person
        would not buy at the price if it is reasoned that the seller's
        pricing strategy will be to lower the offering price in the
        future, which would be a reasonable deduction if the posted
        price is considered too high. What this means is that the
        seller must consider not only the behavior of the other
        person, but what the other person thinks the seller's behavior
        will be, ie., the seller must base the pricing strategy on the
        seller's pricing strategy.  Such convoluted logical processes
        are termed ``self referential,'' and the implication is that
        the market can never operate in a consistent fashion that can
        be the subject of deductive
        analysis~\cite[pp. 101]{Penrose}\footnote{Penrose, referencing
        Russell's paradox, presents a very good example of logical
        contradiction in a self-referential system. Consider a library
        of books. The librarian notes that some books in the library
        contain their titles, and some do not, and wants to add two
        index books to the library, labeled ``A'' and ``B,''
        respectively; the ``A'' book will contain the list of all of
        the titles of books in the library that contain their titles;
        and the ``B'' book will contain the list of all of the titles
        of the books in the library that do not contain their
        titles. Now, clearly, all book titles will go into either the
        ``A'' book, or the ``B'' book, respectively, depending on
        whether it contains its title, or not.  Now, consider in which
        book, the ``A'' book or the ``B'' book, the title of the ``B''
        book is going to be placed---no matter which book the title is
        placed, it will be contradictory with the rules. And, if you
        leave it out, the two books will be incomplete.)}. As pointed
        out by~\cite[Abstract]{Arthur:CIEAFM}, these types of
        indeterminacies pervade economics.

        What the two players do, in absence of a deductively
        consistent and complete theory of the market, is to rely on
        inductive reasoning. They form subjective expectations or
        hypotheses about how the market operates. These expectations
        and hypothesis are constantly formulated and changed, in a
        world that forms from others' subjective expectations. What
        this means is that equity values will fluctuate as the
        expectations and hypothesis concerning the future of equity
        values change\footnote{Interestingly, the system described is
        a stable system, ie., if the players have a hypothesis that
        changing equity positions may be of benefit, then the equity
        values will fluctuate---a self fulfilling prophecy. Not all
        such systems are stable, however. Suppose that one or both
        players suddenly discover that equity values can be ``timed,''
        ie., there are certain times when equities can be purchased,
        and chances are that the equity values will increase in the
        very near future. This means that at certain times, the
        equites would have more value, which would soon be arbitrated
        away. Such a scenario would not be stable.}. The fluctuations
        created by these indeterminacies in the equity market are
        represented by the term $f_{t} F_{t}$ in
        Equation~\ref{summary3}, and since there are many such
        indeterminacies, we would anticipate $F_t$ to have a Gaussian
        distribution.

        This is a rather interesting conclusion, since analyzing the
        actions of aggregately many ``agents,'' each operating on
        subjective hypothesis in a market that is deductively
        indeterminate, can result in a system that can not only be
        analyzed, but optimized.

        The only remaining derivation is to show that the optimal
        wagering strategy is, as cited above:

        \begin{equation}
            f = rms = 2P - 1
        \end{equation}

        \noindent where $f$ is the fraction of a gambler's capital
        wagered on each toss of a coin that has a Shannon
        probability, $P$, of winning.

        Following~\cite[pp. 450]{Reza}, consider that the gambler has
        a private wire into the future who places wagers on the
        outcomes of a game of chance. We assume that the side
        information which he receives has a probability, $P$, of being
        true, and of $1 - P$, of being false. Let the original capital
        of gambler be $V(0)$, and $V(n)$ his capital after the $n$'th
        wager. Since the gambler is not certain that the side
        information is entirely reliable, he places only a fraction,
        $f$, of his capital on each wager. Thus, subsequent to $n$
        many wagers, assuming the independence of successive tips from
        the future, his capital is:

        \begin{equation}
            V\left(n\right) = \left(1 + f\right)^{w} \left(1 - f\right)^{l} V\left(0\right)
        \end{equation}

        \noindent where $w$ is the number of times he won, and $l = n
        - w$, the number of times he lost. These numbers are, in
        general, values taken by two random variables, denoted by $W$
        and $L$. According to the law of large numbers:

        \begin{equation}
            \lim_{n\to\infty} \frac{1}{n} W = P
        \end{equation}

        \noindent and:

        \begin{equation}
            \lim_{n\to\infty} \frac{1}{n} L = q = 1 - P
        \end{equation}

        The problem with which the gambler is faced is the
        determination of $f$ leading to the maximum of the average
        exponential rate of growth of his capital. That is, he wishes
        to maximize the value of:

            \begin{equation}
                G = \lim_{n\to\infty} \frac{1}{n} \ln \frac{V\left(n\right)}{V\left(0\right)}
            \end{equation}

        \noindent with respect to $f$, assuming a fixed original
        capital and specified $P$:

            \begin{equation}
                G = \lim_{n\to\infty} \frac{W}{n} \ln \left(1 + f\right) + \frac{L}{n} \ln \left(1 - f\right)
            \end{equation}

        \noindent or:

            \begin{equation}
                G = P \ln \left(1 + f\right) + q \ln \left(1 - f\right)
            \end{equation}

        \noindent which, by taking the derivative with respect to $f$,
        and equating to zero, can be shown to have a maxima when:

            \begin{equation}
                \frac{dG}{df} = P\left(1 + f\right)^{P - 1} \left(1 - f\right)^{1 - P} - \left(1 - P\right)\left(1 - f\right)^{1 - P - 1} \left(1 + f\right)^{P} = 0
            \end{equation}

        \noindent combining terms:

            \begin{equation}
                P\left(1 + f\right)^{P - 1} \left(1 - f\right)^{1 - P} - \left(1 - P\right) \left(1 - f\right)^{P} \left(1 + f\right)^{P} = 0
            \end{equation}

        \noindent and splitting:

            \begin{equation}
                P\left(1 + f\right)^{P - 1} \left(1 - f\right)^{1 - P} = \left(1 - P\right) \left(1 - f\right)^{P} \left(1 + f\right)^{P}
            \end{equation}

        \noindent then taking the logarithm of both sides:

            \begin{equation}
                \ln \left(P\right) + \left(P - 1\right) \ln \left(1 + f\right) + \left(1 - P\right) \ln \left(1 - f\right) = \ln \left(1 - P\right) - P \ln \left(1 - f\right) + P \ln \left(1 + f\right)
            \end{equation}

        \noindent and combining terms:

            \begin{equation}
                \left(P - 1\right) \ln \left(1 + f\right) - P \ln \left(1 + f\right) + \left(1 - P\right) \ln \left(1 - f\right) + P \ln \left(1 - f\right) = \ln \left(1 - P\right) - \ln \left(P\right)
            \end{equation}

        \noindent or:

            \begin{equation}
                \ln \left(1 - f\right) - \ln \left(1 + f\right) = \ln \left(1 - P\right) - \ln \left(P\right)
            \end{equation}

        \noindent and performing the logarithmic operations:

            \begin{equation}
                \ln \left(\frac{1 - f}{1 + f}\right) = \ln \left(\frac{1 - P}{P}\right)
            \end{equation}

        \noindent and exponentiating:

            \begin{equation}
                \frac{1 - f}{1 + f} = \frac{1 - P}{P}
            \end{equation}

        \noindent which reduces to:

            \begin{equation}
                P\left(1 - f\right) = \left(1 - P\right) \left(1 + f\right)
            \end{equation}

        \noindent and expanding:

            \begin{equation}
                P - Pf = 1 - Pf - P + f
            \end{equation}

        \noindent or:

            \begin{equation}
                P = 1 - P + f
            \end{equation}

        \noindent and, finally:

            \begin{equation}
                f = 2P - 1
            \end{equation}

% Local Variables:
% TeX-parse-self: t
% TeX-auto-save: t
% TeX-master: "fractal.tex"
% End:
